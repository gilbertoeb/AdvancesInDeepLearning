{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.049732144415167434,
  "eval_steps": 500,
  "global_step": 1900,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 2.6174812850088122e-05,
      "grad_norm": 30.22783851623535,
      "learning_rate": 1.2e-05,
      "loss": 7.5026,
      "step": 1
    },
    {
      "epoch": 5.2349625700176245e-05,
      "grad_norm": 29.683502197265625,
      "learning_rate": 1.1996859460874118e-05,
      "loss": 7.443,
      "step": 2
    },
    {
      "epoch": 7.852443855026436e-05,
      "grad_norm": 27.453813552856445,
      "learning_rate": 1.1993718921748234e-05,
      "loss": 7.9002,
      "step": 3
    },
    {
      "epoch": 0.00010469925140035249,
      "grad_norm": 24.240352630615234,
      "learning_rate": 1.1990578382622351e-05,
      "loss": 7.3203,
      "step": 4
    },
    {
      "epoch": 0.00013087406425044061,
      "grad_norm": 16.31001853942871,
      "learning_rate": 1.1987437843496467e-05,
      "loss": 7.0234,
      "step": 5
    },
    {
      "epoch": 0.00015704887710052873,
      "grad_norm": 25.663909912109375,
      "learning_rate": 1.1984297304370585e-05,
      "loss": 7.9986,
      "step": 6
    },
    {
      "epoch": 0.00018322368995061687,
      "grad_norm": 26.023481369018555,
      "learning_rate": 1.19811567652447e-05,
      "loss": 7.2023,
      "step": 7
    },
    {
      "epoch": 0.00020939850280070498,
      "grad_norm": 20.31952667236328,
      "learning_rate": 1.1978016226118818e-05,
      "loss": 7.9735,
      "step": 8
    },
    {
      "epoch": 0.0002355733156507931,
      "grad_norm": 25.408531188964844,
      "learning_rate": 1.1974875686992934e-05,
      "loss": 7.825,
      "step": 9
    },
    {
      "epoch": 0.00026174812850088123,
      "grad_norm": 29.22234344482422,
      "learning_rate": 1.197173514786705e-05,
      "loss": 8.8159,
      "step": 10
    },
    {
      "epoch": 0.0002879229413509693,
      "grad_norm": 27.097631454467773,
      "learning_rate": 1.1968594608741167e-05,
      "loss": 6.9501,
      "step": 11
    },
    {
      "epoch": 0.00031409775420105745,
      "grad_norm": 26.209110260009766,
      "learning_rate": 1.1965454069615283e-05,
      "loss": 8.0336,
      "step": 12
    },
    {
      "epoch": 0.0003402725670511456,
      "grad_norm": 23.530088424682617,
      "learning_rate": 1.19623135304894e-05,
      "loss": 7.3187,
      "step": 13
    },
    {
      "epoch": 0.00036644737990123373,
      "grad_norm": 22.233415603637695,
      "learning_rate": 1.1959172991363518e-05,
      "loss": 7.5003,
      "step": 14
    },
    {
      "epoch": 0.0003926221927513218,
      "grad_norm": 20.459087371826172,
      "learning_rate": 1.1956032452237634e-05,
      "loss": 6.8246,
      "step": 15
    },
    {
      "epoch": 0.00041879700560140996,
      "grad_norm": 24.221656799316406,
      "learning_rate": 1.1952891913111752e-05,
      "loss": 6.913,
      "step": 16
    },
    {
      "epoch": 0.0004449718184514981,
      "grad_norm": 15.340860366821289,
      "learning_rate": 1.1949751373985868e-05,
      "loss": 6.4441,
      "step": 17
    },
    {
      "epoch": 0.0004711466313015862,
      "grad_norm": 17.736690521240234,
      "learning_rate": 1.1946610834859985e-05,
      "loss": 7.4307,
      "step": 18
    },
    {
      "epoch": 0.0004973214441516743,
      "grad_norm": 24.10845184326172,
      "learning_rate": 1.1943470295734101e-05,
      "loss": 7.4598,
      "step": 19
    },
    {
      "epoch": 0.0005234962570017625,
      "grad_norm": 19.871971130371094,
      "learning_rate": 1.1940329756608219e-05,
      "loss": 6.3105,
      "step": 20
    },
    {
      "epoch": 0.0005496710698518506,
      "grad_norm": 30.47126579284668,
      "learning_rate": 1.1937189217482334e-05,
      "loss": 7.0865,
      "step": 21
    },
    {
      "epoch": 0.0005758458827019386,
      "grad_norm": 18.690874099731445,
      "learning_rate": 1.1934048678356452e-05,
      "loss": 6.3588,
      "step": 22
    },
    {
      "epoch": 0.0006020206955520268,
      "grad_norm": 19.9085750579834,
      "learning_rate": 1.193090813923057e-05,
      "loss": 6.2853,
      "step": 23
    },
    {
      "epoch": 0.0006281955084021149,
      "grad_norm": 22.348464965820312,
      "learning_rate": 1.1927767600104685e-05,
      "loss": 7.0539,
      "step": 24
    },
    {
      "epoch": 0.000654370321252203,
      "grad_norm": 17.024673461914062,
      "learning_rate": 1.1924627060978801e-05,
      "loss": 6.9062,
      "step": 25
    },
    {
      "epoch": 0.0006805451341022912,
      "grad_norm": 23.76247215270996,
      "learning_rate": 1.1921486521852917e-05,
      "loss": 7.154,
      "step": 26
    },
    {
      "epoch": 0.0007067199469523793,
      "grad_norm": 19.746639251708984,
      "learning_rate": 1.1918345982727035e-05,
      "loss": 6.2787,
      "step": 27
    },
    {
      "epoch": 0.0007328947598024675,
      "grad_norm": 19.664539337158203,
      "learning_rate": 1.1915205443601152e-05,
      "loss": 7.0984,
      "step": 28
    },
    {
      "epoch": 0.0007590695726525555,
      "grad_norm": 15.368788719177246,
      "learning_rate": 1.1912064904475268e-05,
      "loss": 6.8251,
      "step": 29
    },
    {
      "epoch": 0.0007852443855026436,
      "grad_norm": 14.586758613586426,
      "learning_rate": 1.1908924365349386e-05,
      "loss": 5.9218,
      "step": 30
    },
    {
      "epoch": 0.0008114191983527318,
      "grad_norm": 20.014503479003906,
      "learning_rate": 1.1905783826223501e-05,
      "loss": 6.4127,
      "step": 31
    },
    {
      "epoch": 0.0008375940112028199,
      "grad_norm": 16.258520126342773,
      "learning_rate": 1.1902643287097619e-05,
      "loss": 5.542,
      "step": 32
    },
    {
      "epoch": 0.000863768824052908,
      "grad_norm": 15.960464477539062,
      "learning_rate": 1.1899502747971735e-05,
      "loss": 5.913,
      "step": 33
    },
    {
      "epoch": 0.0008899436369029962,
      "grad_norm": 15.393472671508789,
      "learning_rate": 1.1896362208845852e-05,
      "loss": 5.518,
      "step": 34
    },
    {
      "epoch": 0.0009161184497530842,
      "grad_norm": 22.119590759277344,
      "learning_rate": 1.1893221669719968e-05,
      "loss": 6.6795,
      "step": 35
    },
    {
      "epoch": 0.0009422932626031724,
      "grad_norm": 22.641530990600586,
      "learning_rate": 1.1890081130594086e-05,
      "loss": 6.7796,
      "step": 36
    },
    {
      "epoch": 0.0009684680754532605,
      "grad_norm": 15.211116790771484,
      "learning_rate": 1.1886940591468203e-05,
      "loss": 6.2389,
      "step": 37
    },
    {
      "epoch": 0.0009946428883033486,
      "grad_norm": 17.28351593017578,
      "learning_rate": 1.188380005234232e-05,
      "loss": 5.4961,
      "step": 38
    },
    {
      "epoch": 0.0010208177011534368,
      "grad_norm": 14.06243896484375,
      "learning_rate": 1.1880659513216437e-05,
      "loss": 5.426,
      "step": 39
    },
    {
      "epoch": 0.001046992514003525,
      "grad_norm": 15.821523666381836,
      "learning_rate": 1.1877518974090553e-05,
      "loss": 6.035,
      "step": 40
    },
    {
      "epoch": 0.001073167326853613,
      "grad_norm": 17.898828506469727,
      "learning_rate": 1.1874378434964669e-05,
      "loss": 5.6535,
      "step": 41
    },
    {
      "epoch": 0.0010993421397037012,
      "grad_norm": 21.517593383789062,
      "learning_rate": 1.1871237895838786e-05,
      "loss": 5.962,
      "step": 42
    },
    {
      "epoch": 0.0011255169525537893,
      "grad_norm": 16.947418212890625,
      "learning_rate": 1.1868097356712902e-05,
      "loss": 6.4819,
      "step": 43
    },
    {
      "epoch": 0.0011516917654038773,
      "grad_norm": 17.106189727783203,
      "learning_rate": 1.186495681758702e-05,
      "loss": 5.1147,
      "step": 44
    },
    {
      "epoch": 0.0011778665782539654,
      "grad_norm": 32.797977447509766,
      "learning_rate": 1.1861816278461135e-05,
      "loss": 6.9105,
      "step": 45
    },
    {
      "epoch": 0.0012040413911040535,
      "grad_norm": 21.497854232788086,
      "learning_rate": 1.1858675739335253e-05,
      "loss": 6.4169,
      "step": 46
    },
    {
      "epoch": 0.0012302162039541417,
      "grad_norm": 17.993343353271484,
      "learning_rate": 1.1855535200209369e-05,
      "loss": 5.1963,
      "step": 47
    },
    {
      "epoch": 0.0012563910168042298,
      "grad_norm": 23.032163619995117,
      "learning_rate": 1.1852394661083486e-05,
      "loss": 6.4351,
      "step": 48
    },
    {
      "epoch": 0.001282565829654318,
      "grad_norm": 20.04737091064453,
      "learning_rate": 1.1849254121957604e-05,
      "loss": 4.7691,
      "step": 49
    },
    {
      "epoch": 0.001308740642504406,
      "grad_norm": 17.606029510498047,
      "learning_rate": 1.184611358283172e-05,
      "loss": 4.9135,
      "step": 50
    },
    {
      "epoch": 0.0013349154553544942,
      "grad_norm": 22.45792007446289,
      "learning_rate": 1.1842973043705837e-05,
      "loss": 4.2721,
      "step": 51
    },
    {
      "epoch": 0.0013610902682045824,
      "grad_norm": 23.00812530517578,
      "learning_rate": 1.1839832504579953e-05,
      "loss": 4.598,
      "step": 52
    },
    {
      "epoch": 0.0013872650810546705,
      "grad_norm": 26.22503089904785,
      "learning_rate": 1.183669196545407e-05,
      "loss": 4.6337,
      "step": 53
    },
    {
      "epoch": 0.0014134398939047586,
      "grad_norm": 27.360754013061523,
      "learning_rate": 1.1833551426328187e-05,
      "loss": 3.7572,
      "step": 54
    },
    {
      "epoch": 0.0014396147067548468,
      "grad_norm": 20.84589385986328,
      "learning_rate": 1.1830410887202304e-05,
      "loss": 5.1462,
      "step": 55
    },
    {
      "epoch": 0.001465789519604935,
      "grad_norm": 25.76931381225586,
      "learning_rate": 1.182727034807642e-05,
      "loss": 4.3577,
      "step": 56
    },
    {
      "epoch": 0.0014919643324550228,
      "grad_norm": 21.385438919067383,
      "learning_rate": 1.1824129808950536e-05,
      "loss": 5.1476,
      "step": 57
    },
    {
      "epoch": 0.001518139145305111,
      "grad_norm": 33.590694427490234,
      "learning_rate": 1.1820989269824653e-05,
      "loss": 3.8346,
      "step": 58
    },
    {
      "epoch": 0.0015443139581551991,
      "grad_norm": 25.63347625732422,
      "learning_rate": 1.181784873069877e-05,
      "loss": 3.3297,
      "step": 59
    },
    {
      "epoch": 0.0015704887710052873,
      "grad_norm": 25.36574363708496,
      "learning_rate": 1.1814708191572887e-05,
      "loss": 4.9581,
      "step": 60
    },
    {
      "epoch": 0.0015966635838553754,
      "grad_norm": 31.0465030670166,
      "learning_rate": 1.1811567652447004e-05,
      "loss": 4.6948,
      "step": 61
    },
    {
      "epoch": 0.0016228383967054635,
      "grad_norm": 29.258100509643555,
      "learning_rate": 1.180842711332112e-05,
      "loss": 4.7846,
      "step": 62
    },
    {
      "epoch": 0.0016490132095555517,
      "grad_norm": 23.9012451171875,
      "learning_rate": 1.1805286574195238e-05,
      "loss": 4.0025,
      "step": 63
    },
    {
      "epoch": 0.0016751880224056398,
      "grad_norm": 29.01807975769043,
      "learning_rate": 1.1802146035069354e-05,
      "loss": 4.4877,
      "step": 64
    },
    {
      "epoch": 0.001701362835255728,
      "grad_norm": 28.187484741210938,
      "learning_rate": 1.1799005495943471e-05,
      "loss": 3.0827,
      "step": 65
    },
    {
      "epoch": 0.001727537648105816,
      "grad_norm": 35.91884231567383,
      "learning_rate": 1.1795864956817587e-05,
      "loss": 2.6421,
      "step": 66
    },
    {
      "epoch": 0.0017537124609559042,
      "grad_norm": 28.194583892822266,
      "learning_rate": 1.1792724417691705e-05,
      "loss": 3.4314,
      "step": 67
    },
    {
      "epoch": 0.0017798872738059924,
      "grad_norm": 23.687419891357422,
      "learning_rate": 1.178958387856582e-05,
      "loss": 3.691,
      "step": 68
    },
    {
      "epoch": 0.0018060620866560803,
      "grad_norm": 20.045509338378906,
      "learning_rate": 1.1786443339439938e-05,
      "loss": 3.7732,
      "step": 69
    },
    {
      "epoch": 0.0018322368995061684,
      "grad_norm": 20.962646484375,
      "learning_rate": 1.1783302800314056e-05,
      "loss": 4.0607,
      "step": 70
    },
    {
      "epoch": 0.0018584117123562566,
      "grad_norm": 30.48273277282715,
      "learning_rate": 1.178016226118817e-05,
      "loss": 2.9529,
      "step": 71
    },
    {
      "epoch": 0.0018845865252063447,
      "grad_norm": 11.651561737060547,
      "learning_rate": 1.1777021722062287e-05,
      "loss": 1.394,
      "step": 72
    },
    {
      "epoch": 0.0019107613380564329,
      "grad_norm": 20.1381893157959,
      "learning_rate": 1.1773881182936403e-05,
      "loss": 2.4774,
      "step": 73
    },
    {
      "epoch": 0.001936936150906521,
      "grad_norm": 17.626211166381836,
      "learning_rate": 1.177074064381052e-05,
      "loss": 3.911,
      "step": 74
    },
    {
      "epoch": 0.0019631109637566094,
      "grad_norm": 14.677168846130371,
      "learning_rate": 1.1767600104684638e-05,
      "loss": 2.1644,
      "step": 75
    },
    {
      "epoch": 0.0019892857766066973,
      "grad_norm": 24.24646759033203,
      "learning_rate": 1.1764459565558754e-05,
      "loss": 4.2923,
      "step": 76
    },
    {
      "epoch": 0.002015460589456785,
      "grad_norm": 15.711065292358398,
      "learning_rate": 1.1761319026432872e-05,
      "loss": 2.2434,
      "step": 77
    },
    {
      "epoch": 0.0020416354023068736,
      "grad_norm": 19.13817024230957,
      "learning_rate": 1.1758178487306988e-05,
      "loss": 2.7628,
      "step": 78
    },
    {
      "epoch": 0.0020678102151569615,
      "grad_norm": 18.389123916625977,
      "learning_rate": 1.1755037948181105e-05,
      "loss": 2.8062,
      "step": 79
    },
    {
      "epoch": 0.00209398502800705,
      "grad_norm": 17.289493560791016,
      "learning_rate": 1.1751897409055221e-05,
      "loss": 3.033,
      "step": 80
    },
    {
      "epoch": 0.0021201598408571378,
      "grad_norm": 18.462589263916016,
      "learning_rate": 1.1748756869929338e-05,
      "loss": 2.6751,
      "step": 81
    },
    {
      "epoch": 0.002146334653707226,
      "grad_norm": 19.505393981933594,
      "learning_rate": 1.1745616330803456e-05,
      "loss": 1.9767,
      "step": 82
    },
    {
      "epoch": 0.002172509466557314,
      "grad_norm": 14.440767288208008,
      "learning_rate": 1.1742475791677572e-05,
      "loss": 2.1662,
      "step": 83
    },
    {
      "epoch": 0.0021986842794074024,
      "grad_norm": 14.605199813842773,
      "learning_rate": 1.173933525255169e-05,
      "loss": 2.809,
      "step": 84
    },
    {
      "epoch": 0.0022248590922574903,
      "grad_norm": 14.445960998535156,
      "learning_rate": 1.1736194713425805e-05,
      "loss": 2.0437,
      "step": 85
    },
    {
      "epoch": 0.0022510339051075787,
      "grad_norm": 19.794342041015625,
      "learning_rate": 1.1733054174299923e-05,
      "loss": 2.2116,
      "step": 86
    },
    {
      "epoch": 0.0022772087179576666,
      "grad_norm": 18.496395111083984,
      "learning_rate": 1.1729913635174039e-05,
      "loss": 2.4008,
      "step": 87
    },
    {
      "epoch": 0.0023033835308077545,
      "grad_norm": 18.180294036865234,
      "learning_rate": 1.1726773096048155e-05,
      "loss": 1.5929,
      "step": 88
    },
    {
      "epoch": 0.002329558343657843,
      "grad_norm": 16.627222061157227,
      "learning_rate": 1.1723632556922272e-05,
      "loss": 3.528,
      "step": 89
    },
    {
      "epoch": 0.002355733156507931,
      "grad_norm": 14.420065879821777,
      "learning_rate": 1.1720492017796388e-05,
      "loss": 1.469,
      "step": 90
    },
    {
      "epoch": 0.002381907969358019,
      "grad_norm": 25.988649368286133,
      "learning_rate": 1.1717351478670506e-05,
      "loss": 2.8033,
      "step": 91
    },
    {
      "epoch": 0.002408082782208107,
      "grad_norm": 24.650802612304688,
      "learning_rate": 1.1714210939544621e-05,
      "loss": 2.6106,
      "step": 92
    },
    {
      "epoch": 0.0024342575950581954,
      "grad_norm": 22.823925018310547,
      "learning_rate": 1.1711070400418739e-05,
      "loss": 3.2885,
      "step": 93
    },
    {
      "epoch": 0.0024604324079082833,
      "grad_norm": 25.272377014160156,
      "learning_rate": 1.1707929861292855e-05,
      "loss": 4.0585,
      "step": 94
    },
    {
      "epoch": 0.0024866072207583717,
      "grad_norm": 19.218107223510742,
      "learning_rate": 1.1704789322166972e-05,
      "loss": 1.5222,
      "step": 95
    },
    {
      "epoch": 0.0025127820336084596,
      "grad_norm": 12.57055377960205,
      "learning_rate": 1.170164878304109e-05,
      "loss": 1.4616,
      "step": 96
    },
    {
      "epoch": 0.002538956846458548,
      "grad_norm": 21.43596076965332,
      "learning_rate": 1.1698508243915206e-05,
      "loss": 3.5691,
      "step": 97
    },
    {
      "epoch": 0.002565131659308636,
      "grad_norm": 15.383261680603027,
      "learning_rate": 1.1695367704789323e-05,
      "loss": 2.7256,
      "step": 98
    },
    {
      "epoch": 0.0025913064721587243,
      "grad_norm": 16.71521759033203,
      "learning_rate": 1.169222716566344e-05,
      "loss": 2.692,
      "step": 99
    },
    {
      "epoch": 0.002617481285008812,
      "grad_norm": 23.095685958862305,
      "learning_rate": 1.1689086626537557e-05,
      "loss": 2.3494,
      "step": 100
    },
    {
      "epoch": 0.0026436560978589,
      "grad_norm": 11.330353736877441,
      "learning_rate": 1.1685946087411673e-05,
      "loss": 0.8398,
      "step": 101
    },
    {
      "epoch": 0.0026698309107089885,
      "grad_norm": 13.569964408874512,
      "learning_rate": 1.1682805548285788e-05,
      "loss": 1.9375,
      "step": 102
    },
    {
      "epoch": 0.0026960057235590764,
      "grad_norm": 20.64599609375,
      "learning_rate": 1.1679665009159906e-05,
      "loss": 3.5732,
      "step": 103
    },
    {
      "epoch": 0.0027221805364091647,
      "grad_norm": 16.05816650390625,
      "learning_rate": 1.1676524470034022e-05,
      "loss": 2.4012,
      "step": 104
    },
    {
      "epoch": 0.0027483553492592527,
      "grad_norm": 21.189777374267578,
      "learning_rate": 1.167338393090814e-05,
      "loss": 2.0351,
      "step": 105
    },
    {
      "epoch": 0.002774530162109341,
      "grad_norm": 14.837639808654785,
      "learning_rate": 1.1670243391782255e-05,
      "loss": 2.0259,
      "step": 106
    },
    {
      "epoch": 0.002800704974959429,
      "grad_norm": 15.209829330444336,
      "learning_rate": 1.1667102852656373e-05,
      "loss": 0.9615,
      "step": 107
    },
    {
      "epoch": 0.0028268797878095173,
      "grad_norm": 13.797417640686035,
      "learning_rate": 1.166396231353049e-05,
      "loss": 2.6921,
      "step": 108
    },
    {
      "epoch": 0.0028530546006596052,
      "grad_norm": 16.877059936523438,
      "learning_rate": 1.1660821774404606e-05,
      "loss": 2.4156,
      "step": 109
    },
    {
      "epoch": 0.0028792294135096936,
      "grad_norm": 15.073802947998047,
      "learning_rate": 1.1657681235278724e-05,
      "loss": 1.5567,
      "step": 110
    },
    {
      "epoch": 0.0029054042263597815,
      "grad_norm": 22.69402503967285,
      "learning_rate": 1.165454069615284e-05,
      "loss": 1.9831,
      "step": 111
    },
    {
      "epoch": 0.00293157903920987,
      "grad_norm": 13.091713905334473,
      "learning_rate": 1.1651400157026957e-05,
      "loss": 1.7704,
      "step": 112
    },
    {
      "epoch": 0.0029577538520599578,
      "grad_norm": 16.164756774902344,
      "learning_rate": 1.1648259617901073e-05,
      "loss": 2.1245,
      "step": 113
    },
    {
      "epoch": 0.0029839286649100457,
      "grad_norm": 11.855355262756348,
      "learning_rate": 1.164511907877519e-05,
      "loss": 1.4096,
      "step": 114
    },
    {
      "epoch": 0.003010103477760134,
      "grad_norm": 17.004735946655273,
      "learning_rate": 1.1641978539649307e-05,
      "loss": 3.1814,
      "step": 115
    },
    {
      "epoch": 0.003036278290610222,
      "grad_norm": 16.72195053100586,
      "learning_rate": 1.1638838000523424e-05,
      "loss": 3.4206,
      "step": 116
    },
    {
      "epoch": 0.0030624531034603103,
      "grad_norm": 16.99317169189453,
      "learning_rate": 1.1635697461397542e-05,
      "loss": 3.1969,
      "step": 117
    },
    {
      "epoch": 0.0030886279163103983,
      "grad_norm": 14.970856666564941,
      "learning_rate": 1.1632556922271656e-05,
      "loss": 1.8483,
      "step": 118
    },
    {
      "epoch": 0.0031148027291604866,
      "grad_norm": 16.661405563354492,
      "learning_rate": 1.1629416383145773e-05,
      "loss": 2.4612,
      "step": 119
    },
    {
      "epoch": 0.0031409775420105745,
      "grad_norm": 15.496171951293945,
      "learning_rate": 1.162627584401989e-05,
      "loss": 2.1451,
      "step": 120
    },
    {
      "epoch": 0.003167152354860663,
      "grad_norm": 14.659793853759766,
      "learning_rate": 1.1623135304894007e-05,
      "loss": 3.0658,
      "step": 121
    },
    {
      "epoch": 0.003193327167710751,
      "grad_norm": 20.552831649780273,
      "learning_rate": 1.1619994765768124e-05,
      "loss": 2.1227,
      "step": 122
    },
    {
      "epoch": 0.003219501980560839,
      "grad_norm": 15.305253028869629,
      "learning_rate": 1.161685422664224e-05,
      "loss": 1.8505,
      "step": 123
    },
    {
      "epoch": 0.003245676793410927,
      "grad_norm": 10.772724151611328,
      "learning_rate": 1.1613713687516358e-05,
      "loss": 0.7333,
      "step": 124
    },
    {
      "epoch": 0.0032718516062610154,
      "grad_norm": 11.5306978225708,
      "learning_rate": 1.1610573148390474e-05,
      "loss": 1.6245,
      "step": 125
    },
    {
      "epoch": 0.0032980264191111034,
      "grad_norm": 15.563087463378906,
      "learning_rate": 1.1607432609264591e-05,
      "loss": 2.5764,
      "step": 126
    },
    {
      "epoch": 0.0033242012319611913,
      "grad_norm": 13.194500923156738,
      "learning_rate": 1.1604292070138707e-05,
      "loss": 3.5995,
      "step": 127
    },
    {
      "epoch": 0.0033503760448112796,
      "grad_norm": 20.081491470336914,
      "learning_rate": 1.1601151531012825e-05,
      "loss": 3.2537,
      "step": 128
    },
    {
      "epoch": 0.0033765508576613676,
      "grad_norm": 10.320838928222656,
      "learning_rate": 1.1598010991886942e-05,
      "loss": 1.1903,
      "step": 129
    },
    {
      "epoch": 0.003402725670511456,
      "grad_norm": 15.799105644226074,
      "learning_rate": 1.1594870452761058e-05,
      "loss": 2.5996,
      "step": 130
    },
    {
      "epoch": 0.003428900483361544,
      "grad_norm": 11.129196166992188,
      "learning_rate": 1.1591729913635176e-05,
      "loss": 1.8434,
      "step": 131
    },
    {
      "epoch": 0.003455075296211632,
      "grad_norm": 20.662139892578125,
      "learning_rate": 1.1588589374509291e-05,
      "loss": 1.8442,
      "step": 132
    },
    {
      "epoch": 0.00348125010906172,
      "grad_norm": 12.548161506652832,
      "learning_rate": 1.1585448835383407e-05,
      "loss": 2.7647,
      "step": 133
    },
    {
      "epoch": 0.0035074249219118085,
      "grad_norm": 15.477767944335938,
      "learning_rate": 1.1582308296257525e-05,
      "loss": 1.2986,
      "step": 134
    },
    {
      "epoch": 0.0035335997347618964,
      "grad_norm": 13.959481239318848,
      "learning_rate": 1.157916775713164e-05,
      "loss": 1.9339,
      "step": 135
    },
    {
      "epoch": 0.0035597745476119848,
      "grad_norm": 11.074966430664062,
      "learning_rate": 1.1576027218005758e-05,
      "loss": 0.7969,
      "step": 136
    },
    {
      "epoch": 0.0035859493604620727,
      "grad_norm": 17.44649314880371,
      "learning_rate": 1.1572886678879874e-05,
      "loss": 3.6024,
      "step": 137
    },
    {
      "epoch": 0.0036121241733121606,
      "grad_norm": 19.50830078125,
      "learning_rate": 1.1569746139753992e-05,
      "loss": 2.5693,
      "step": 138
    },
    {
      "epoch": 0.003638298986162249,
      "grad_norm": 16.172344207763672,
      "learning_rate": 1.1566605600628107e-05,
      "loss": 3.5693,
      "step": 139
    },
    {
      "epoch": 0.003664473799012337,
      "grad_norm": 9.623823165893555,
      "learning_rate": 1.1563465061502225e-05,
      "loss": 1.3644,
      "step": 140
    },
    {
      "epoch": 0.0036906486118624252,
      "grad_norm": 17.274850845336914,
      "learning_rate": 1.1560324522376341e-05,
      "loss": 2.3273,
      "step": 141
    },
    {
      "epoch": 0.003716823424712513,
      "grad_norm": 10.902981758117676,
      "learning_rate": 1.1557183983250458e-05,
      "loss": 0.7059,
      "step": 142
    },
    {
      "epoch": 0.0037429982375626015,
      "grad_norm": 13.711991310119629,
      "learning_rate": 1.1554043444124576e-05,
      "loss": 2.1896,
      "step": 143
    },
    {
      "epoch": 0.0037691730504126894,
      "grad_norm": 12.256510734558105,
      "learning_rate": 1.1550902904998692e-05,
      "loss": 1.0948,
      "step": 144
    },
    {
      "epoch": 0.003795347863262778,
      "grad_norm": 13.120607376098633,
      "learning_rate": 1.154776236587281e-05,
      "loss": 2.105,
      "step": 145
    },
    {
      "epoch": 0.0038215226761128657,
      "grad_norm": 14.109667778015137,
      "learning_rate": 1.1544621826746925e-05,
      "loss": 2.0919,
      "step": 146
    },
    {
      "epoch": 0.003847697488962954,
      "grad_norm": 9.945662498474121,
      "learning_rate": 1.1541481287621043e-05,
      "loss": 2.181,
      "step": 147
    },
    {
      "epoch": 0.003873872301813042,
      "grad_norm": 17.676101684570312,
      "learning_rate": 1.1538340748495159e-05,
      "loss": 2.1376,
      "step": 148
    },
    {
      "epoch": 0.0039000471146631304,
      "grad_norm": 12.687335014343262,
      "learning_rate": 1.1535200209369275e-05,
      "loss": 1.5024,
      "step": 149
    },
    {
      "epoch": 0.003926221927513219,
      "grad_norm": 13.829209327697754,
      "learning_rate": 1.1532059670243392e-05,
      "loss": 1.347,
      "step": 150
    },
    {
      "epoch": 0.003952396740363307,
      "grad_norm": 15.27659797668457,
      "learning_rate": 1.1528919131117508e-05,
      "loss": 3.4027,
      "step": 151
    },
    {
      "epoch": 0.0039785715532133946,
      "grad_norm": 19.26668930053711,
      "learning_rate": 1.1525778591991625e-05,
      "loss": 2.7934,
      "step": 152
    },
    {
      "epoch": 0.0040047463660634825,
      "grad_norm": 12.895267486572266,
      "learning_rate": 1.1522638052865741e-05,
      "loss": 2.0696,
      "step": 153
    },
    {
      "epoch": 0.00403092117891357,
      "grad_norm": 13.362541198730469,
      "learning_rate": 1.1519497513739859e-05,
      "loss": 1.9762,
      "step": 154
    },
    {
      "epoch": 0.004057095991763659,
      "grad_norm": 17.350093841552734,
      "learning_rate": 1.1516356974613976e-05,
      "loss": 3.0854,
      "step": 155
    },
    {
      "epoch": 0.004083270804613747,
      "grad_norm": 11.796960830688477,
      "learning_rate": 1.1513216435488092e-05,
      "loss": 0.8029,
      "step": 156
    },
    {
      "epoch": 0.004109445617463835,
      "grad_norm": 17.074609756469727,
      "learning_rate": 1.151007589636221e-05,
      "loss": 2.0252,
      "step": 157
    },
    {
      "epoch": 0.004135620430313923,
      "grad_norm": 14.722773551940918,
      "learning_rate": 1.1506935357236326e-05,
      "loss": 2.4514,
      "step": 158
    },
    {
      "epoch": 0.004161795243164012,
      "grad_norm": 8.824915885925293,
      "learning_rate": 1.1503794818110443e-05,
      "loss": 1.6378,
      "step": 159
    },
    {
      "epoch": 0.0041879700560141,
      "grad_norm": 9.449216842651367,
      "learning_rate": 1.1500654278984559e-05,
      "loss": 0.9903,
      "step": 160
    },
    {
      "epoch": 0.004214144868864188,
      "grad_norm": 15.854833602905273,
      "learning_rate": 1.1497513739858677e-05,
      "loss": 2.7349,
      "step": 161
    },
    {
      "epoch": 0.0042403196817142755,
      "grad_norm": 11.094951629638672,
      "learning_rate": 1.1494373200732793e-05,
      "loss": 2.4186,
      "step": 162
    },
    {
      "epoch": 0.0042664944945643634,
      "grad_norm": 16.610332489013672,
      "learning_rate": 1.149123266160691e-05,
      "loss": 2.261,
      "step": 163
    },
    {
      "epoch": 0.004292669307414452,
      "grad_norm": 11.434932708740234,
      "learning_rate": 1.1488092122481026e-05,
      "loss": 1.9423,
      "step": 164
    },
    {
      "epoch": 0.00431884412026454,
      "grad_norm": 11.406320571899414,
      "learning_rate": 1.1484951583355142e-05,
      "loss": 1.8591,
      "step": 165
    },
    {
      "epoch": 0.004345018933114628,
      "grad_norm": 17.76449966430664,
      "learning_rate": 1.148181104422926e-05,
      "loss": 2.4188,
      "step": 166
    },
    {
      "epoch": 0.004371193745964716,
      "grad_norm": 13.17193603515625,
      "learning_rate": 1.1478670505103375e-05,
      "loss": 1.3708,
      "step": 167
    },
    {
      "epoch": 0.004397368558814805,
      "grad_norm": 10.870563507080078,
      "learning_rate": 1.1475529965977493e-05,
      "loss": 0.8146,
      "step": 168
    },
    {
      "epoch": 0.004423543371664893,
      "grad_norm": 15.043729782104492,
      "learning_rate": 1.147238942685161e-05,
      "loss": 1.1763,
      "step": 169
    },
    {
      "epoch": 0.004449718184514981,
      "grad_norm": 17.411113739013672,
      "learning_rate": 1.1469248887725726e-05,
      "loss": 2.6194,
      "step": 170
    },
    {
      "epoch": 0.0044758929973650685,
      "grad_norm": 17.68866729736328,
      "learning_rate": 1.1466108348599844e-05,
      "loss": 2.6683,
      "step": 171
    },
    {
      "epoch": 0.004502067810215157,
      "grad_norm": 14.316873550415039,
      "learning_rate": 1.146296780947396e-05,
      "loss": 1.4204,
      "step": 172
    },
    {
      "epoch": 0.004528242623065245,
      "grad_norm": 16.87318992614746,
      "learning_rate": 1.1459827270348077e-05,
      "loss": 2.0249,
      "step": 173
    },
    {
      "epoch": 0.004554417435915333,
      "grad_norm": 14.415238380432129,
      "learning_rate": 1.1456686731222193e-05,
      "loss": 1.6838,
      "step": 174
    },
    {
      "epoch": 0.004580592248765421,
      "grad_norm": 12.448020935058594,
      "learning_rate": 1.145354619209631e-05,
      "loss": 2.0115,
      "step": 175
    },
    {
      "epoch": 0.004606767061615509,
      "grad_norm": 19.25944709777832,
      "learning_rate": 1.1450405652970428e-05,
      "loss": 1.3213,
      "step": 176
    },
    {
      "epoch": 0.004632941874465598,
      "grad_norm": 21.150325775146484,
      "learning_rate": 1.1447265113844544e-05,
      "loss": 1.7329,
      "step": 177
    },
    {
      "epoch": 0.004659116687315686,
      "grad_norm": 12.001667976379395,
      "learning_rate": 1.1444124574718662e-05,
      "loss": 1.2908,
      "step": 178
    },
    {
      "epoch": 0.004685291500165774,
      "grad_norm": 11.877140045166016,
      "learning_rate": 1.1440984035592776e-05,
      "loss": 0.9327,
      "step": 179
    },
    {
      "epoch": 0.004711466313015862,
      "grad_norm": 16.84221649169922,
      "learning_rate": 1.1437843496466893e-05,
      "loss": 2.0072,
      "step": 180
    },
    {
      "epoch": 0.00473764112586595,
      "grad_norm": 14.974508285522461,
      "learning_rate": 1.143470295734101e-05,
      "loss": 2.6614,
      "step": 181
    },
    {
      "epoch": 0.004763815938716038,
      "grad_norm": 14.474237442016602,
      "learning_rate": 1.1431562418215127e-05,
      "loss": 1.7737,
      "step": 182
    },
    {
      "epoch": 0.004789990751566126,
      "grad_norm": 11.452691078186035,
      "learning_rate": 1.1428421879089244e-05,
      "loss": 0.6434,
      "step": 183
    },
    {
      "epoch": 0.004816165564416214,
      "grad_norm": 18.34880256652832,
      "learning_rate": 1.142528133996336e-05,
      "loss": 1.1745,
      "step": 184
    },
    {
      "epoch": 0.004842340377266303,
      "grad_norm": 16.266033172607422,
      "learning_rate": 1.1422140800837478e-05,
      "loss": 1.9531,
      "step": 185
    },
    {
      "epoch": 0.004868515190116391,
      "grad_norm": 13.216902732849121,
      "learning_rate": 1.1419000261711593e-05,
      "loss": 1.1746,
      "step": 186
    },
    {
      "epoch": 0.004894690002966479,
      "grad_norm": 15.451672554016113,
      "learning_rate": 1.1415859722585711e-05,
      "loss": 2.0025,
      "step": 187
    },
    {
      "epoch": 0.004920864815816567,
      "grad_norm": 27.117507934570312,
      "learning_rate": 1.1412719183459827e-05,
      "loss": 1.8598,
      "step": 188
    },
    {
      "epoch": 0.004947039628666655,
      "grad_norm": 12.628796577453613,
      "learning_rate": 1.1409578644333944e-05,
      "loss": 1.6837,
      "step": 189
    },
    {
      "epoch": 0.004973214441516743,
      "grad_norm": 9.43991756439209,
      "learning_rate": 1.1406438105208062e-05,
      "loss": 1.1251,
      "step": 190
    },
    {
      "epoch": 0.004999389254366831,
      "grad_norm": 15.370447158813477,
      "learning_rate": 1.1403297566082178e-05,
      "loss": 1.5517,
      "step": 191
    },
    {
      "epoch": 0.005025564067216919,
      "grad_norm": 16.280874252319336,
      "learning_rate": 1.1400157026956295e-05,
      "loss": 1.4637,
      "step": 192
    },
    {
      "epoch": 0.005051738880067007,
      "grad_norm": 13.504317283630371,
      "learning_rate": 1.1397016487830411e-05,
      "loss": 2.3779,
      "step": 193
    },
    {
      "epoch": 0.005077913692917096,
      "grad_norm": 16.56597328186035,
      "learning_rate": 1.1393875948704529e-05,
      "loss": 1.8118,
      "step": 194
    },
    {
      "epoch": 0.005104088505767184,
      "grad_norm": 11.969481468200684,
      "learning_rate": 1.1390735409578645e-05,
      "loss": 1.333,
      "step": 195
    },
    {
      "epoch": 0.005130263318617272,
      "grad_norm": 15.130587577819824,
      "learning_rate": 1.138759487045276e-05,
      "loss": 2.3335,
      "step": 196
    },
    {
      "epoch": 0.00515643813146736,
      "grad_norm": 12.055906295776367,
      "learning_rate": 1.1384454331326878e-05,
      "loss": 1.386,
      "step": 197
    },
    {
      "epoch": 0.0051826129443174485,
      "grad_norm": 13.507920265197754,
      "learning_rate": 1.1381313792200994e-05,
      "loss": 2.3502,
      "step": 198
    },
    {
      "epoch": 0.0052087877571675364,
      "grad_norm": 13.850314140319824,
      "learning_rate": 1.1378173253075112e-05,
      "loss": 2.3893,
      "step": 199
    },
    {
      "epoch": 0.005234962570017624,
      "grad_norm": 12.840747833251953,
      "learning_rate": 1.1375032713949227e-05,
      "loss": 2.0314,
      "step": 200
    },
    {
      "epoch": 0.005261137382867712,
      "grad_norm": 13.096674919128418,
      "learning_rate": 1.1371892174823345e-05,
      "loss": 1.7229,
      "step": 201
    },
    {
      "epoch": 0.0052873121957178,
      "grad_norm": 15.769506454467773,
      "learning_rate": 1.1368751635697462e-05,
      "loss": 1.7678,
      "step": 202
    },
    {
      "epoch": 0.005313487008567889,
      "grad_norm": 13.398155212402344,
      "learning_rate": 1.1365611096571578e-05,
      "loss": 0.7023,
      "step": 203
    },
    {
      "epoch": 0.005339661821417977,
      "grad_norm": 14.75139045715332,
      "learning_rate": 1.1362470557445696e-05,
      "loss": 1.9156,
      "step": 204
    },
    {
      "epoch": 0.005365836634268065,
      "grad_norm": 14.622982025146484,
      "learning_rate": 1.1359330018319812e-05,
      "loss": 1.3304,
      "step": 205
    },
    {
      "epoch": 0.005392011447118153,
      "grad_norm": 15.189102172851562,
      "learning_rate": 1.135618947919393e-05,
      "loss": 0.9287,
      "step": 206
    },
    {
      "epoch": 0.0054181862599682416,
      "grad_norm": 19.454303741455078,
      "learning_rate": 1.1353048940068045e-05,
      "loss": 1.6403,
      "step": 207
    },
    {
      "epoch": 0.0054443610728183295,
      "grad_norm": 17.01598358154297,
      "learning_rate": 1.1349908400942163e-05,
      "loss": 2.7376,
      "step": 208
    },
    {
      "epoch": 0.005470535885668417,
      "grad_norm": 15.76163101196289,
      "learning_rate": 1.1346767861816279e-05,
      "loss": 2.3374,
      "step": 209
    },
    {
      "epoch": 0.005496710698518505,
      "grad_norm": 14.11268424987793,
      "learning_rate": 1.1343627322690394e-05,
      "loss": 1.655,
      "step": 210
    },
    {
      "epoch": 0.005522885511368594,
      "grad_norm": 20.920007705688477,
      "learning_rate": 1.1340486783564512e-05,
      "loss": 0.767,
      "step": 211
    },
    {
      "epoch": 0.005549060324218682,
      "grad_norm": 9.898805618286133,
      "learning_rate": 1.1337346244438628e-05,
      "loss": 1.6528,
      "step": 212
    },
    {
      "epoch": 0.00557523513706877,
      "grad_norm": 12.516654014587402,
      "learning_rate": 1.1334205705312745e-05,
      "loss": 1.4345,
      "step": 213
    },
    {
      "epoch": 0.005601409949918858,
      "grad_norm": 18.73650550842285,
      "learning_rate": 1.1331065166186861e-05,
      "loss": 1.4362,
      "step": 214
    },
    {
      "epoch": 0.005627584762768946,
      "grad_norm": 15.803741455078125,
      "learning_rate": 1.1327924627060979e-05,
      "loss": 1.7166,
      "step": 215
    },
    {
      "epoch": 0.005653759575619035,
      "grad_norm": 14.517212867736816,
      "learning_rate": 1.1324784087935096e-05,
      "loss": 1.7294,
      "step": 216
    },
    {
      "epoch": 0.0056799343884691225,
      "grad_norm": 16.518701553344727,
      "learning_rate": 1.1321643548809212e-05,
      "loss": 1.2964,
      "step": 217
    },
    {
      "epoch": 0.0057061092013192104,
      "grad_norm": 13.22188663482666,
      "learning_rate": 1.131850300968333e-05,
      "loss": 1.4885,
      "step": 218
    },
    {
      "epoch": 0.005732284014169298,
      "grad_norm": 12.286432266235352,
      "learning_rate": 1.1315362470557446e-05,
      "loss": 1.4088,
      "step": 219
    },
    {
      "epoch": 0.005758458827019387,
      "grad_norm": 11.964247703552246,
      "learning_rate": 1.1312221931431563e-05,
      "loss": 0.9841,
      "step": 220
    },
    {
      "epoch": 0.005784633639869475,
      "grad_norm": 15.483153343200684,
      "learning_rate": 1.1309081392305679e-05,
      "loss": 1.1353,
      "step": 221
    },
    {
      "epoch": 0.005810808452719563,
      "grad_norm": 13.376920700073242,
      "learning_rate": 1.1305940853179797e-05,
      "loss": 1.7308,
      "step": 222
    },
    {
      "epoch": 0.005836983265569651,
      "grad_norm": 15.010568618774414,
      "learning_rate": 1.1302800314053914e-05,
      "loss": 1.1283,
      "step": 223
    },
    {
      "epoch": 0.00586315807841974,
      "grad_norm": 15.958560943603516,
      "learning_rate": 1.129965977492803e-05,
      "loss": 1.5827,
      "step": 224
    },
    {
      "epoch": 0.005889332891269828,
      "grad_norm": 16.578712463378906,
      "learning_rate": 1.1296519235802148e-05,
      "loss": 1.458,
      "step": 225
    },
    {
      "epoch": 0.0059155077041199156,
      "grad_norm": 15.302216529846191,
      "learning_rate": 1.1293378696676262e-05,
      "loss": 1.1604,
      "step": 226
    },
    {
      "epoch": 0.0059416825169700035,
      "grad_norm": 15.29163646697998,
      "learning_rate": 1.129023815755038e-05,
      "loss": 0.7297,
      "step": 227
    },
    {
      "epoch": 0.005967857329820091,
      "grad_norm": 12.139481544494629,
      "learning_rate": 1.1287097618424497e-05,
      "loss": 0.9888,
      "step": 228
    },
    {
      "epoch": 0.00599403214267018,
      "grad_norm": 19.516498565673828,
      "learning_rate": 1.1283957079298613e-05,
      "loss": 2.5996,
      "step": 229
    },
    {
      "epoch": 0.006020206955520268,
      "grad_norm": 12.15183162689209,
      "learning_rate": 1.128081654017273e-05,
      "loss": 2.5289,
      "step": 230
    },
    {
      "epoch": 0.006046381768370356,
      "grad_norm": 12.720767974853516,
      "learning_rate": 1.1277676001046846e-05,
      "loss": 0.967,
      "step": 231
    },
    {
      "epoch": 0.006072556581220444,
      "grad_norm": 14.053345680236816,
      "learning_rate": 1.1274535461920964e-05,
      "loss": 1.1607,
      "step": 232
    },
    {
      "epoch": 0.006098731394070533,
      "grad_norm": 12.142253875732422,
      "learning_rate": 1.127139492279508e-05,
      "loss": 1.0286,
      "step": 233
    },
    {
      "epoch": 0.006124906206920621,
      "grad_norm": 13.495305061340332,
      "learning_rate": 1.1268254383669197e-05,
      "loss": 1.8786,
      "step": 234
    },
    {
      "epoch": 0.006151081019770709,
      "grad_norm": 15.427663803100586,
      "learning_rate": 1.1265113844543313e-05,
      "loss": 1.2116,
      "step": 235
    },
    {
      "epoch": 0.0061772558326207965,
      "grad_norm": 16.25979995727539,
      "learning_rate": 1.126197330541743e-05,
      "loss": 2.0223,
      "step": 236
    },
    {
      "epoch": 0.006203430645470885,
      "grad_norm": 16.369291305541992,
      "learning_rate": 1.1258832766291548e-05,
      "loss": 3.3264,
      "step": 237
    },
    {
      "epoch": 0.006229605458320973,
      "grad_norm": 10.123537063598633,
      "learning_rate": 1.1255692227165664e-05,
      "loss": 1.6257,
      "step": 238
    },
    {
      "epoch": 0.006255780271171061,
      "grad_norm": 12.045762062072754,
      "learning_rate": 1.1252551688039781e-05,
      "loss": 1.8353,
      "step": 239
    },
    {
      "epoch": 0.006281955084021149,
      "grad_norm": 12.249852180480957,
      "learning_rate": 1.1249411148913897e-05,
      "loss": 1.5886,
      "step": 240
    },
    {
      "epoch": 0.006308129896871237,
      "grad_norm": 21.565555572509766,
      "learning_rate": 1.1246270609788013e-05,
      "loss": 1.772,
      "step": 241
    },
    {
      "epoch": 0.006334304709721326,
      "grad_norm": 14.761540412902832,
      "learning_rate": 1.124313007066213e-05,
      "loss": 2.3544,
      "step": 242
    },
    {
      "epoch": 0.006360479522571414,
      "grad_norm": 14.064984321594238,
      "learning_rate": 1.1239989531536247e-05,
      "loss": 0.4517,
      "step": 243
    },
    {
      "epoch": 0.006386654335421502,
      "grad_norm": 15.47919750213623,
      "learning_rate": 1.1236848992410364e-05,
      "loss": 1.3596,
      "step": 244
    },
    {
      "epoch": 0.0064128291482715895,
      "grad_norm": 15.840648651123047,
      "learning_rate": 1.123370845328448e-05,
      "loss": 1.7255,
      "step": 245
    },
    {
      "epoch": 0.006439003961121678,
      "grad_norm": 13.832386016845703,
      "learning_rate": 1.1230567914158598e-05,
      "loss": 2.2063,
      "step": 246
    },
    {
      "epoch": 0.006465178773971766,
      "grad_norm": 14.32754135131836,
      "learning_rate": 1.1227427375032713e-05,
      "loss": 1.3076,
      "step": 247
    },
    {
      "epoch": 0.006491353586821854,
      "grad_norm": 13.14931869506836,
      "learning_rate": 1.1224286835906831e-05,
      "loss": 2.1726,
      "step": 248
    },
    {
      "epoch": 0.006517528399671942,
      "grad_norm": 18.986228942871094,
      "learning_rate": 1.1221146296780949e-05,
      "loss": 1.9514,
      "step": 249
    },
    {
      "epoch": 0.006543703212522031,
      "grad_norm": 18.050270080566406,
      "learning_rate": 1.1218005757655064e-05,
      "loss": 2.3808,
      "step": 250
    },
    {
      "epoch": 0.006569878025372119,
      "grad_norm": 12.211296081542969,
      "learning_rate": 1.1214865218529182e-05,
      "loss": 2.079,
      "step": 251
    },
    {
      "epoch": 0.006596052838222207,
      "grad_norm": 22.59772491455078,
      "learning_rate": 1.1211724679403298e-05,
      "loss": 1.7225,
      "step": 252
    },
    {
      "epoch": 0.006622227651072295,
      "grad_norm": 16.774682998657227,
      "learning_rate": 1.1208584140277415e-05,
      "loss": 1.1822,
      "step": 253
    },
    {
      "epoch": 0.006648402463922383,
      "grad_norm": 14.39283275604248,
      "learning_rate": 1.1205443601151531e-05,
      "loss": 3.0094,
      "step": 254
    },
    {
      "epoch": 0.006674577276772471,
      "grad_norm": 12.797060012817383,
      "learning_rate": 1.1202303062025649e-05,
      "loss": 1.1131,
      "step": 255
    },
    {
      "epoch": 0.006700752089622559,
      "grad_norm": 11.716316223144531,
      "learning_rate": 1.1199162522899765e-05,
      "loss": 0.7453,
      "step": 256
    },
    {
      "epoch": 0.006726926902472647,
      "grad_norm": 17.063180923461914,
      "learning_rate": 1.119602198377388e-05,
      "loss": 2.3483,
      "step": 257
    },
    {
      "epoch": 0.006753101715322735,
      "grad_norm": 11.267877578735352,
      "learning_rate": 1.1192881444647998e-05,
      "loss": 1.8233,
      "step": 258
    },
    {
      "epoch": 0.006779276528172824,
      "grad_norm": 10.438720703125,
      "learning_rate": 1.1189740905522114e-05,
      "loss": 1.3407,
      "step": 259
    },
    {
      "epoch": 0.006805451341022912,
      "grad_norm": 13.037179946899414,
      "learning_rate": 1.1186600366396231e-05,
      "loss": 1.2294,
      "step": 260
    },
    {
      "epoch": 0.006831626153873,
      "grad_norm": 14.77631664276123,
      "learning_rate": 1.1183459827270347e-05,
      "loss": 2.1465,
      "step": 261
    },
    {
      "epoch": 0.006857800966723088,
      "grad_norm": 12.245406150817871,
      "learning_rate": 1.1180319288144465e-05,
      "loss": 0.8685,
      "step": 262
    },
    {
      "epoch": 0.006883975779573176,
      "grad_norm": 17.907636642456055,
      "learning_rate": 1.1177178749018582e-05,
      "loss": 2.0669,
      "step": 263
    },
    {
      "epoch": 0.006910150592423264,
      "grad_norm": 13.983388900756836,
      "learning_rate": 1.1174038209892698e-05,
      "loss": 1.9092,
      "step": 264
    },
    {
      "epoch": 0.006936325405273352,
      "grad_norm": 15.62343692779541,
      "learning_rate": 1.1170897670766816e-05,
      "loss": 1.1995,
      "step": 265
    },
    {
      "epoch": 0.00696250021812344,
      "grad_norm": 9.424201011657715,
      "learning_rate": 1.1167757131640932e-05,
      "loss": 1.1862,
      "step": 266
    },
    {
      "epoch": 0.006988675030973528,
      "grad_norm": 14.142909049987793,
      "learning_rate": 1.116461659251505e-05,
      "loss": 1.8591,
      "step": 267
    },
    {
      "epoch": 0.007014849843823617,
      "grad_norm": 12.401169776916504,
      "learning_rate": 1.1161476053389165e-05,
      "loss": 1.1515,
      "step": 268
    },
    {
      "epoch": 0.007041024656673705,
      "grad_norm": 18.058483123779297,
      "learning_rate": 1.1158335514263283e-05,
      "loss": 1.1207,
      "step": 269
    },
    {
      "epoch": 0.007067199469523793,
      "grad_norm": 20.262357711791992,
      "learning_rate": 1.11551949751374e-05,
      "loss": 2.8084,
      "step": 270
    },
    {
      "epoch": 0.007093374282373881,
      "grad_norm": 14.646797180175781,
      "learning_rate": 1.1152054436011516e-05,
      "loss": 1.5655,
      "step": 271
    },
    {
      "epoch": 0.0071195490952239695,
      "grad_norm": 13.775413513183594,
      "learning_rate": 1.1148913896885632e-05,
      "loss": 1.8068,
      "step": 272
    },
    {
      "epoch": 0.0071457239080740574,
      "grad_norm": 14.789746284484863,
      "learning_rate": 1.1145773357759748e-05,
      "loss": 1.3959,
      "step": 273
    },
    {
      "epoch": 0.007171898720924145,
      "grad_norm": 18.808486938476562,
      "learning_rate": 1.1142632818633865e-05,
      "loss": 1.3654,
      "step": 274
    },
    {
      "epoch": 0.007198073533774233,
      "grad_norm": 12.5117826461792,
      "learning_rate": 1.1139492279507983e-05,
      "loss": 1.236,
      "step": 275
    },
    {
      "epoch": 0.007224248346624321,
      "grad_norm": 14.488447189331055,
      "learning_rate": 1.1136351740382099e-05,
      "loss": 2.2505,
      "step": 276
    },
    {
      "epoch": 0.00725042315947441,
      "grad_norm": 14.731329917907715,
      "learning_rate": 1.1133211201256216e-05,
      "loss": 2.1046,
      "step": 277
    },
    {
      "epoch": 0.007276597972324498,
      "grad_norm": 17.36949920654297,
      "learning_rate": 1.1130070662130332e-05,
      "loss": 1.9663,
      "step": 278
    },
    {
      "epoch": 0.007302772785174586,
      "grad_norm": 15.755168914794922,
      "learning_rate": 1.112693012300445e-05,
      "loss": 1.1911,
      "step": 279
    },
    {
      "epoch": 0.007328947598024674,
      "grad_norm": 19.237802505493164,
      "learning_rate": 1.1123789583878566e-05,
      "loss": 0.9595,
      "step": 280
    },
    {
      "epoch": 0.0073551224108747626,
      "grad_norm": 15.116825103759766,
      "learning_rate": 1.1120649044752683e-05,
      "loss": 1.5026,
      "step": 281
    },
    {
      "epoch": 0.0073812972237248505,
      "grad_norm": 13.805624008178711,
      "learning_rate": 1.1117508505626799e-05,
      "loss": 2.2041,
      "step": 282
    },
    {
      "epoch": 0.007407472036574938,
      "grad_norm": 23.11675453186035,
      "learning_rate": 1.1114367966500917e-05,
      "loss": 2.1789,
      "step": 283
    },
    {
      "epoch": 0.007433646849425026,
      "grad_norm": 18.663558959960938,
      "learning_rate": 1.1111227427375034e-05,
      "loss": 1.7239,
      "step": 284
    },
    {
      "epoch": 0.007459821662275115,
      "grad_norm": 10.9806547164917,
      "learning_rate": 1.110808688824915e-05,
      "loss": 0.6113,
      "step": 285
    },
    {
      "epoch": 0.007485996475125203,
      "grad_norm": 10.12496566772461,
      "learning_rate": 1.1104946349123268e-05,
      "loss": 1.1653,
      "step": 286
    },
    {
      "epoch": 0.007512171287975291,
      "grad_norm": 13.140800476074219,
      "learning_rate": 1.1101805809997383e-05,
      "loss": 0.5907,
      "step": 287
    },
    {
      "epoch": 0.007538346100825379,
      "grad_norm": 19.366743087768555,
      "learning_rate": 1.10986652708715e-05,
      "loss": 0.9592,
      "step": 288
    },
    {
      "epoch": 0.007564520913675467,
      "grad_norm": 12.549296379089355,
      "learning_rate": 1.1095524731745617e-05,
      "loss": 2.4665,
      "step": 289
    },
    {
      "epoch": 0.007590695726525556,
      "grad_norm": 16.548173904418945,
      "learning_rate": 1.1092384192619733e-05,
      "loss": 1.3756,
      "step": 290
    },
    {
      "epoch": 0.0076168705393756435,
      "grad_norm": 12.961524963378906,
      "learning_rate": 1.108924365349385e-05,
      "loss": 1.2124,
      "step": 291
    },
    {
      "epoch": 0.0076430453522257314,
      "grad_norm": 17.963848114013672,
      "learning_rate": 1.1086103114367966e-05,
      "loss": 2.061,
      "step": 292
    },
    {
      "epoch": 0.007669220165075819,
      "grad_norm": 19.636781692504883,
      "learning_rate": 1.1082962575242084e-05,
      "loss": 1.3615,
      "step": 293
    },
    {
      "epoch": 0.007695394977925908,
      "grad_norm": 15.720267295837402,
      "learning_rate": 1.10798220361162e-05,
      "loss": 1.5583,
      "step": 294
    },
    {
      "epoch": 0.007721569790775996,
      "grad_norm": 15.742332458496094,
      "learning_rate": 1.1076681496990317e-05,
      "loss": 1.7538,
      "step": 295
    },
    {
      "epoch": 0.007747744603626084,
      "grad_norm": 19.154739379882812,
      "learning_rate": 1.1073540957864435e-05,
      "loss": 1.7403,
      "step": 296
    },
    {
      "epoch": 0.007773919416476172,
      "grad_norm": 23.00737953186035,
      "learning_rate": 1.107040041873855e-05,
      "loss": 1.6915,
      "step": 297
    },
    {
      "epoch": 0.007800094229326261,
      "grad_norm": 15.868215560913086,
      "learning_rate": 1.1067259879612668e-05,
      "loss": 1.1833,
      "step": 298
    },
    {
      "epoch": 0.007826269042176348,
      "grad_norm": 22.073097229003906,
      "learning_rate": 1.1064119340486784e-05,
      "loss": 1.1888,
      "step": 299
    },
    {
      "epoch": 0.007852443855026437,
      "grad_norm": 11.909801483154297,
      "learning_rate": 1.1060978801360901e-05,
      "loss": 1.605,
      "step": 300
    },
    {
      "epoch": 0.007878618667876525,
      "grad_norm": 16.42983627319336,
      "learning_rate": 1.1057838262235017e-05,
      "loss": 1.3688,
      "step": 301
    },
    {
      "epoch": 0.007904793480726613,
      "grad_norm": 19.567707061767578,
      "learning_rate": 1.1054697723109135e-05,
      "loss": 1.2214,
      "step": 302
    },
    {
      "epoch": 0.007930968293576701,
      "grad_norm": 11.74425220489502,
      "learning_rate": 1.105155718398325e-05,
      "loss": 0.7944,
      "step": 303
    },
    {
      "epoch": 0.007957143106426789,
      "grad_norm": 14.515366554260254,
      "learning_rate": 1.1048416644857367e-05,
      "loss": 1.0281,
      "step": 304
    },
    {
      "epoch": 0.007983317919276877,
      "grad_norm": 20.552743911743164,
      "learning_rate": 1.1045276105731484e-05,
      "loss": 1.5882,
      "step": 305
    },
    {
      "epoch": 0.008009492732126965,
      "grad_norm": 21.663604736328125,
      "learning_rate": 1.10421355666056e-05,
      "loss": 1.8039,
      "step": 306
    },
    {
      "epoch": 0.008035667544977053,
      "grad_norm": 20.33736228942871,
      "learning_rate": 1.1038995027479717e-05,
      "loss": 1.1212,
      "step": 307
    },
    {
      "epoch": 0.00806184235782714,
      "grad_norm": 14.70441722869873,
      "learning_rate": 1.1035854488353833e-05,
      "loss": 2.4091,
      "step": 308
    },
    {
      "epoch": 0.00808801717067723,
      "grad_norm": 14.672015190124512,
      "learning_rate": 1.1032713949227951e-05,
      "loss": 2.106,
      "step": 309
    },
    {
      "epoch": 0.008114191983527318,
      "grad_norm": 14.566289901733398,
      "learning_rate": 1.1029573410102068e-05,
      "loss": 2.0872,
      "step": 310
    },
    {
      "epoch": 0.008140366796377406,
      "grad_norm": 15.018850326538086,
      "learning_rate": 1.1026432870976184e-05,
      "loss": 2.0676,
      "step": 311
    },
    {
      "epoch": 0.008166541609227494,
      "grad_norm": 18.827930450439453,
      "learning_rate": 1.1023292331850302e-05,
      "loss": 1.7748,
      "step": 312
    },
    {
      "epoch": 0.008192716422077582,
      "grad_norm": 14.074217796325684,
      "learning_rate": 1.1020151792724418e-05,
      "loss": 2.1583,
      "step": 313
    },
    {
      "epoch": 0.00821889123492767,
      "grad_norm": 10.189136505126953,
      "learning_rate": 1.1017011253598535e-05,
      "loss": 1.6949,
      "step": 314
    },
    {
      "epoch": 0.008245066047777758,
      "grad_norm": 18.352317810058594,
      "learning_rate": 1.1013870714472651e-05,
      "loss": 0.4191,
      "step": 315
    },
    {
      "epoch": 0.008271240860627846,
      "grad_norm": 15.7222318649292,
      "learning_rate": 1.1010730175346769e-05,
      "loss": 0.5871,
      "step": 316
    },
    {
      "epoch": 0.008297415673477934,
      "grad_norm": 13.767792701721191,
      "learning_rate": 1.1007589636220886e-05,
      "loss": 1.2207,
      "step": 317
    },
    {
      "epoch": 0.008323590486328023,
      "grad_norm": 12.374411582946777,
      "learning_rate": 1.1004449097095002e-05,
      "loss": 1.0025,
      "step": 318
    },
    {
      "epoch": 0.008349765299178111,
      "grad_norm": 13.591221809387207,
      "learning_rate": 1.1001308557969118e-05,
      "loss": 1.5102,
      "step": 319
    },
    {
      "epoch": 0.0083759401120282,
      "grad_norm": 16.649709701538086,
      "learning_rate": 1.0998168018843234e-05,
      "loss": 0.6206,
      "step": 320
    },
    {
      "epoch": 0.008402114924878287,
      "grad_norm": 11.719310760498047,
      "learning_rate": 1.0995027479717351e-05,
      "loss": 1.6413,
      "step": 321
    },
    {
      "epoch": 0.008428289737728375,
      "grad_norm": 16.285432815551758,
      "learning_rate": 1.0991886940591469e-05,
      "loss": 2.1585,
      "step": 322
    },
    {
      "epoch": 0.008454464550578463,
      "grad_norm": 16.335044860839844,
      "learning_rate": 1.0988746401465585e-05,
      "loss": 1.5047,
      "step": 323
    },
    {
      "epoch": 0.008480639363428551,
      "grad_norm": 30.429353713989258,
      "learning_rate": 1.0985605862339702e-05,
      "loss": 0.864,
      "step": 324
    },
    {
      "epoch": 0.008506814176278639,
      "grad_norm": 22.31454086303711,
      "learning_rate": 1.0982465323213818e-05,
      "loss": 1.1125,
      "step": 325
    },
    {
      "epoch": 0.008532988989128727,
      "grad_norm": 17.5647029876709,
      "learning_rate": 1.0979324784087936e-05,
      "loss": 1.7887,
      "step": 326
    },
    {
      "epoch": 0.008559163801978817,
      "grad_norm": 15.903203964233398,
      "learning_rate": 1.0976184244962052e-05,
      "loss": 1.343,
      "step": 327
    },
    {
      "epoch": 0.008585338614828904,
      "grad_norm": 18.91958999633789,
      "learning_rate": 1.097304370583617e-05,
      "loss": 1.2594,
      "step": 328
    },
    {
      "epoch": 0.008611513427678992,
      "grad_norm": 14.548200607299805,
      "learning_rate": 1.0969903166710285e-05,
      "loss": 0.9094,
      "step": 329
    },
    {
      "epoch": 0.00863768824052908,
      "grad_norm": 11.368436813354492,
      "learning_rate": 1.0966762627584403e-05,
      "loss": 1.8223,
      "step": 330
    },
    {
      "epoch": 0.008663863053379168,
      "grad_norm": 11.939844131469727,
      "learning_rate": 1.096362208845852e-05,
      "loss": 0.6597,
      "step": 331
    },
    {
      "epoch": 0.008690037866229256,
      "grad_norm": 17.506649017333984,
      "learning_rate": 1.0960481549332636e-05,
      "loss": 1.6236,
      "step": 332
    },
    {
      "epoch": 0.008716212679079344,
      "grad_norm": 17.037145614624023,
      "learning_rate": 1.0957341010206754e-05,
      "loss": 2.3782,
      "step": 333
    },
    {
      "epoch": 0.008742387491929432,
      "grad_norm": 20.199565887451172,
      "learning_rate": 1.0954200471080868e-05,
      "loss": 0.6111,
      "step": 334
    },
    {
      "epoch": 0.008768562304779522,
      "grad_norm": 15.771493911743164,
      "learning_rate": 1.0951059931954985e-05,
      "loss": 1.3612,
      "step": 335
    },
    {
      "epoch": 0.00879473711762961,
      "grad_norm": 13.405550956726074,
      "learning_rate": 1.0947919392829103e-05,
      "loss": 2.2104,
      "step": 336
    },
    {
      "epoch": 0.008820911930479697,
      "grad_norm": 13.113595008850098,
      "learning_rate": 1.0944778853703219e-05,
      "loss": 1.4872,
      "step": 337
    },
    {
      "epoch": 0.008847086743329785,
      "grad_norm": 13.193557739257812,
      "learning_rate": 1.0941638314577336e-05,
      "loss": 1.818,
      "step": 338
    },
    {
      "epoch": 0.008873261556179873,
      "grad_norm": 22.578781127929688,
      "learning_rate": 1.0938497775451452e-05,
      "loss": 1.2875,
      "step": 339
    },
    {
      "epoch": 0.008899436369029961,
      "grad_norm": 10.925480842590332,
      "learning_rate": 1.093535723632557e-05,
      "loss": 1.06,
      "step": 340
    },
    {
      "epoch": 0.00892561118188005,
      "grad_norm": 16.95890998840332,
      "learning_rate": 1.0932216697199686e-05,
      "loss": 2.0116,
      "step": 341
    },
    {
      "epoch": 0.008951785994730137,
      "grad_norm": 20.626190185546875,
      "learning_rate": 1.0929076158073803e-05,
      "loss": 1.916,
      "step": 342
    },
    {
      "epoch": 0.008977960807580225,
      "grad_norm": 14.161541938781738,
      "learning_rate": 1.092593561894792e-05,
      "loss": 1.8118,
      "step": 343
    },
    {
      "epoch": 0.009004135620430315,
      "grad_norm": 17.72511100769043,
      "learning_rate": 1.0922795079822036e-05,
      "loss": 2.0665,
      "step": 344
    },
    {
      "epoch": 0.009030310433280403,
      "grad_norm": 17.07662582397461,
      "learning_rate": 1.0919654540696154e-05,
      "loss": 1.3605,
      "step": 345
    },
    {
      "epoch": 0.00905648524613049,
      "grad_norm": 19.506410598754883,
      "learning_rate": 1.091651400157027e-05,
      "loss": 0.4042,
      "step": 346
    },
    {
      "epoch": 0.009082660058980578,
      "grad_norm": 14.735677719116211,
      "learning_rate": 1.0913373462444387e-05,
      "loss": 1.6496,
      "step": 347
    },
    {
      "epoch": 0.009108834871830666,
      "grad_norm": 16.169328689575195,
      "learning_rate": 1.0910232923318503e-05,
      "loss": 2.5778,
      "step": 348
    },
    {
      "epoch": 0.009135009684680754,
      "grad_norm": 18.068321228027344,
      "learning_rate": 1.090709238419262e-05,
      "loss": 1.8639,
      "step": 349
    },
    {
      "epoch": 0.009161184497530842,
      "grad_norm": 25.43743324279785,
      "learning_rate": 1.0903951845066737e-05,
      "loss": 1.4617,
      "step": 350
    },
    {
      "epoch": 0.00918735931038093,
      "grad_norm": 15.793428421020508,
      "learning_rate": 1.0900811305940853e-05,
      "loss": 1.403,
      "step": 351
    },
    {
      "epoch": 0.009213534123231018,
      "grad_norm": 17.325904846191406,
      "learning_rate": 1.089767076681497e-05,
      "loss": 2.0604,
      "step": 352
    },
    {
      "epoch": 0.009239708936081108,
      "grad_norm": 15.80048942565918,
      "learning_rate": 1.0894530227689086e-05,
      "loss": 2.7016,
      "step": 353
    },
    {
      "epoch": 0.009265883748931196,
      "grad_norm": 10.43928337097168,
      "learning_rate": 1.0891389688563204e-05,
      "loss": 2.0932,
      "step": 354
    },
    {
      "epoch": 0.009292058561781284,
      "grad_norm": 11.128812789916992,
      "learning_rate": 1.088824914943732e-05,
      "loss": 0.7772,
      "step": 355
    },
    {
      "epoch": 0.009318233374631371,
      "grad_norm": 9.787101745605469,
      "learning_rate": 1.0885108610311437e-05,
      "loss": 1.9128,
      "step": 356
    },
    {
      "epoch": 0.00934440818748146,
      "grad_norm": 14.501033782958984,
      "learning_rate": 1.0881968071185554e-05,
      "loss": 1.8565,
      "step": 357
    },
    {
      "epoch": 0.009370583000331547,
      "grad_norm": 20.69178009033203,
      "learning_rate": 1.087882753205967e-05,
      "loss": 1.2296,
      "step": 358
    },
    {
      "epoch": 0.009396757813181635,
      "grad_norm": 15.595779418945312,
      "learning_rate": 1.0875686992933788e-05,
      "loss": 1.791,
      "step": 359
    },
    {
      "epoch": 0.009422932626031723,
      "grad_norm": 11.90365219116211,
      "learning_rate": 1.0872546453807904e-05,
      "loss": 1.906,
      "step": 360
    },
    {
      "epoch": 0.009449107438881813,
      "grad_norm": 16.038898468017578,
      "learning_rate": 1.0869405914682021e-05,
      "loss": 2.1006,
      "step": 361
    },
    {
      "epoch": 0.0094752822517319,
      "grad_norm": 15.617024421691895,
      "learning_rate": 1.0866265375556137e-05,
      "loss": 1.6754,
      "step": 362
    },
    {
      "epoch": 0.009501457064581989,
      "grad_norm": 16.19228744506836,
      "learning_rate": 1.0863124836430255e-05,
      "loss": 1.9961,
      "step": 363
    },
    {
      "epoch": 0.009527631877432077,
      "grad_norm": 12.502939224243164,
      "learning_rate": 1.0859984297304372e-05,
      "loss": 1.0281,
      "step": 364
    },
    {
      "epoch": 0.009553806690282165,
      "grad_norm": 15.915958404541016,
      "learning_rate": 1.0856843758178486e-05,
      "loss": 0.6828,
      "step": 365
    },
    {
      "epoch": 0.009579981503132252,
      "grad_norm": 16.778366088867188,
      "learning_rate": 1.0853703219052604e-05,
      "loss": 3.2747,
      "step": 366
    },
    {
      "epoch": 0.00960615631598234,
      "grad_norm": 13.121718406677246,
      "learning_rate": 1.085056267992672e-05,
      "loss": 1.331,
      "step": 367
    },
    {
      "epoch": 0.009632331128832428,
      "grad_norm": 14.476509094238281,
      "learning_rate": 1.0847422140800837e-05,
      "loss": 2.5347,
      "step": 368
    },
    {
      "epoch": 0.009658505941682516,
      "grad_norm": 10.666698455810547,
      "learning_rate": 1.0844281601674955e-05,
      "loss": 0.6903,
      "step": 369
    },
    {
      "epoch": 0.009684680754532606,
      "grad_norm": 11.608527183532715,
      "learning_rate": 1.084114106254907e-05,
      "loss": 1.458,
      "step": 370
    },
    {
      "epoch": 0.009710855567382694,
      "grad_norm": 18.27562713623047,
      "learning_rate": 1.0838000523423188e-05,
      "loss": 2.6017,
      "step": 371
    },
    {
      "epoch": 0.009737030380232782,
      "grad_norm": 9.951774597167969,
      "learning_rate": 1.0834859984297304e-05,
      "loss": 1.1349,
      "step": 372
    },
    {
      "epoch": 0.00976320519308287,
      "grad_norm": 12.113003730773926,
      "learning_rate": 1.0831719445171422e-05,
      "loss": 0.9292,
      "step": 373
    },
    {
      "epoch": 0.009789380005932958,
      "grad_norm": 13.29439926147461,
      "learning_rate": 1.0828578906045538e-05,
      "loss": 0.463,
      "step": 374
    },
    {
      "epoch": 0.009815554818783045,
      "grad_norm": 12.57712459564209,
      "learning_rate": 1.0825438366919655e-05,
      "loss": 1.6228,
      "step": 375
    },
    {
      "epoch": 0.009841729631633133,
      "grad_norm": 17.21816062927246,
      "learning_rate": 1.0822297827793771e-05,
      "loss": 1.159,
      "step": 376
    },
    {
      "epoch": 0.009867904444483221,
      "grad_norm": 13.877412796020508,
      "learning_rate": 1.0819157288667889e-05,
      "loss": 1.415,
      "step": 377
    },
    {
      "epoch": 0.00989407925733331,
      "grad_norm": 14.684090614318848,
      "learning_rate": 1.0816016749542006e-05,
      "loss": 1.9764,
      "step": 378
    },
    {
      "epoch": 0.009920254070183399,
      "grad_norm": 13.613813400268555,
      "learning_rate": 1.0812876210416122e-05,
      "loss": 1.2159,
      "step": 379
    },
    {
      "epoch": 0.009946428883033487,
      "grad_norm": 11.879084587097168,
      "learning_rate": 1.080973567129024e-05,
      "loss": 1.1953,
      "step": 380
    },
    {
      "epoch": 0.009972603695883575,
      "grad_norm": 15.752952575683594,
      "learning_rate": 1.0806595132164354e-05,
      "loss": 0.5855,
      "step": 381
    },
    {
      "epoch": 0.009998778508733663,
      "grad_norm": 20.031707763671875,
      "learning_rate": 1.0803454593038471e-05,
      "loss": 0.705,
      "step": 382
    },
    {
      "epoch": 0.01002495332158375,
      "grad_norm": 14.504175186157227,
      "learning_rate": 1.0800314053912589e-05,
      "loss": 1.343,
      "step": 383
    },
    {
      "epoch": 0.010051128134433839,
      "grad_norm": 17.33380699157715,
      "learning_rate": 1.0797173514786705e-05,
      "loss": 1.6279,
      "step": 384
    },
    {
      "epoch": 0.010077302947283926,
      "grad_norm": 14.024443626403809,
      "learning_rate": 1.0794032975660822e-05,
      "loss": 1.2053,
      "step": 385
    },
    {
      "epoch": 0.010103477760134014,
      "grad_norm": 16.3476505279541,
      "learning_rate": 1.0790892436534938e-05,
      "loss": 0.8981,
      "step": 386
    },
    {
      "epoch": 0.010129652572984104,
      "grad_norm": 13.890003204345703,
      "learning_rate": 1.0787751897409056e-05,
      "loss": 1.7365,
      "step": 387
    },
    {
      "epoch": 0.010155827385834192,
      "grad_norm": 15.620462417602539,
      "learning_rate": 1.0784611358283172e-05,
      "loss": 0.8853,
      "step": 388
    },
    {
      "epoch": 0.01018200219868428,
      "grad_norm": 18.737411499023438,
      "learning_rate": 1.0781470819157289e-05,
      "loss": 1.7459,
      "step": 389
    },
    {
      "epoch": 0.010208177011534368,
      "grad_norm": 15.257023811340332,
      "learning_rate": 1.0778330280031407e-05,
      "loss": 1.618,
      "step": 390
    },
    {
      "epoch": 0.010234351824384456,
      "grad_norm": 16.263566970825195,
      "learning_rate": 1.0775189740905523e-05,
      "loss": 1.8461,
      "step": 391
    },
    {
      "epoch": 0.010260526637234544,
      "grad_norm": 18.674882888793945,
      "learning_rate": 1.077204920177964e-05,
      "loss": 0.8709,
      "step": 392
    },
    {
      "epoch": 0.010286701450084632,
      "grad_norm": 12.205535888671875,
      "learning_rate": 1.0768908662653756e-05,
      "loss": 0.3403,
      "step": 393
    },
    {
      "epoch": 0.01031287626293472,
      "grad_norm": 32.3964958190918,
      "learning_rate": 1.0765768123527873e-05,
      "loss": 1.6504,
      "step": 394
    },
    {
      "epoch": 0.010339051075784807,
      "grad_norm": 17.43682098388672,
      "learning_rate": 1.076262758440199e-05,
      "loss": 0.5355,
      "step": 395
    },
    {
      "epoch": 0.010365225888634897,
      "grad_norm": 15.280424118041992,
      "learning_rate": 1.0759487045276105e-05,
      "loss": 1.1856,
      "step": 396
    },
    {
      "epoch": 0.010391400701484985,
      "grad_norm": 14.801470756530762,
      "learning_rate": 1.0756346506150223e-05,
      "loss": 1.2562,
      "step": 397
    },
    {
      "epoch": 0.010417575514335073,
      "grad_norm": 24.957561492919922,
      "learning_rate": 1.0753205967024339e-05,
      "loss": 0.8216,
      "step": 398
    },
    {
      "epoch": 0.01044375032718516,
      "grad_norm": 18.14193344116211,
      "learning_rate": 1.0750065427898456e-05,
      "loss": 1.7284,
      "step": 399
    },
    {
      "epoch": 0.010469925140035249,
      "grad_norm": 20.479413986206055,
      "learning_rate": 1.0746924888772572e-05,
      "loss": 1.6653,
      "step": 400
    },
    {
      "epoch": 0.010496099952885337,
      "grad_norm": 21.713754653930664,
      "learning_rate": 1.074378434964669e-05,
      "loss": 1.5809,
      "step": 401
    },
    {
      "epoch": 0.010522274765735425,
      "grad_norm": 26.573183059692383,
      "learning_rate": 1.0740643810520805e-05,
      "loss": 0.7763,
      "step": 402
    },
    {
      "epoch": 0.010548449578585513,
      "grad_norm": 21.182371139526367,
      "learning_rate": 1.0737503271394923e-05,
      "loss": 2.2911,
      "step": 403
    },
    {
      "epoch": 0.0105746243914356,
      "grad_norm": 15.674162864685059,
      "learning_rate": 1.073436273226904e-05,
      "loss": 1.6113,
      "step": 404
    },
    {
      "epoch": 0.01060079920428569,
      "grad_norm": 20.34441375732422,
      "learning_rate": 1.0731222193143156e-05,
      "loss": 1.9567,
      "step": 405
    },
    {
      "epoch": 0.010626974017135778,
      "grad_norm": 15.73863410949707,
      "learning_rate": 1.0728081654017274e-05,
      "loss": 1.2086,
      "step": 406
    },
    {
      "epoch": 0.010653148829985866,
      "grad_norm": 18.402544021606445,
      "learning_rate": 1.072494111489139e-05,
      "loss": 2.7246,
      "step": 407
    },
    {
      "epoch": 0.010679323642835954,
      "grad_norm": 18.604225158691406,
      "learning_rate": 1.0721800575765507e-05,
      "loss": 2.0139,
      "step": 408
    },
    {
      "epoch": 0.010705498455686042,
      "grad_norm": 11.0678129196167,
      "learning_rate": 1.0718660036639623e-05,
      "loss": 0.6523,
      "step": 409
    },
    {
      "epoch": 0.01073167326853613,
      "grad_norm": 17.70045280456543,
      "learning_rate": 1.071551949751374e-05,
      "loss": 1.9054,
      "step": 410
    },
    {
      "epoch": 0.010757848081386218,
      "grad_norm": 20.07218360900879,
      "learning_rate": 1.0712378958387857e-05,
      "loss": 0.5899,
      "step": 411
    },
    {
      "epoch": 0.010784022894236306,
      "grad_norm": 11.439613342285156,
      "learning_rate": 1.0709238419261972e-05,
      "loss": 1.6186,
      "step": 412
    },
    {
      "epoch": 0.010810197707086395,
      "grad_norm": 15.654707908630371,
      "learning_rate": 1.070609788013609e-05,
      "loss": 2.0962,
      "step": 413
    },
    {
      "epoch": 0.010836372519936483,
      "grad_norm": 13.228864669799805,
      "learning_rate": 1.0702957341010206e-05,
      "loss": 1.4326,
      "step": 414
    },
    {
      "epoch": 0.010862547332786571,
      "grad_norm": 18.476831436157227,
      "learning_rate": 1.0699816801884323e-05,
      "loss": 1.8706,
      "step": 415
    },
    {
      "epoch": 0.010888722145636659,
      "grad_norm": 17.671348571777344,
      "learning_rate": 1.0696676262758441e-05,
      "loss": 0.9268,
      "step": 416
    },
    {
      "epoch": 0.010914896958486747,
      "grad_norm": 14.055368423461914,
      "learning_rate": 1.0693535723632557e-05,
      "loss": 1.9308,
      "step": 417
    },
    {
      "epoch": 0.010941071771336835,
      "grad_norm": 21.15837860107422,
      "learning_rate": 1.0690395184506674e-05,
      "loss": 1.7632,
      "step": 418
    },
    {
      "epoch": 0.010967246584186923,
      "grad_norm": 13.882519721984863,
      "learning_rate": 1.068725464538079e-05,
      "loss": 2.4609,
      "step": 419
    },
    {
      "epoch": 0.01099342139703701,
      "grad_norm": 12.520979881286621,
      "learning_rate": 1.0684114106254908e-05,
      "loss": 1.5518,
      "step": 420
    },
    {
      "epoch": 0.011019596209887099,
      "grad_norm": 13.968025207519531,
      "learning_rate": 1.0680973567129024e-05,
      "loss": 0.2303,
      "step": 421
    },
    {
      "epoch": 0.011045771022737188,
      "grad_norm": 25.262121200561523,
      "learning_rate": 1.0677833028003141e-05,
      "loss": 1.2172,
      "step": 422
    },
    {
      "epoch": 0.011071945835587276,
      "grad_norm": 17.831470489501953,
      "learning_rate": 1.0674692488877257e-05,
      "loss": 1.6826,
      "step": 423
    },
    {
      "epoch": 0.011098120648437364,
      "grad_norm": 17.561426162719727,
      "learning_rate": 1.0671551949751375e-05,
      "loss": 1.532,
      "step": 424
    },
    {
      "epoch": 0.011124295461287452,
      "grad_norm": 11.379362106323242,
      "learning_rate": 1.0668411410625492e-05,
      "loss": 1.1711,
      "step": 425
    },
    {
      "epoch": 0.01115047027413754,
      "grad_norm": 14.574835777282715,
      "learning_rate": 1.0665270871499608e-05,
      "loss": 0.9812,
      "step": 426
    },
    {
      "epoch": 0.011176645086987628,
      "grad_norm": 26.660511016845703,
      "learning_rate": 1.0662130332373724e-05,
      "loss": 1.0162,
      "step": 427
    },
    {
      "epoch": 0.011202819899837716,
      "grad_norm": 14.806595802307129,
      "learning_rate": 1.065898979324784e-05,
      "loss": 1.7548,
      "step": 428
    },
    {
      "epoch": 0.011228994712687804,
      "grad_norm": 12.115906715393066,
      "learning_rate": 1.0655849254121957e-05,
      "loss": 0.5802,
      "step": 429
    },
    {
      "epoch": 0.011255169525537892,
      "grad_norm": 11.633270263671875,
      "learning_rate": 1.0652708714996075e-05,
      "loss": 0.983,
      "step": 430
    },
    {
      "epoch": 0.011281344338387981,
      "grad_norm": 15.43632698059082,
      "learning_rate": 1.064956817587019e-05,
      "loss": 1.6864,
      "step": 431
    },
    {
      "epoch": 0.01130751915123807,
      "grad_norm": 17.35012435913086,
      "learning_rate": 1.0646427636744308e-05,
      "loss": 1.5649,
      "step": 432
    },
    {
      "epoch": 0.011333693964088157,
      "grad_norm": 16.485750198364258,
      "learning_rate": 1.0643287097618424e-05,
      "loss": 2.7503,
      "step": 433
    },
    {
      "epoch": 0.011359868776938245,
      "grad_norm": 13.972626686096191,
      "learning_rate": 1.0640146558492542e-05,
      "loss": 2.4709,
      "step": 434
    },
    {
      "epoch": 0.011386043589788333,
      "grad_norm": 11.040950775146484,
      "learning_rate": 1.0637006019366658e-05,
      "loss": 0.5166,
      "step": 435
    },
    {
      "epoch": 0.011412218402638421,
      "grad_norm": 19.529491424560547,
      "learning_rate": 1.0633865480240775e-05,
      "loss": 3.101,
      "step": 436
    },
    {
      "epoch": 0.011438393215488509,
      "grad_norm": 7.284560203552246,
      "learning_rate": 1.0630724941114893e-05,
      "loss": 1.3231,
      "step": 437
    },
    {
      "epoch": 0.011464568028338597,
      "grad_norm": 11.900135040283203,
      "learning_rate": 1.0627584401989009e-05,
      "loss": 1.4284,
      "step": 438
    },
    {
      "epoch": 0.011490742841188685,
      "grad_norm": 20.86232566833496,
      "learning_rate": 1.0624443862863126e-05,
      "loss": 0.5947,
      "step": 439
    },
    {
      "epoch": 0.011516917654038774,
      "grad_norm": 15.35389232635498,
      "learning_rate": 1.0621303323737242e-05,
      "loss": 1.1575,
      "step": 440
    },
    {
      "epoch": 0.011543092466888862,
      "grad_norm": 19.859607696533203,
      "learning_rate": 1.061816278461136e-05,
      "loss": 0.8267,
      "step": 441
    },
    {
      "epoch": 0.01156926727973895,
      "grad_norm": 15.935205459594727,
      "learning_rate": 1.0615022245485475e-05,
      "loss": 1.0885,
      "step": 442
    },
    {
      "epoch": 0.011595442092589038,
      "grad_norm": 17.462512969970703,
      "learning_rate": 1.0611881706359591e-05,
      "loss": 0.2306,
      "step": 443
    },
    {
      "epoch": 0.011621616905439126,
      "grad_norm": 15.117812156677246,
      "learning_rate": 1.0608741167233709e-05,
      "loss": 1.4718,
      "step": 444
    },
    {
      "epoch": 0.011647791718289214,
      "grad_norm": 19.19476318359375,
      "learning_rate": 1.0605600628107825e-05,
      "loss": 1.4816,
      "step": 445
    },
    {
      "epoch": 0.011673966531139302,
      "grad_norm": 11.454585075378418,
      "learning_rate": 1.0602460088981942e-05,
      "loss": 0.6296,
      "step": 446
    },
    {
      "epoch": 0.01170014134398939,
      "grad_norm": 21.852258682250977,
      "learning_rate": 1.0599319549856058e-05,
      "loss": 2.4447,
      "step": 447
    },
    {
      "epoch": 0.01172631615683948,
      "grad_norm": 10.331344604492188,
      "learning_rate": 1.0596179010730176e-05,
      "loss": 1.3197,
      "step": 448
    },
    {
      "epoch": 0.011752490969689567,
      "grad_norm": 16.449445724487305,
      "learning_rate": 1.0593038471604291e-05,
      "loss": 1.1467,
      "step": 449
    },
    {
      "epoch": 0.011778665782539655,
      "grad_norm": 18.224082946777344,
      "learning_rate": 1.0589897932478409e-05,
      "loss": 1.2427,
      "step": 450
    },
    {
      "epoch": 0.011804840595389743,
      "grad_norm": 13.384048461914062,
      "learning_rate": 1.0586757393352527e-05,
      "loss": 0.7212,
      "step": 451
    },
    {
      "epoch": 0.011831015408239831,
      "grad_norm": 17.2220458984375,
      "learning_rate": 1.0583616854226642e-05,
      "loss": 1.2506,
      "step": 452
    },
    {
      "epoch": 0.011857190221089919,
      "grad_norm": 23.529722213745117,
      "learning_rate": 1.058047631510076e-05,
      "loss": 2.2798,
      "step": 453
    },
    {
      "epoch": 0.011883365033940007,
      "grad_norm": 13.772336959838867,
      "learning_rate": 1.0577335775974876e-05,
      "loss": 0.8208,
      "step": 454
    },
    {
      "epoch": 0.011909539846790095,
      "grad_norm": 17.47730255126953,
      "learning_rate": 1.0574195236848993e-05,
      "loss": 1.08,
      "step": 455
    },
    {
      "epoch": 0.011935714659640183,
      "grad_norm": 11.2437105178833,
      "learning_rate": 1.057105469772311e-05,
      "loss": 1.518,
      "step": 456
    },
    {
      "epoch": 0.011961889472490272,
      "grad_norm": 13.08504581451416,
      "learning_rate": 1.0567914158597227e-05,
      "loss": 1.1616,
      "step": 457
    },
    {
      "epoch": 0.01198806428534036,
      "grad_norm": 18.31845474243164,
      "learning_rate": 1.0564773619471343e-05,
      "loss": 1.4963,
      "step": 458
    },
    {
      "epoch": 0.012014239098190448,
      "grad_norm": 19.127975463867188,
      "learning_rate": 1.0561633080345459e-05,
      "loss": 1.6742,
      "step": 459
    },
    {
      "epoch": 0.012040413911040536,
      "grad_norm": 10.626084327697754,
      "learning_rate": 1.0558492541219576e-05,
      "loss": 1.5333,
      "step": 460
    },
    {
      "epoch": 0.012066588723890624,
      "grad_norm": 10.832667350769043,
      "learning_rate": 1.0555352002093692e-05,
      "loss": 1.2157,
      "step": 461
    },
    {
      "epoch": 0.012092763536740712,
      "grad_norm": 23.05351448059082,
      "learning_rate": 1.055221146296781e-05,
      "loss": 1.2669,
      "step": 462
    },
    {
      "epoch": 0.0121189383495908,
      "grad_norm": 18.36079216003418,
      "learning_rate": 1.0549070923841927e-05,
      "loss": 1.5412,
      "step": 463
    },
    {
      "epoch": 0.012145113162440888,
      "grad_norm": 22.77534294128418,
      "learning_rate": 1.0545930384716043e-05,
      "loss": 0.5649,
      "step": 464
    },
    {
      "epoch": 0.012171287975290976,
      "grad_norm": 16.045751571655273,
      "learning_rate": 1.054278984559016e-05,
      "loss": 1.1233,
      "step": 465
    },
    {
      "epoch": 0.012197462788141065,
      "grad_norm": 16.265403747558594,
      "learning_rate": 1.0539649306464276e-05,
      "loss": 1.679,
      "step": 466
    },
    {
      "epoch": 0.012223637600991153,
      "grad_norm": 20.71302604675293,
      "learning_rate": 1.0536508767338394e-05,
      "loss": 1.7828,
      "step": 467
    },
    {
      "epoch": 0.012249812413841241,
      "grad_norm": 17.872230529785156,
      "learning_rate": 1.053336822821251e-05,
      "loss": 1.9604,
      "step": 468
    },
    {
      "epoch": 0.01227598722669133,
      "grad_norm": 14.933849334716797,
      "learning_rate": 1.0530227689086627e-05,
      "loss": 1.4939,
      "step": 469
    },
    {
      "epoch": 0.012302162039541417,
      "grad_norm": 16.174427032470703,
      "learning_rate": 1.0527087149960743e-05,
      "loss": 0.4945,
      "step": 470
    },
    {
      "epoch": 0.012328336852391505,
      "grad_norm": 18.38136863708496,
      "learning_rate": 1.052394661083486e-05,
      "loss": 1.6362,
      "step": 471
    },
    {
      "epoch": 0.012354511665241593,
      "grad_norm": 18.399559020996094,
      "learning_rate": 1.0520806071708978e-05,
      "loss": 1.0418,
      "step": 472
    },
    {
      "epoch": 0.012380686478091681,
      "grad_norm": 16.468408584594727,
      "learning_rate": 1.0517665532583092e-05,
      "loss": 1.3063,
      "step": 473
    },
    {
      "epoch": 0.01240686129094177,
      "grad_norm": 19.39691162109375,
      "learning_rate": 1.051452499345721e-05,
      "loss": 0.5574,
      "step": 474
    },
    {
      "epoch": 0.012433036103791859,
      "grad_norm": 29.760330200195312,
      "learning_rate": 1.0511384454331326e-05,
      "loss": 0.7921,
      "step": 475
    },
    {
      "epoch": 0.012459210916641946,
      "grad_norm": 18.140844345092773,
      "learning_rate": 1.0508243915205443e-05,
      "loss": 0.3105,
      "step": 476
    },
    {
      "epoch": 0.012485385729492034,
      "grad_norm": 20.809045791625977,
      "learning_rate": 1.0505103376079561e-05,
      "loss": 1.2175,
      "step": 477
    },
    {
      "epoch": 0.012511560542342122,
      "grad_norm": 15.0239896774292,
      "learning_rate": 1.0501962836953677e-05,
      "loss": 1.9543,
      "step": 478
    },
    {
      "epoch": 0.01253773535519221,
      "grad_norm": 17.87742042541504,
      "learning_rate": 1.0498822297827794e-05,
      "loss": 0.9287,
      "step": 479
    },
    {
      "epoch": 0.012563910168042298,
      "grad_norm": 17.054691314697266,
      "learning_rate": 1.049568175870191e-05,
      "loss": 0.5677,
      "step": 480
    },
    {
      "epoch": 0.012590084980892386,
      "grad_norm": 18.954370498657227,
      "learning_rate": 1.0492541219576028e-05,
      "loss": 1.9907,
      "step": 481
    },
    {
      "epoch": 0.012616259793742474,
      "grad_norm": 21.343564987182617,
      "learning_rate": 1.0489400680450144e-05,
      "loss": 1.375,
      "step": 482
    },
    {
      "epoch": 0.012642434606592564,
      "grad_norm": 13.8994779586792,
      "learning_rate": 1.0486260141324261e-05,
      "loss": 1.7046,
      "step": 483
    },
    {
      "epoch": 0.012668609419442652,
      "grad_norm": 17.33802604675293,
      "learning_rate": 1.0483119602198379e-05,
      "loss": 2.1402,
      "step": 484
    },
    {
      "epoch": 0.01269478423229274,
      "grad_norm": 20.242996215820312,
      "learning_rate": 1.0479979063072495e-05,
      "loss": 0.793,
      "step": 485
    },
    {
      "epoch": 0.012720959045142827,
      "grad_norm": 12.956822395324707,
      "learning_rate": 1.0476838523946612e-05,
      "loss": 1.0627,
      "step": 486
    },
    {
      "epoch": 0.012747133857992915,
      "grad_norm": 24.263866424560547,
      "learning_rate": 1.0473697984820728e-05,
      "loss": 1.5583,
      "step": 487
    },
    {
      "epoch": 0.012773308670843003,
      "grad_norm": 14.9137544631958,
      "learning_rate": 1.0470557445694846e-05,
      "loss": 1.044,
      "step": 488
    },
    {
      "epoch": 0.012799483483693091,
      "grad_norm": 19.844078063964844,
      "learning_rate": 1.0467416906568961e-05,
      "loss": 2.2749,
      "step": 489
    },
    {
      "epoch": 0.012825658296543179,
      "grad_norm": 12.397175788879395,
      "learning_rate": 1.0464276367443077e-05,
      "loss": 1.0671,
      "step": 490
    },
    {
      "epoch": 0.012851833109393267,
      "grad_norm": 15.332509994506836,
      "learning_rate": 1.0461135828317195e-05,
      "loss": 1.1739,
      "step": 491
    },
    {
      "epoch": 0.012878007922243357,
      "grad_norm": 11.42781925201416,
      "learning_rate": 1.045799528919131e-05,
      "loss": 1.2521,
      "step": 492
    },
    {
      "epoch": 0.012904182735093445,
      "grad_norm": 15.851160049438477,
      "learning_rate": 1.0454854750065428e-05,
      "loss": 1.3552,
      "step": 493
    },
    {
      "epoch": 0.012930357547943533,
      "grad_norm": 27.704286575317383,
      "learning_rate": 1.0451714210939544e-05,
      "loss": 1.7059,
      "step": 494
    },
    {
      "epoch": 0.01295653236079362,
      "grad_norm": 13.650613784790039,
      "learning_rate": 1.0448573671813662e-05,
      "loss": 1.7345,
      "step": 495
    },
    {
      "epoch": 0.012982707173643708,
      "grad_norm": 13.209798812866211,
      "learning_rate": 1.0445433132687778e-05,
      "loss": 1.0491,
      "step": 496
    },
    {
      "epoch": 0.013008881986493796,
      "grad_norm": 17.11916160583496,
      "learning_rate": 1.0442292593561895e-05,
      "loss": 0.6053,
      "step": 497
    },
    {
      "epoch": 0.013035056799343884,
      "grad_norm": 13.11049747467041,
      "learning_rate": 1.0439152054436013e-05,
      "loss": 1.0892,
      "step": 498
    },
    {
      "epoch": 0.013061231612193972,
      "grad_norm": 9.885848045349121,
      "learning_rate": 1.0436011515310128e-05,
      "loss": 1.4003,
      "step": 499
    },
    {
      "epoch": 0.013087406425044062,
      "grad_norm": 13.354086875915527,
      "learning_rate": 1.0432870976184246e-05,
      "loss": 0.9472,
      "step": 500
    },
    {
      "epoch": 0.01311358123789415,
      "grad_norm": 18.980764389038086,
      "learning_rate": 1.0429730437058362e-05,
      "loss": 1.6532,
      "step": 501
    },
    {
      "epoch": 0.013139756050744238,
      "grad_norm": 14.018961906433105,
      "learning_rate": 1.042658989793248e-05,
      "loss": 1.2485,
      "step": 502
    },
    {
      "epoch": 0.013165930863594326,
      "grad_norm": 19.385862350463867,
      "learning_rate": 1.0423449358806595e-05,
      "loss": 1.8448,
      "step": 503
    },
    {
      "epoch": 0.013192105676444413,
      "grad_norm": 24.350013732910156,
      "learning_rate": 1.0420308819680711e-05,
      "loss": 1.4734,
      "step": 504
    },
    {
      "epoch": 0.013218280489294501,
      "grad_norm": 27.88928985595703,
      "learning_rate": 1.0417168280554829e-05,
      "loss": 0.9323,
      "step": 505
    },
    {
      "epoch": 0.01324445530214459,
      "grad_norm": 11.809305191040039,
      "learning_rate": 1.0414027741428945e-05,
      "loss": 0.5544,
      "step": 506
    },
    {
      "epoch": 0.013270630114994677,
      "grad_norm": 24.713443756103516,
      "learning_rate": 1.0410887202303062e-05,
      "loss": 1.6557,
      "step": 507
    },
    {
      "epoch": 0.013296804927844765,
      "grad_norm": 21.127344131469727,
      "learning_rate": 1.0407746663177178e-05,
      "loss": 1.8234,
      "step": 508
    },
    {
      "epoch": 0.013322979740694855,
      "grad_norm": 12.818073272705078,
      "learning_rate": 1.0404606124051296e-05,
      "loss": 0.3886,
      "step": 509
    },
    {
      "epoch": 0.013349154553544943,
      "grad_norm": 16.266637802124023,
      "learning_rate": 1.0401465584925413e-05,
      "loss": 0.4114,
      "step": 510
    },
    {
      "epoch": 0.01337532936639503,
      "grad_norm": 19.857887268066406,
      "learning_rate": 1.0398325045799529e-05,
      "loss": 1.439,
      "step": 511
    },
    {
      "epoch": 0.013401504179245119,
      "grad_norm": 16.09312629699707,
      "learning_rate": 1.0395184506673647e-05,
      "loss": 1.1217,
      "step": 512
    },
    {
      "epoch": 0.013427678992095207,
      "grad_norm": 17.024757385253906,
      "learning_rate": 1.0392043967547762e-05,
      "loss": 1.1014,
      "step": 513
    },
    {
      "epoch": 0.013453853804945294,
      "grad_norm": 13.367935180664062,
      "learning_rate": 1.038890342842188e-05,
      "loss": 1.2313,
      "step": 514
    },
    {
      "epoch": 0.013480028617795382,
      "grad_norm": 10.811565399169922,
      "learning_rate": 1.0385762889295996e-05,
      "loss": 0.6174,
      "step": 515
    },
    {
      "epoch": 0.01350620343064547,
      "grad_norm": 13.5171480178833,
      "learning_rate": 1.0382622350170113e-05,
      "loss": 1.4272,
      "step": 516
    },
    {
      "epoch": 0.013532378243495558,
      "grad_norm": 17.4696102142334,
      "learning_rate": 1.037948181104423e-05,
      "loss": 1.4821,
      "step": 517
    },
    {
      "epoch": 0.013558553056345648,
      "grad_norm": 15.194046020507812,
      "learning_rate": 1.0376341271918347e-05,
      "loss": 0.6873,
      "step": 518
    },
    {
      "epoch": 0.013584727869195736,
      "grad_norm": 19.164377212524414,
      "learning_rate": 1.0373200732792464e-05,
      "loss": 1.3236,
      "step": 519
    },
    {
      "epoch": 0.013610902682045824,
      "grad_norm": 17.039722442626953,
      "learning_rate": 1.0370060193666578e-05,
      "loss": 2.5316,
      "step": 520
    },
    {
      "epoch": 0.013637077494895912,
      "grad_norm": 10.069087982177734,
      "learning_rate": 1.0366919654540696e-05,
      "loss": 0.2891,
      "step": 521
    },
    {
      "epoch": 0.013663252307746,
      "grad_norm": 17.341087341308594,
      "learning_rate": 1.0363779115414812e-05,
      "loss": 1.7373,
      "step": 522
    },
    {
      "epoch": 0.013689427120596087,
      "grad_norm": 23.268796920776367,
      "learning_rate": 1.036063857628893e-05,
      "loss": 0.8477,
      "step": 523
    },
    {
      "epoch": 0.013715601933446175,
      "grad_norm": 12.995265007019043,
      "learning_rate": 1.0357498037163047e-05,
      "loss": 1.5938,
      "step": 524
    },
    {
      "epoch": 0.013741776746296263,
      "grad_norm": 24.226688385009766,
      "learning_rate": 1.0354357498037163e-05,
      "loss": 1.8073,
      "step": 525
    },
    {
      "epoch": 0.013767951559146351,
      "grad_norm": 19.130029678344727,
      "learning_rate": 1.035121695891128e-05,
      "loss": 0.3941,
      "step": 526
    },
    {
      "epoch": 0.013794126371996441,
      "grad_norm": 17.605464935302734,
      "learning_rate": 1.0348076419785396e-05,
      "loss": 0.6631,
      "step": 527
    },
    {
      "epoch": 0.013820301184846529,
      "grad_norm": 19.67823600769043,
      "learning_rate": 1.0344935880659514e-05,
      "loss": 0.5204,
      "step": 528
    },
    {
      "epoch": 0.013846475997696617,
      "grad_norm": 15.978683471679688,
      "learning_rate": 1.034179534153363e-05,
      "loss": 1.4297,
      "step": 529
    },
    {
      "epoch": 0.013872650810546705,
      "grad_norm": 17.287260055541992,
      "learning_rate": 1.0338654802407747e-05,
      "loss": 1.3039,
      "step": 530
    },
    {
      "epoch": 0.013898825623396793,
      "grad_norm": 24.034582138061523,
      "learning_rate": 1.0335514263281865e-05,
      "loss": 0.87,
      "step": 531
    },
    {
      "epoch": 0.01392500043624688,
      "grad_norm": 16.89599609375,
      "learning_rate": 1.033237372415598e-05,
      "loss": 1.83,
      "step": 532
    },
    {
      "epoch": 0.013951175249096968,
      "grad_norm": 20.946035385131836,
      "learning_rate": 1.0329233185030098e-05,
      "loss": 0.9731,
      "step": 533
    },
    {
      "epoch": 0.013977350061947056,
      "grad_norm": 19.191268920898438,
      "learning_rate": 1.0326092645904214e-05,
      "loss": 1.6011,
      "step": 534
    },
    {
      "epoch": 0.014003524874797146,
      "grad_norm": 18.254608154296875,
      "learning_rate": 1.032295210677833e-05,
      "loss": 0.9383,
      "step": 535
    },
    {
      "epoch": 0.014029699687647234,
      "grad_norm": 26.649686813354492,
      "learning_rate": 1.0319811567652447e-05,
      "loss": 0.6128,
      "step": 536
    },
    {
      "epoch": 0.014055874500497322,
      "grad_norm": 19.536026000976562,
      "learning_rate": 1.0316671028526563e-05,
      "loss": 1.4551,
      "step": 537
    },
    {
      "epoch": 0.01408204931334741,
      "grad_norm": 22.442358016967773,
      "learning_rate": 1.0313530489400681e-05,
      "loss": 1.3228,
      "step": 538
    },
    {
      "epoch": 0.014108224126197498,
      "grad_norm": 35.857269287109375,
      "learning_rate": 1.0310389950274797e-05,
      "loss": 1.1974,
      "step": 539
    },
    {
      "epoch": 0.014134398939047586,
      "grad_norm": 21.048797607421875,
      "learning_rate": 1.0307249411148914e-05,
      "loss": 1.11,
      "step": 540
    },
    {
      "epoch": 0.014160573751897674,
      "grad_norm": 12.779808044433594,
      "learning_rate": 1.030410887202303e-05,
      "loss": 0.595,
      "step": 541
    },
    {
      "epoch": 0.014186748564747761,
      "grad_norm": 21.295724868774414,
      "learning_rate": 1.0300968332897148e-05,
      "loss": 1.8999,
      "step": 542
    },
    {
      "epoch": 0.01421292337759785,
      "grad_norm": 15.289767265319824,
      "learning_rate": 1.0297827793771264e-05,
      "loss": 1.8437,
      "step": 543
    },
    {
      "epoch": 0.014239098190447939,
      "grad_norm": 11.33554744720459,
      "learning_rate": 1.0294687254645381e-05,
      "loss": 1.1999,
      "step": 544
    },
    {
      "epoch": 0.014265273003298027,
      "grad_norm": 18.923627853393555,
      "learning_rate": 1.0291546715519499e-05,
      "loss": 0.9595,
      "step": 545
    },
    {
      "epoch": 0.014291447816148115,
      "grad_norm": 12.86658763885498,
      "learning_rate": 1.0288406176393615e-05,
      "loss": 1.8985,
      "step": 546
    },
    {
      "epoch": 0.014317622628998203,
      "grad_norm": 27.39386558532715,
      "learning_rate": 1.0285265637267732e-05,
      "loss": 1.1023,
      "step": 547
    },
    {
      "epoch": 0.01434379744184829,
      "grad_norm": 8.369140625,
      "learning_rate": 1.0282125098141848e-05,
      "loss": 0.8538,
      "step": 548
    },
    {
      "epoch": 0.014369972254698379,
      "grad_norm": 16.76064109802246,
      "learning_rate": 1.0278984559015965e-05,
      "loss": 0.6368,
      "step": 549
    },
    {
      "epoch": 0.014396147067548467,
      "grad_norm": 14.80250358581543,
      "learning_rate": 1.0275844019890081e-05,
      "loss": 1.1061,
      "step": 550
    },
    {
      "epoch": 0.014422321880398555,
      "grad_norm": 21.996402740478516,
      "learning_rate": 1.0272703480764197e-05,
      "loss": 0.4493,
      "step": 551
    },
    {
      "epoch": 0.014448496693248642,
      "grad_norm": 16.09116554260254,
      "learning_rate": 1.0269562941638315e-05,
      "loss": 1.6801,
      "step": 552
    },
    {
      "epoch": 0.014474671506098732,
      "grad_norm": 14.361990928649902,
      "learning_rate": 1.026642240251243e-05,
      "loss": 1.2444,
      "step": 553
    },
    {
      "epoch": 0.01450084631894882,
      "grad_norm": 16.11039161682129,
      "learning_rate": 1.0263281863386548e-05,
      "loss": 2.086,
      "step": 554
    },
    {
      "epoch": 0.014527021131798908,
      "grad_norm": 17.37653160095215,
      "learning_rate": 1.0260141324260664e-05,
      "loss": 1.0474,
      "step": 555
    },
    {
      "epoch": 0.014553195944648996,
      "grad_norm": 15.034843444824219,
      "learning_rate": 1.0257000785134782e-05,
      "loss": 0.9432,
      "step": 556
    },
    {
      "epoch": 0.014579370757499084,
      "grad_norm": 13.30081844329834,
      "learning_rate": 1.0253860246008899e-05,
      "loss": 1.4939,
      "step": 557
    },
    {
      "epoch": 0.014605545570349172,
      "grad_norm": 15.75667953491211,
      "learning_rate": 1.0250719706883015e-05,
      "loss": 1.2449,
      "step": 558
    },
    {
      "epoch": 0.01463172038319926,
      "grad_norm": 17.654273986816406,
      "learning_rate": 1.0247579167757133e-05,
      "loss": 1.4912,
      "step": 559
    },
    {
      "epoch": 0.014657895196049348,
      "grad_norm": 11.865370750427246,
      "learning_rate": 1.0244438628631248e-05,
      "loss": 0.8986,
      "step": 560
    },
    {
      "epoch": 0.014684070008899437,
      "grad_norm": 15.675153732299805,
      "learning_rate": 1.0241298089505366e-05,
      "loss": 1.1994,
      "step": 561
    },
    {
      "epoch": 0.014710244821749525,
      "grad_norm": 28.483217239379883,
      "learning_rate": 1.0238157550379482e-05,
      "loss": 1.322,
      "step": 562
    },
    {
      "epoch": 0.014736419634599613,
      "grad_norm": 14.541632652282715,
      "learning_rate": 1.02350170112536e-05,
      "loss": 0.77,
      "step": 563
    },
    {
      "epoch": 0.014762594447449701,
      "grad_norm": 17.677297592163086,
      "learning_rate": 1.0231876472127715e-05,
      "loss": 1.1277,
      "step": 564
    },
    {
      "epoch": 0.014788769260299789,
      "grad_norm": 16.82846450805664,
      "learning_rate": 1.0228735933001833e-05,
      "loss": 1.3135,
      "step": 565
    },
    {
      "epoch": 0.014814944073149877,
      "grad_norm": 15.833956718444824,
      "learning_rate": 1.0225595393875949e-05,
      "loss": 0.5963,
      "step": 566
    },
    {
      "epoch": 0.014841118885999965,
      "grad_norm": 12.425332069396973,
      "learning_rate": 1.0222454854750064e-05,
      "loss": 0.6848,
      "step": 567
    },
    {
      "epoch": 0.014867293698850053,
      "grad_norm": 16.202608108520508,
      "learning_rate": 1.0219314315624182e-05,
      "loss": 1.6065,
      "step": 568
    },
    {
      "epoch": 0.01489346851170014,
      "grad_norm": 14.26634407043457,
      "learning_rate": 1.02161737764983e-05,
      "loss": 0.8677,
      "step": 569
    },
    {
      "epoch": 0.01491964332455023,
      "grad_norm": 17.37994956970215,
      "learning_rate": 1.0213033237372415e-05,
      "loss": 1.5347,
      "step": 570
    },
    {
      "epoch": 0.014945818137400318,
      "grad_norm": 24.423118591308594,
      "learning_rate": 1.0209892698246533e-05,
      "loss": 1.2907,
      "step": 571
    },
    {
      "epoch": 0.014971992950250406,
      "grad_norm": 16.64789581298828,
      "learning_rate": 1.0206752159120649e-05,
      "loss": 0.9604,
      "step": 572
    },
    {
      "epoch": 0.014998167763100494,
      "grad_norm": 11.346396446228027,
      "learning_rate": 1.0203611619994766e-05,
      "loss": 0.6298,
      "step": 573
    },
    {
      "epoch": 0.015024342575950582,
      "grad_norm": 15.451828002929688,
      "learning_rate": 1.0200471080868882e-05,
      "loss": 0.849,
      "step": 574
    },
    {
      "epoch": 0.01505051738880067,
      "grad_norm": 20.49965476989746,
      "learning_rate": 1.0197330541743e-05,
      "loss": 1.0977,
      "step": 575
    },
    {
      "epoch": 0.015076692201650758,
      "grad_norm": 25.713367462158203,
      "learning_rate": 1.0194190002617116e-05,
      "loss": 1.3771,
      "step": 576
    },
    {
      "epoch": 0.015102867014500846,
      "grad_norm": 13.735161781311035,
      "learning_rate": 1.0191049463491233e-05,
      "loss": 1.3282,
      "step": 577
    },
    {
      "epoch": 0.015129041827350934,
      "grad_norm": 21.298664093017578,
      "learning_rate": 1.018790892436535e-05,
      "loss": 1.3679,
      "step": 578
    },
    {
      "epoch": 0.015155216640201023,
      "grad_norm": 23.778156280517578,
      "learning_rate": 1.0184768385239467e-05,
      "loss": 0.7208,
      "step": 579
    },
    {
      "epoch": 0.015181391453051111,
      "grad_norm": 17.97779655456543,
      "learning_rate": 1.0181627846113584e-05,
      "loss": 0.7386,
      "step": 580
    },
    {
      "epoch": 0.015207566265901199,
      "grad_norm": 17.146623611450195,
      "learning_rate": 1.01784873069877e-05,
      "loss": 0.6495,
      "step": 581
    },
    {
      "epoch": 0.015233741078751287,
      "grad_norm": 10.271608352661133,
      "learning_rate": 1.0175346767861816e-05,
      "loss": 0.6259,
      "step": 582
    },
    {
      "epoch": 0.015259915891601375,
      "grad_norm": 16.0623722076416,
      "learning_rate": 1.0172206228735933e-05,
      "loss": 1.9244,
      "step": 583
    },
    {
      "epoch": 0.015286090704451463,
      "grad_norm": 21.74067497253418,
      "learning_rate": 1.016906568961005e-05,
      "loss": 0.677,
      "step": 584
    },
    {
      "epoch": 0.01531226551730155,
      "grad_norm": 23.636545181274414,
      "learning_rate": 1.0165925150484167e-05,
      "loss": 1.4505,
      "step": 585
    },
    {
      "epoch": 0.015338440330151639,
      "grad_norm": 17.47941017150879,
      "learning_rate": 1.0162784611358283e-05,
      "loss": 0.435,
      "step": 586
    },
    {
      "epoch": 0.015364615143001728,
      "grad_norm": 11.056567192077637,
      "learning_rate": 1.01596440722324e-05,
      "loss": 0.4203,
      "step": 587
    },
    {
      "epoch": 0.015390789955851816,
      "grad_norm": 20.983240127563477,
      "learning_rate": 1.0156503533106516e-05,
      "loss": 0.3669,
      "step": 588
    },
    {
      "epoch": 0.015416964768701904,
      "grad_norm": 24.821361541748047,
      "learning_rate": 1.0153362993980634e-05,
      "loss": 0.4563,
      "step": 589
    },
    {
      "epoch": 0.015443139581551992,
      "grad_norm": 16.500356674194336,
      "learning_rate": 1.015022245485475e-05,
      "loss": 1.7656,
      "step": 590
    },
    {
      "epoch": 0.01546931439440208,
      "grad_norm": 18.274242401123047,
      "learning_rate": 1.0147081915728867e-05,
      "loss": 2.6145,
      "step": 591
    },
    {
      "epoch": 0.015495489207252168,
      "grad_norm": 31.088054656982422,
      "learning_rate": 1.0143941376602985e-05,
      "loss": 1.531,
      "step": 592
    },
    {
      "epoch": 0.015521664020102256,
      "grad_norm": 15.16018295288086,
      "learning_rate": 1.01408008374771e-05,
      "loss": 1.6802,
      "step": 593
    },
    {
      "epoch": 0.015547838832952344,
      "grad_norm": 22.216768264770508,
      "learning_rate": 1.0137660298351218e-05,
      "loss": 1.2264,
      "step": 594
    },
    {
      "epoch": 0.015574013645802432,
      "grad_norm": 18.0421085357666,
      "learning_rate": 1.0134519759225334e-05,
      "loss": 1.1351,
      "step": 595
    },
    {
      "epoch": 0.015600188458652521,
      "grad_norm": 29.573402404785156,
      "learning_rate": 1.0131379220099452e-05,
      "loss": 1.6207,
      "step": 596
    },
    {
      "epoch": 0.01562636327150261,
      "grad_norm": 11.64367389678955,
      "learning_rate": 1.0128238680973567e-05,
      "loss": 0.329,
      "step": 597
    },
    {
      "epoch": 0.015652538084352696,
      "grad_norm": 24.179574966430664,
      "learning_rate": 1.0125098141847683e-05,
      "loss": 1.1547,
      "step": 598
    },
    {
      "epoch": 0.015678712897202785,
      "grad_norm": 17.136369705200195,
      "learning_rate": 1.01219576027218e-05,
      "loss": 0.9613,
      "step": 599
    },
    {
      "epoch": 0.015704887710052875,
      "grad_norm": 21.638912200927734,
      "learning_rate": 1.0118817063595917e-05,
      "loss": 1.6673,
      "step": 600
    },
    {
      "epoch": 0.01573106252290296,
      "grad_norm": 15.265533447265625,
      "learning_rate": 1.0115676524470034e-05,
      "loss": 0.9758,
      "step": 601
    },
    {
      "epoch": 0.01575723733575305,
      "grad_norm": 16.836572647094727,
      "learning_rate": 1.011253598534415e-05,
      "loss": 1.216,
      "step": 602
    },
    {
      "epoch": 0.015783412148603137,
      "grad_norm": 17.460739135742188,
      "learning_rate": 1.0109395446218268e-05,
      "loss": 1.6175,
      "step": 603
    },
    {
      "epoch": 0.015809586961453227,
      "grad_norm": 13.88507080078125,
      "learning_rate": 1.0106254907092385e-05,
      "loss": 0.4466,
      "step": 604
    },
    {
      "epoch": 0.015835761774303313,
      "grad_norm": 15.179227828979492,
      "learning_rate": 1.0103114367966501e-05,
      "loss": 0.7412,
      "step": 605
    },
    {
      "epoch": 0.015861936587153402,
      "grad_norm": 19.4517822265625,
      "learning_rate": 1.0099973828840619e-05,
      "loss": 1.2817,
      "step": 606
    },
    {
      "epoch": 0.01588811140000349,
      "grad_norm": 17.643497467041016,
      "learning_rate": 1.0096833289714734e-05,
      "loss": 0.9285,
      "step": 607
    },
    {
      "epoch": 0.015914286212853578,
      "grad_norm": 16.297937393188477,
      "learning_rate": 1.0093692750588852e-05,
      "loss": 1.655,
      "step": 608
    },
    {
      "epoch": 0.015940461025703668,
      "grad_norm": 19.885833740234375,
      "learning_rate": 1.0090552211462968e-05,
      "loss": 1.3252,
      "step": 609
    },
    {
      "epoch": 0.015966635838553754,
      "grad_norm": 17.58584976196289,
      "learning_rate": 1.0087411672337085e-05,
      "loss": 1.1313,
      "step": 610
    },
    {
      "epoch": 0.015992810651403844,
      "grad_norm": 17.28054428100586,
      "learning_rate": 1.0084271133211201e-05,
      "loss": 0.6783,
      "step": 611
    },
    {
      "epoch": 0.01601898546425393,
      "grad_norm": 17.80898666381836,
      "learning_rate": 1.0081130594085319e-05,
      "loss": 0.7197,
      "step": 612
    },
    {
      "epoch": 0.01604516027710402,
      "grad_norm": 12.051491737365723,
      "learning_rate": 1.0077990054959435e-05,
      "loss": 1.4658,
      "step": 613
    },
    {
      "epoch": 0.016071335089954106,
      "grad_norm": 16.611989974975586,
      "learning_rate": 1.007484951583355e-05,
      "loss": 1.5251,
      "step": 614
    },
    {
      "epoch": 0.016097509902804195,
      "grad_norm": 13.03529167175293,
      "learning_rate": 1.0071708976707668e-05,
      "loss": 1.0846,
      "step": 615
    },
    {
      "epoch": 0.01612368471565428,
      "grad_norm": 15.323126792907715,
      "learning_rate": 1.0068568437581786e-05,
      "loss": 1.5093,
      "step": 616
    },
    {
      "epoch": 0.01614985952850437,
      "grad_norm": 17.194801330566406,
      "learning_rate": 1.0065427898455902e-05,
      "loss": 1.1138,
      "step": 617
    },
    {
      "epoch": 0.01617603434135446,
      "grad_norm": 15.417850494384766,
      "learning_rate": 1.0062287359330019e-05,
      "loss": 1.9317,
      "step": 618
    },
    {
      "epoch": 0.016202209154204547,
      "grad_norm": 15.945985794067383,
      "learning_rate": 1.0059146820204135e-05,
      "loss": 1.4502,
      "step": 619
    },
    {
      "epoch": 0.016228383967054637,
      "grad_norm": 20.059925079345703,
      "learning_rate": 1.0056006281078252e-05,
      "loss": 1.8998,
      "step": 620
    },
    {
      "epoch": 0.016254558779904723,
      "grad_norm": 9.894926071166992,
      "learning_rate": 1.0052865741952368e-05,
      "loss": 1.1092,
      "step": 621
    },
    {
      "epoch": 0.016280733592754813,
      "grad_norm": 13.820646286010742,
      "learning_rate": 1.0049725202826486e-05,
      "loss": 0.5971,
      "step": 622
    },
    {
      "epoch": 0.0163069084056049,
      "grad_norm": 19.131881713867188,
      "learning_rate": 1.0046584663700602e-05,
      "loss": 1.7721,
      "step": 623
    },
    {
      "epoch": 0.01633308321845499,
      "grad_norm": 13.103532791137695,
      "learning_rate": 1.004344412457472e-05,
      "loss": 0.6004,
      "step": 624
    },
    {
      "epoch": 0.016359258031305075,
      "grad_norm": 24.562854766845703,
      "learning_rate": 1.0040303585448837e-05,
      "loss": 1.2052,
      "step": 625
    },
    {
      "epoch": 0.016385432844155164,
      "grad_norm": 14.394401550292969,
      "learning_rate": 1.0037163046322953e-05,
      "loss": 1.0025,
      "step": 626
    },
    {
      "epoch": 0.016411607657005254,
      "grad_norm": 12.460485458374023,
      "learning_rate": 1.003402250719707e-05,
      "loss": 0.9003,
      "step": 627
    },
    {
      "epoch": 0.01643778246985534,
      "grad_norm": 16.823667526245117,
      "learning_rate": 1.0030881968071184e-05,
      "loss": 1.5238,
      "step": 628
    },
    {
      "epoch": 0.01646395728270543,
      "grad_norm": 20.3724422454834,
      "learning_rate": 1.0027741428945302e-05,
      "loss": 0.9671,
      "step": 629
    },
    {
      "epoch": 0.016490132095555516,
      "grad_norm": 14.112608909606934,
      "learning_rate": 1.002460088981942e-05,
      "loss": 1.4033,
      "step": 630
    },
    {
      "epoch": 0.016516306908405606,
      "grad_norm": 14.81662368774414,
      "learning_rate": 1.0021460350693535e-05,
      "loss": 1.6368,
      "step": 631
    },
    {
      "epoch": 0.016542481721255692,
      "grad_norm": 14.163483619689941,
      "learning_rate": 1.0018319811567653e-05,
      "loss": 1.4091,
      "step": 632
    },
    {
      "epoch": 0.01656865653410578,
      "grad_norm": 18.039451599121094,
      "learning_rate": 1.0015179272441769e-05,
      "loss": 1.3988,
      "step": 633
    },
    {
      "epoch": 0.016594831346955868,
      "grad_norm": 15.886393547058105,
      "learning_rate": 1.0012038733315886e-05,
      "loss": 1.1611,
      "step": 634
    },
    {
      "epoch": 0.016621006159805957,
      "grad_norm": 15.184565544128418,
      "learning_rate": 1.0008898194190002e-05,
      "loss": 0.7846,
      "step": 635
    },
    {
      "epoch": 0.016647180972656047,
      "grad_norm": 23.3873291015625,
      "learning_rate": 1.000575765506412e-05,
      "loss": 1.078,
      "step": 636
    },
    {
      "epoch": 0.016673355785506133,
      "grad_norm": 26.920629501342773,
      "learning_rate": 1.0002617115938236e-05,
      "loss": 0.6904,
      "step": 637
    },
    {
      "epoch": 0.016699530598356223,
      "grad_norm": 19.735254287719727,
      "learning_rate": 9.999476576812353e-06,
      "loss": 1.4643,
      "step": 638
    },
    {
      "epoch": 0.01672570541120631,
      "grad_norm": 13.736712455749512,
      "learning_rate": 9.99633603768647e-06,
      "loss": 0.6774,
      "step": 639
    },
    {
      "epoch": 0.0167518802240564,
      "grad_norm": 17.016040802001953,
      "learning_rate": 9.993195498560587e-06,
      "loss": 1.862,
      "step": 640
    },
    {
      "epoch": 0.016778055036906485,
      "grad_norm": 19.27604103088379,
      "learning_rate": 9.990054959434704e-06,
      "loss": 1.6934,
      "step": 641
    },
    {
      "epoch": 0.016804229849756575,
      "grad_norm": 12.857343673706055,
      "learning_rate": 9.98691442030882e-06,
      "loss": 0.7248,
      "step": 642
    },
    {
      "epoch": 0.01683040466260666,
      "grad_norm": 14.547774314880371,
      "learning_rate": 9.983773881182936e-06,
      "loss": 1.4196,
      "step": 643
    },
    {
      "epoch": 0.01685657947545675,
      "grad_norm": 19.974966049194336,
      "learning_rate": 9.980633342057053e-06,
      "loss": 1.8311,
      "step": 644
    },
    {
      "epoch": 0.01688275428830684,
      "grad_norm": 27.84545135498047,
      "learning_rate": 9.97749280293117e-06,
      "loss": 1.4235,
      "step": 645
    },
    {
      "epoch": 0.016908929101156926,
      "grad_norm": 17.007457733154297,
      "learning_rate": 9.974352263805287e-06,
      "loss": 1.1627,
      "step": 646
    },
    {
      "epoch": 0.016935103914007016,
      "grad_norm": 13.931382179260254,
      "learning_rate": 9.971211724679403e-06,
      "loss": 1.36,
      "step": 647
    },
    {
      "epoch": 0.016961278726857102,
      "grad_norm": 13.613231658935547,
      "learning_rate": 9.96807118555352e-06,
      "loss": 0.9076,
      "step": 648
    },
    {
      "epoch": 0.01698745353970719,
      "grad_norm": 17.082406997680664,
      "learning_rate": 9.964930646427636e-06,
      "loss": 0.6391,
      "step": 649
    },
    {
      "epoch": 0.017013628352557278,
      "grad_norm": 16.20777130126953,
      "learning_rate": 9.961790107301754e-06,
      "loss": 1.1833,
      "step": 650
    },
    {
      "epoch": 0.017039803165407368,
      "grad_norm": 15.951851844787598,
      "learning_rate": 9.958649568175871e-06,
      "loss": 1.6044,
      "step": 651
    },
    {
      "epoch": 0.017065977978257454,
      "grad_norm": 21.459369659423828,
      "learning_rate": 9.955509029049987e-06,
      "loss": 0.7955,
      "step": 652
    },
    {
      "epoch": 0.017092152791107543,
      "grad_norm": 20.098995208740234,
      "learning_rate": 9.952368489924105e-06,
      "loss": 0.8043,
      "step": 653
    },
    {
      "epoch": 0.017118327603957633,
      "grad_norm": 17.153419494628906,
      "learning_rate": 9.94922795079822e-06,
      "loss": 1.6519,
      "step": 654
    },
    {
      "epoch": 0.01714450241680772,
      "grad_norm": 23.202945709228516,
      "learning_rate": 9.946087411672338e-06,
      "loss": 1.0297,
      "step": 655
    },
    {
      "epoch": 0.01717067722965781,
      "grad_norm": 20.658382415771484,
      "learning_rate": 9.942946872546454e-06,
      "loss": 2.0945,
      "step": 656
    },
    {
      "epoch": 0.017196852042507895,
      "grad_norm": 15.702583312988281,
      "learning_rate": 9.939806333420571e-06,
      "loss": 1.7336,
      "step": 657
    },
    {
      "epoch": 0.017223026855357985,
      "grad_norm": 16.572620391845703,
      "learning_rate": 9.936665794294687e-06,
      "loss": 1.4731,
      "step": 658
    },
    {
      "epoch": 0.01724920166820807,
      "grad_norm": 25.60801887512207,
      "learning_rate": 9.933525255168803e-06,
      "loss": 0.713,
      "step": 659
    },
    {
      "epoch": 0.01727537648105816,
      "grad_norm": 27.76213264465332,
      "learning_rate": 9.93038471604292e-06,
      "loss": 1.2635,
      "step": 660
    },
    {
      "epoch": 0.01730155129390825,
      "grad_norm": 19.563274383544922,
      "learning_rate": 9.927244176917037e-06,
      "loss": 0.928,
      "step": 661
    },
    {
      "epoch": 0.017327726106758336,
      "grad_norm": 12.06052017211914,
      "learning_rate": 9.924103637791154e-06,
      "loss": 0.6016,
      "step": 662
    },
    {
      "epoch": 0.017353900919608426,
      "grad_norm": 23.698055267333984,
      "learning_rate": 9.920963098665272e-06,
      "loss": 1.2381,
      "step": 663
    },
    {
      "epoch": 0.017380075732458512,
      "grad_norm": 22.761512756347656,
      "learning_rate": 9.917822559539388e-06,
      "loss": 1.2595,
      "step": 664
    },
    {
      "epoch": 0.017406250545308602,
      "grad_norm": 26.94725799560547,
      "learning_rate": 9.914682020413505e-06,
      "loss": 0.7076,
      "step": 665
    },
    {
      "epoch": 0.017432425358158688,
      "grad_norm": 15.709563255310059,
      "learning_rate": 9.911541481287621e-06,
      "loss": 0.7692,
      "step": 666
    },
    {
      "epoch": 0.017458600171008778,
      "grad_norm": 17.319183349609375,
      "learning_rate": 9.908400942161739e-06,
      "loss": 1.0559,
      "step": 667
    },
    {
      "epoch": 0.017484774983858864,
      "grad_norm": 11.224342346191406,
      "learning_rate": 9.905260403035854e-06,
      "loss": 0.8501,
      "step": 668
    },
    {
      "epoch": 0.017510949796708954,
      "grad_norm": 31.263355255126953,
      "learning_rate": 9.902119863909972e-06,
      "loss": 1.5874,
      "step": 669
    },
    {
      "epoch": 0.017537124609559043,
      "grad_norm": 17.1675968170166,
      "learning_rate": 9.898979324784088e-06,
      "loss": 2.0209,
      "step": 670
    },
    {
      "epoch": 0.01756329942240913,
      "grad_norm": 27.857654571533203,
      "learning_rate": 9.895838785658205e-06,
      "loss": 1.3345,
      "step": 671
    },
    {
      "epoch": 0.01758947423525922,
      "grad_norm": 15.675129890441895,
      "learning_rate": 9.892698246532323e-06,
      "loss": 0.9799,
      "step": 672
    },
    {
      "epoch": 0.017615649048109305,
      "grad_norm": 17.52001953125,
      "learning_rate": 9.889557707406439e-06,
      "loss": 0.5756,
      "step": 673
    },
    {
      "epoch": 0.017641823860959395,
      "grad_norm": 17.825580596923828,
      "learning_rate": 9.886417168280555e-06,
      "loss": 1.1695,
      "step": 674
    },
    {
      "epoch": 0.01766799867380948,
      "grad_norm": 34.43243408203125,
      "learning_rate": 9.88327662915467e-06,
      "loss": 1.4855,
      "step": 675
    },
    {
      "epoch": 0.01769417348665957,
      "grad_norm": 13.802438735961914,
      "learning_rate": 9.880136090028788e-06,
      "loss": 1.2805,
      "step": 676
    },
    {
      "epoch": 0.017720348299509657,
      "grad_norm": 14.27303695678711,
      "learning_rate": 9.876995550902906e-06,
      "loss": 1.2247,
      "step": 677
    },
    {
      "epoch": 0.017746523112359747,
      "grad_norm": 16.53683853149414,
      "learning_rate": 9.873855011777021e-06,
      "loss": 1.0945,
      "step": 678
    },
    {
      "epoch": 0.017772697925209836,
      "grad_norm": 17.676671981811523,
      "learning_rate": 9.870714472651139e-06,
      "loss": 0.6955,
      "step": 679
    },
    {
      "epoch": 0.017798872738059923,
      "grad_norm": 19.15084457397461,
      "learning_rate": 9.867573933525255e-06,
      "loss": 1.3074,
      "step": 680
    },
    {
      "epoch": 0.017825047550910012,
      "grad_norm": 15.243623733520508,
      "learning_rate": 9.864433394399372e-06,
      "loss": 0.8546,
      "step": 681
    },
    {
      "epoch": 0.0178512223637601,
      "grad_norm": 17.337862014770508,
      "learning_rate": 9.861292855273488e-06,
      "loss": 1.3106,
      "step": 682
    },
    {
      "epoch": 0.017877397176610188,
      "grad_norm": 16.083049774169922,
      "learning_rate": 9.858152316147606e-06,
      "loss": 1.1712,
      "step": 683
    },
    {
      "epoch": 0.017903571989460274,
      "grad_norm": 16.112470626831055,
      "learning_rate": 9.855011777021723e-06,
      "loss": 1.0885,
      "step": 684
    },
    {
      "epoch": 0.017929746802310364,
      "grad_norm": 17.15744972229004,
      "learning_rate": 9.85187123789584e-06,
      "loss": 0.5098,
      "step": 685
    },
    {
      "epoch": 0.01795592161516045,
      "grad_norm": 14.356409072875977,
      "learning_rate": 9.848730698769957e-06,
      "loss": 0.5409,
      "step": 686
    },
    {
      "epoch": 0.01798209642801054,
      "grad_norm": 18.209165573120117,
      "learning_rate": 9.845590159644073e-06,
      "loss": 0.639,
      "step": 687
    },
    {
      "epoch": 0.01800827124086063,
      "grad_norm": 13.603002548217773,
      "learning_rate": 9.84244962051819e-06,
      "loss": 0.3668,
      "step": 688
    },
    {
      "epoch": 0.018034446053710716,
      "grad_norm": 21.50389289855957,
      "learning_rate": 9.839309081392306e-06,
      "loss": 1.9122,
      "step": 689
    },
    {
      "epoch": 0.018060620866560805,
      "grad_norm": 12.84823989868164,
      "learning_rate": 9.836168542266422e-06,
      "loss": 0.9989,
      "step": 690
    },
    {
      "epoch": 0.01808679567941089,
      "grad_norm": 10.444880485534668,
      "learning_rate": 9.83302800314054e-06,
      "loss": 0.4122,
      "step": 691
    },
    {
      "epoch": 0.01811297049226098,
      "grad_norm": 12.18832778930664,
      "learning_rate": 9.829887464014655e-06,
      "loss": 0.2205,
      "step": 692
    },
    {
      "epoch": 0.018139145305111067,
      "grad_norm": 13.163751602172852,
      "learning_rate": 9.826746924888773e-06,
      "loss": 0.4694,
      "step": 693
    },
    {
      "epoch": 0.018165320117961157,
      "grad_norm": 16.712440490722656,
      "learning_rate": 9.823606385762889e-06,
      "loss": 0.8977,
      "step": 694
    },
    {
      "epoch": 0.018191494930811243,
      "grad_norm": 14.04924201965332,
      "learning_rate": 9.820465846637006e-06,
      "loss": 0.449,
      "step": 695
    },
    {
      "epoch": 0.018217669743661333,
      "grad_norm": 9.60124397277832,
      "learning_rate": 9.817325307511122e-06,
      "loss": 0.7003,
      "step": 696
    },
    {
      "epoch": 0.018243844556511422,
      "grad_norm": 12.51932430267334,
      "learning_rate": 9.81418476838524e-06,
      "loss": 0.7259,
      "step": 697
    },
    {
      "epoch": 0.01827001936936151,
      "grad_norm": 11.624013900756836,
      "learning_rate": 9.811044229259357e-06,
      "loss": 0.7229,
      "step": 698
    },
    {
      "epoch": 0.018296194182211598,
      "grad_norm": 13.647758483886719,
      "learning_rate": 9.807903690133473e-06,
      "loss": 0.3062,
      "step": 699
    },
    {
      "epoch": 0.018322368995061684,
      "grad_norm": 13.513497352600098,
      "learning_rate": 9.80476315100759e-06,
      "loss": 0.8838,
      "step": 700
    },
    {
      "epoch": 0.018348543807911774,
      "grad_norm": 32.562904357910156,
      "learning_rate": 9.801622611881707e-06,
      "loss": 1.1553,
      "step": 701
    },
    {
      "epoch": 0.01837471862076186,
      "grad_norm": 20.50946044921875,
      "learning_rate": 9.798482072755824e-06,
      "loss": 0.9915,
      "step": 702
    },
    {
      "epoch": 0.01840089343361195,
      "grad_norm": 24.44700813293457,
      "learning_rate": 9.79534153362994e-06,
      "loss": 0.9622,
      "step": 703
    },
    {
      "epoch": 0.018427068246462036,
      "grad_norm": 18.90838050842285,
      "learning_rate": 9.792200994504057e-06,
      "loss": 1.5541,
      "step": 704
    },
    {
      "epoch": 0.018453243059312126,
      "grad_norm": 23.44221305847168,
      "learning_rate": 9.789060455378173e-06,
      "loss": 0.9468,
      "step": 705
    },
    {
      "epoch": 0.018479417872162215,
      "grad_norm": 16.447603225708008,
      "learning_rate": 9.78591991625229e-06,
      "loss": 0.5329,
      "step": 706
    },
    {
      "epoch": 0.0185055926850123,
      "grad_norm": 12.879626274108887,
      "learning_rate": 9.782779377126407e-06,
      "loss": 0.8984,
      "step": 707
    },
    {
      "epoch": 0.01853176749786239,
      "grad_norm": 18.267839431762695,
      "learning_rate": 9.779638838000523e-06,
      "loss": 0.4716,
      "step": 708
    },
    {
      "epoch": 0.018557942310712477,
      "grad_norm": 21.336830139160156,
      "learning_rate": 9.77649829887464e-06,
      "loss": 0.9637,
      "step": 709
    },
    {
      "epoch": 0.018584117123562567,
      "grad_norm": 16.403493881225586,
      "learning_rate": 9.773357759748758e-06,
      "loss": 0.9196,
      "step": 710
    },
    {
      "epoch": 0.018610291936412653,
      "grad_norm": 27.69179344177246,
      "learning_rate": 9.770217220622874e-06,
      "loss": 1.7087,
      "step": 711
    },
    {
      "epoch": 0.018636466749262743,
      "grad_norm": 14.248048782348633,
      "learning_rate": 9.767076681496991e-06,
      "loss": 1.428,
      "step": 712
    },
    {
      "epoch": 0.018662641562112833,
      "grad_norm": 21.588621139526367,
      "learning_rate": 9.763936142371107e-06,
      "loss": 1.364,
      "step": 713
    },
    {
      "epoch": 0.01868881637496292,
      "grad_norm": 15.811211585998535,
      "learning_rate": 9.760795603245225e-06,
      "loss": 0.9928,
      "step": 714
    },
    {
      "epoch": 0.01871499118781301,
      "grad_norm": 15.685888290405273,
      "learning_rate": 9.75765506411934e-06,
      "loss": 1.4014,
      "step": 715
    },
    {
      "epoch": 0.018741166000663095,
      "grad_norm": 13.539826393127441,
      "learning_rate": 9.754514524993458e-06,
      "loss": 0.8617,
      "step": 716
    },
    {
      "epoch": 0.018767340813513184,
      "grad_norm": 26.868104934692383,
      "learning_rate": 9.751373985867574e-06,
      "loss": 0.871,
      "step": 717
    },
    {
      "epoch": 0.01879351562636327,
      "grad_norm": 23.70384407043457,
      "learning_rate": 9.748233446741691e-06,
      "loss": 0.8109,
      "step": 718
    },
    {
      "epoch": 0.01881969043921336,
      "grad_norm": 22.60154914855957,
      "learning_rate": 9.745092907615809e-06,
      "loss": 0.9675,
      "step": 719
    },
    {
      "epoch": 0.018845865252063446,
      "grad_norm": 17.463958740234375,
      "learning_rate": 9.741952368489925e-06,
      "loss": 1.4124,
      "step": 720
    },
    {
      "epoch": 0.018872040064913536,
      "grad_norm": 19.23381805419922,
      "learning_rate": 9.73881182936404e-06,
      "loss": 1.0755,
      "step": 721
    },
    {
      "epoch": 0.018898214877763626,
      "grad_norm": 20.375213623046875,
      "learning_rate": 9.735671290238157e-06,
      "loss": 0.3726,
      "step": 722
    },
    {
      "epoch": 0.018924389690613712,
      "grad_norm": 18.133026123046875,
      "learning_rate": 9.732530751112274e-06,
      "loss": 1.4864,
      "step": 723
    },
    {
      "epoch": 0.0189505645034638,
      "grad_norm": 12.9170560836792,
      "learning_rate": 9.729390211986392e-06,
      "loss": 0.4277,
      "step": 724
    },
    {
      "epoch": 0.018976739316313888,
      "grad_norm": 16.993864059448242,
      "learning_rate": 9.726249672860507e-06,
      "loss": 0.5723,
      "step": 725
    },
    {
      "epoch": 0.019002914129163977,
      "grad_norm": 16.910964965820312,
      "learning_rate": 9.723109133734625e-06,
      "loss": 1.581,
      "step": 726
    },
    {
      "epoch": 0.019029088942014064,
      "grad_norm": 20.793357849121094,
      "learning_rate": 9.719968594608741e-06,
      "loss": 1.3553,
      "step": 727
    },
    {
      "epoch": 0.019055263754864153,
      "grad_norm": 17.70785903930664,
      "learning_rate": 9.716828055482858e-06,
      "loss": 0.6134,
      "step": 728
    },
    {
      "epoch": 0.01908143856771424,
      "grad_norm": 24.655664443969727,
      "learning_rate": 9.713687516356974e-06,
      "loss": 1.293,
      "step": 729
    },
    {
      "epoch": 0.01910761338056433,
      "grad_norm": 18.088884353637695,
      "learning_rate": 9.710546977231092e-06,
      "loss": 0.7405,
      "step": 730
    },
    {
      "epoch": 0.01913378819341442,
      "grad_norm": 18.831151962280273,
      "learning_rate": 9.70740643810521e-06,
      "loss": 0.8394,
      "step": 731
    },
    {
      "epoch": 0.019159963006264505,
      "grad_norm": 21.053407669067383,
      "learning_rate": 9.704265898979325e-06,
      "loss": 0.561,
      "step": 732
    },
    {
      "epoch": 0.019186137819114595,
      "grad_norm": 19.18998146057129,
      "learning_rate": 9.701125359853443e-06,
      "loss": 1.0119,
      "step": 733
    },
    {
      "epoch": 0.01921231263196468,
      "grad_norm": 17.89796257019043,
      "learning_rate": 9.697984820727559e-06,
      "loss": 1.3368,
      "step": 734
    },
    {
      "epoch": 0.01923848744481477,
      "grad_norm": 18.24168586730957,
      "learning_rate": 9.694844281601676e-06,
      "loss": 1.0352,
      "step": 735
    },
    {
      "epoch": 0.019264662257664857,
      "grad_norm": 24.430374145507812,
      "learning_rate": 9.691703742475792e-06,
      "loss": 0.543,
      "step": 736
    },
    {
      "epoch": 0.019290837070514946,
      "grad_norm": 14.245957374572754,
      "learning_rate": 9.688563203349908e-06,
      "loss": 1.0272,
      "step": 737
    },
    {
      "epoch": 0.019317011883365032,
      "grad_norm": 19.776704788208008,
      "learning_rate": 9.685422664224025e-06,
      "loss": 1.6423,
      "step": 738
    },
    {
      "epoch": 0.019343186696215122,
      "grad_norm": 14.0235595703125,
      "learning_rate": 9.682282125098141e-06,
      "loss": 0.9076,
      "step": 739
    },
    {
      "epoch": 0.01936936150906521,
      "grad_norm": 35.515357971191406,
      "learning_rate": 9.679141585972259e-06,
      "loss": 1.1531,
      "step": 740
    },
    {
      "epoch": 0.019395536321915298,
      "grad_norm": 20.482864379882812,
      "learning_rate": 9.676001046846375e-06,
      "loss": 1.2819,
      "step": 741
    },
    {
      "epoch": 0.019421711134765388,
      "grad_norm": 11.562335968017578,
      "learning_rate": 9.672860507720492e-06,
      "loss": 0.7234,
      "step": 742
    },
    {
      "epoch": 0.019447885947615474,
      "grad_norm": 22.146974563598633,
      "learning_rate": 9.669719968594608e-06,
      "loss": 1.5042,
      "step": 743
    },
    {
      "epoch": 0.019474060760465563,
      "grad_norm": 13.914840698242188,
      "learning_rate": 9.666579429468726e-06,
      "loss": 0.3689,
      "step": 744
    },
    {
      "epoch": 0.01950023557331565,
      "grad_norm": 19.512216567993164,
      "learning_rate": 9.663438890342843e-06,
      "loss": 0.262,
      "step": 745
    },
    {
      "epoch": 0.01952641038616574,
      "grad_norm": 17.122352600097656,
      "learning_rate": 9.660298351216959e-06,
      "loss": 1.5711,
      "step": 746
    },
    {
      "epoch": 0.019552585199015825,
      "grad_norm": 16.549701690673828,
      "learning_rate": 9.657157812091077e-06,
      "loss": 1.4184,
      "step": 747
    },
    {
      "epoch": 0.019578760011865915,
      "grad_norm": 41.34917449951172,
      "learning_rate": 9.654017272965193e-06,
      "loss": 0.8543,
      "step": 748
    },
    {
      "epoch": 0.019604934824716005,
      "grad_norm": 39.634857177734375,
      "learning_rate": 9.65087673383931e-06,
      "loss": 2.4896,
      "step": 749
    },
    {
      "epoch": 0.01963110963756609,
      "grad_norm": 23.902969360351562,
      "learning_rate": 9.647736194713426e-06,
      "loss": 0.8284,
      "step": 750
    },
    {
      "epoch": 0.01965728445041618,
      "grad_norm": 17.309837341308594,
      "learning_rate": 9.644595655587544e-06,
      "loss": 0.6987,
      "step": 751
    },
    {
      "epoch": 0.019683459263266267,
      "grad_norm": 21.494098663330078,
      "learning_rate": 9.64145511646166e-06,
      "loss": 1.3827,
      "step": 752
    },
    {
      "epoch": 0.019709634076116356,
      "grad_norm": 27.717483520507812,
      "learning_rate": 9.638314577335775e-06,
      "loss": 1.4204,
      "step": 753
    },
    {
      "epoch": 0.019735808888966443,
      "grad_norm": 24.596920013427734,
      "learning_rate": 9.635174038209893e-06,
      "loss": 0.9197,
      "step": 754
    },
    {
      "epoch": 0.019761983701816532,
      "grad_norm": 9.76498794555664,
      "learning_rate": 9.632033499084009e-06,
      "loss": 0.7534,
      "step": 755
    },
    {
      "epoch": 0.01978815851466662,
      "grad_norm": 12.94786262512207,
      "learning_rate": 9.628892959958126e-06,
      "loss": 1.1284,
      "step": 756
    },
    {
      "epoch": 0.019814333327516708,
      "grad_norm": 26.44408416748047,
      "learning_rate": 9.625752420832244e-06,
      "loss": 1.7044,
      "step": 757
    },
    {
      "epoch": 0.019840508140366798,
      "grad_norm": 17.869482040405273,
      "learning_rate": 9.62261188170636e-06,
      "loss": 0.7075,
      "step": 758
    },
    {
      "epoch": 0.019866682953216884,
      "grad_norm": 20.225215911865234,
      "learning_rate": 9.619471342580477e-06,
      "loss": 1.1812,
      "step": 759
    },
    {
      "epoch": 0.019892857766066974,
      "grad_norm": 16.958093643188477,
      "learning_rate": 9.616330803454593e-06,
      "loss": 1.4305,
      "step": 760
    },
    {
      "epoch": 0.01991903257891706,
      "grad_norm": 24.149658203125,
      "learning_rate": 9.61319026432871e-06,
      "loss": 0.6364,
      "step": 761
    },
    {
      "epoch": 0.01994520739176715,
      "grad_norm": 30.773408889770508,
      "learning_rate": 9.610049725202826e-06,
      "loss": 1.7494,
      "step": 762
    },
    {
      "epoch": 0.019971382204617236,
      "grad_norm": 49.424156188964844,
      "learning_rate": 9.606909186076944e-06,
      "loss": 0.5795,
      "step": 763
    },
    {
      "epoch": 0.019997557017467325,
      "grad_norm": 20.740760803222656,
      "learning_rate": 9.60376864695106e-06,
      "loss": 1.1851,
      "step": 764
    },
    {
      "epoch": 0.02002373183031741,
      "grad_norm": 20.17764663696289,
      "learning_rate": 9.600628107825177e-06,
      "loss": 1.5211,
      "step": 765
    },
    {
      "epoch": 0.0200499066431675,
      "grad_norm": 15.325651168823242,
      "learning_rate": 9.597487568699295e-06,
      "loss": 0.8559,
      "step": 766
    },
    {
      "epoch": 0.02007608145601759,
      "grad_norm": 18.244585037231445,
      "learning_rate": 9.594347029573409e-06,
      "loss": 1.2824,
      "step": 767
    },
    {
      "epoch": 0.020102256268867677,
      "grad_norm": 26.134756088256836,
      "learning_rate": 9.591206490447527e-06,
      "loss": 0.8992,
      "step": 768
    },
    {
      "epoch": 0.020128431081717767,
      "grad_norm": 20.381423950195312,
      "learning_rate": 9.588065951321643e-06,
      "loss": 0.8284,
      "step": 769
    },
    {
      "epoch": 0.020154605894567853,
      "grad_norm": 19.88959312438965,
      "learning_rate": 9.58492541219576e-06,
      "loss": 1.0607,
      "step": 770
    },
    {
      "epoch": 0.020180780707417943,
      "grad_norm": 16.797544479370117,
      "learning_rate": 9.581784873069878e-06,
      "loss": 1.3892,
      "step": 771
    },
    {
      "epoch": 0.02020695552026803,
      "grad_norm": 19.421751022338867,
      "learning_rate": 9.578644333943994e-06,
      "loss": 0.8593,
      "step": 772
    },
    {
      "epoch": 0.02023313033311812,
      "grad_norm": 24.74592399597168,
      "learning_rate": 9.575503794818111e-06,
      "loss": 0.7753,
      "step": 773
    },
    {
      "epoch": 0.020259305145968208,
      "grad_norm": 15.516802787780762,
      "learning_rate": 9.572363255692227e-06,
      "loss": 1.8316,
      "step": 774
    },
    {
      "epoch": 0.020285479958818294,
      "grad_norm": 17.479339599609375,
      "learning_rate": 9.569222716566344e-06,
      "loss": 0.8337,
      "step": 775
    },
    {
      "epoch": 0.020311654771668384,
      "grad_norm": 17.79015350341797,
      "learning_rate": 9.56608217744046e-06,
      "loss": 1.3226,
      "step": 776
    },
    {
      "epoch": 0.02033782958451847,
      "grad_norm": 13.167654991149902,
      "learning_rate": 9.562941638314578e-06,
      "loss": 0.2478,
      "step": 777
    },
    {
      "epoch": 0.02036400439736856,
      "grad_norm": 23.760839462280273,
      "learning_rate": 9.559801099188695e-06,
      "loss": 0.6621,
      "step": 778
    },
    {
      "epoch": 0.020390179210218646,
      "grad_norm": 14.234769821166992,
      "learning_rate": 9.556660560062811e-06,
      "loss": 0.7955,
      "step": 779
    },
    {
      "epoch": 0.020416354023068736,
      "grad_norm": 20.016338348388672,
      "learning_rate": 9.553520020936929e-06,
      "loss": 0.4906,
      "step": 780
    },
    {
      "epoch": 0.020442528835918822,
      "grad_norm": 15.903776168823242,
      "learning_rate": 9.550379481811045e-06,
      "loss": 0.8145,
      "step": 781
    },
    {
      "epoch": 0.02046870364876891,
      "grad_norm": 10.543067932128906,
      "learning_rate": 9.547238942685162e-06,
      "loss": 1.0888,
      "step": 782
    },
    {
      "epoch": 0.020494878461619,
      "grad_norm": 16.61180877685547,
      "learning_rate": 9.544098403559278e-06,
      "loss": 0.898,
      "step": 783
    },
    {
      "epoch": 0.020521053274469087,
      "grad_norm": 20.497962951660156,
      "learning_rate": 9.540957864433394e-06,
      "loss": 0.3235,
      "step": 784
    },
    {
      "epoch": 0.020547228087319177,
      "grad_norm": 19.104768753051758,
      "learning_rate": 9.537817325307512e-06,
      "loss": 0.98,
      "step": 785
    },
    {
      "epoch": 0.020573402900169263,
      "grad_norm": 36.84475326538086,
      "learning_rate": 9.534676786181627e-06,
      "loss": 1.5279,
      "step": 786
    },
    {
      "epoch": 0.020599577713019353,
      "grad_norm": 18.15375518798828,
      "learning_rate": 9.531536247055745e-06,
      "loss": 0.7684,
      "step": 787
    },
    {
      "epoch": 0.02062575252586944,
      "grad_norm": 17.69504165649414,
      "learning_rate": 9.52839570792986e-06,
      "loss": 0.8457,
      "step": 788
    },
    {
      "epoch": 0.02065192733871953,
      "grad_norm": 17.214595794677734,
      "learning_rate": 9.525255168803978e-06,
      "loss": 1.2459,
      "step": 789
    },
    {
      "epoch": 0.020678102151569615,
      "grad_norm": 20.988004684448242,
      "learning_rate": 9.522114629678094e-06,
      "loss": 0.6531,
      "step": 790
    },
    {
      "epoch": 0.020704276964419704,
      "grad_norm": 18.54210090637207,
      "learning_rate": 9.518974090552212e-06,
      "loss": 1.1261,
      "step": 791
    },
    {
      "epoch": 0.020730451777269794,
      "grad_norm": 18.410629272460938,
      "learning_rate": 9.51583355142633e-06,
      "loss": 1.41,
      "step": 792
    },
    {
      "epoch": 0.02075662659011988,
      "grad_norm": 20.796512603759766,
      "learning_rate": 9.512693012300445e-06,
      "loss": 0.9583,
      "step": 793
    },
    {
      "epoch": 0.02078280140296997,
      "grad_norm": 18.472131729125977,
      "learning_rate": 9.509552473174563e-06,
      "loss": 1.3308,
      "step": 794
    },
    {
      "epoch": 0.020808976215820056,
      "grad_norm": 16.66689682006836,
      "learning_rate": 9.506411934048679e-06,
      "loss": 0.5378,
      "step": 795
    },
    {
      "epoch": 0.020835151028670146,
      "grad_norm": 16.13934898376465,
      "learning_rate": 9.503271394922796e-06,
      "loss": 0.9605,
      "step": 796
    },
    {
      "epoch": 0.020861325841520232,
      "grad_norm": 14.453042984008789,
      "learning_rate": 9.500130855796912e-06,
      "loss": 1.1476,
      "step": 797
    },
    {
      "epoch": 0.02088750065437032,
      "grad_norm": 17.827566146850586,
      "learning_rate": 9.496990316671028e-06,
      "loss": 0.7694,
      "step": 798
    },
    {
      "epoch": 0.020913675467220408,
      "grad_norm": 20.049230575561523,
      "learning_rate": 9.493849777545145e-06,
      "loss": 0.9847,
      "step": 799
    },
    {
      "epoch": 0.020939850280070497,
      "grad_norm": 18.180843353271484,
      "learning_rate": 9.490709238419261e-06,
      "loss": 0.7593,
      "step": 800
    },
    {
      "epoch": 0.020966025092920587,
      "grad_norm": 22.90544319152832,
      "learning_rate": 9.487568699293379e-06,
      "loss": 1.9158,
      "step": 801
    },
    {
      "epoch": 0.020992199905770673,
      "grad_norm": 15.583829879760742,
      "learning_rate": 9.484428160167495e-06,
      "loss": 0.7942,
      "step": 802
    },
    {
      "epoch": 0.021018374718620763,
      "grad_norm": 15.503565788269043,
      "learning_rate": 9.481287621041612e-06,
      "loss": 1.1382,
      "step": 803
    },
    {
      "epoch": 0.02104454953147085,
      "grad_norm": 16.46419334411621,
      "learning_rate": 9.47814708191573e-06,
      "loss": 0.7715,
      "step": 804
    },
    {
      "epoch": 0.02107072434432094,
      "grad_norm": 14.946967124938965,
      "learning_rate": 9.475006542789846e-06,
      "loss": 1.55,
      "step": 805
    },
    {
      "epoch": 0.021096899157171025,
      "grad_norm": 19.88494873046875,
      "learning_rate": 9.471866003663963e-06,
      "loss": 1.4033,
      "step": 806
    },
    {
      "epoch": 0.021123073970021115,
      "grad_norm": 13.883177757263184,
      "learning_rate": 9.468725464538079e-06,
      "loss": 0.7306,
      "step": 807
    },
    {
      "epoch": 0.0211492487828712,
      "grad_norm": 21.33315658569336,
      "learning_rate": 9.465584925412197e-06,
      "loss": 0.9368,
      "step": 808
    },
    {
      "epoch": 0.02117542359572129,
      "grad_norm": 22.590017318725586,
      "learning_rate": 9.462444386286312e-06,
      "loss": 1.7683,
      "step": 809
    },
    {
      "epoch": 0.02120159840857138,
      "grad_norm": 21.756120681762695,
      "learning_rate": 9.45930384716043e-06,
      "loss": 1.4662,
      "step": 810
    },
    {
      "epoch": 0.021227773221421466,
      "grad_norm": 23.95503044128418,
      "learning_rate": 9.456163308034546e-06,
      "loss": 1.5254,
      "step": 811
    },
    {
      "epoch": 0.021253948034271556,
      "grad_norm": 13.111517906188965,
      "learning_rate": 9.453022768908663e-06,
      "loss": 0.826,
      "step": 812
    },
    {
      "epoch": 0.021280122847121642,
      "grad_norm": 41.495079040527344,
      "learning_rate": 9.449882229782781e-06,
      "loss": 0.5876,
      "step": 813
    },
    {
      "epoch": 0.021306297659971732,
      "grad_norm": 22.80171775817871,
      "learning_rate": 9.446741690656895e-06,
      "loss": 0.6666,
      "step": 814
    },
    {
      "epoch": 0.021332472472821818,
      "grad_norm": 14.649494171142578,
      "learning_rate": 9.443601151531013e-06,
      "loss": 0.8617,
      "step": 815
    },
    {
      "epoch": 0.021358647285671908,
      "grad_norm": 19.83968734741211,
      "learning_rate": 9.440460612405129e-06,
      "loss": 0.7347,
      "step": 816
    },
    {
      "epoch": 0.021384822098521994,
      "grad_norm": 18.460817337036133,
      "learning_rate": 9.437320073279246e-06,
      "loss": 1.3103,
      "step": 817
    },
    {
      "epoch": 0.021410996911372084,
      "grad_norm": 13.688819885253906,
      "learning_rate": 9.434179534153364e-06,
      "loss": 0.5554,
      "step": 818
    },
    {
      "epoch": 0.021437171724222173,
      "grad_norm": 21.763050079345703,
      "learning_rate": 9.43103899502748e-06,
      "loss": 0.9539,
      "step": 819
    },
    {
      "epoch": 0.02146334653707226,
      "grad_norm": 16.133323669433594,
      "learning_rate": 9.427898455901597e-06,
      "loss": 1.0567,
      "step": 820
    },
    {
      "epoch": 0.02148952134992235,
      "grad_norm": 20.971946716308594,
      "learning_rate": 9.424757916775713e-06,
      "loss": 1.0149,
      "step": 821
    },
    {
      "epoch": 0.021515696162772435,
      "grad_norm": 13.58545207977295,
      "learning_rate": 9.42161737764983e-06,
      "loss": 0.9597,
      "step": 822
    },
    {
      "epoch": 0.021541870975622525,
      "grad_norm": 11.846320152282715,
      "learning_rate": 9.418476838523946e-06,
      "loss": 0.7976,
      "step": 823
    },
    {
      "epoch": 0.02156804578847261,
      "grad_norm": 13.286391258239746,
      "learning_rate": 9.415336299398064e-06,
      "loss": 0.8484,
      "step": 824
    },
    {
      "epoch": 0.0215942206013227,
      "grad_norm": 20.09535026550293,
      "learning_rate": 9.412195760272181e-06,
      "loss": 0.9009,
      "step": 825
    },
    {
      "epoch": 0.02162039541417279,
      "grad_norm": 30.039936065673828,
      "learning_rate": 9.409055221146297e-06,
      "loss": 1.0417,
      "step": 826
    },
    {
      "epoch": 0.021646570227022877,
      "grad_norm": 18.605485916137695,
      "learning_rate": 9.405914682020415e-06,
      "loss": 1.3671,
      "step": 827
    },
    {
      "epoch": 0.021672745039872966,
      "grad_norm": 16.464641571044922,
      "learning_rate": 9.40277414289453e-06,
      "loss": 1.4686,
      "step": 828
    },
    {
      "epoch": 0.021698919852723052,
      "grad_norm": 14.652993202209473,
      "learning_rate": 9.399633603768647e-06,
      "loss": 0.6242,
      "step": 829
    },
    {
      "epoch": 0.021725094665573142,
      "grad_norm": 11.027660369873047,
      "learning_rate": 9.396493064642764e-06,
      "loss": 0.5346,
      "step": 830
    },
    {
      "epoch": 0.02175126947842323,
      "grad_norm": 21.316246032714844,
      "learning_rate": 9.39335252551688e-06,
      "loss": 1.2364,
      "step": 831
    },
    {
      "epoch": 0.021777444291273318,
      "grad_norm": 12.937393188476562,
      "learning_rate": 9.390211986390998e-06,
      "loss": 0.8563,
      "step": 832
    },
    {
      "epoch": 0.021803619104123404,
      "grad_norm": 12.538131713867188,
      "learning_rate": 9.387071447265113e-06,
      "loss": 0.611,
      "step": 833
    },
    {
      "epoch": 0.021829793916973494,
      "grad_norm": 26.206907272338867,
      "learning_rate": 9.383930908139231e-06,
      "loss": 0.6199,
      "step": 834
    },
    {
      "epoch": 0.021855968729823583,
      "grad_norm": 15.744893074035645,
      "learning_rate": 9.380790369013347e-06,
      "loss": 0.8119,
      "step": 835
    },
    {
      "epoch": 0.02188214354267367,
      "grad_norm": 18.580564498901367,
      "learning_rate": 9.377649829887464e-06,
      "loss": 1.0415,
      "step": 836
    },
    {
      "epoch": 0.02190831835552376,
      "grad_norm": 23.435142517089844,
      "learning_rate": 9.37450929076158e-06,
      "loss": 0.9622,
      "step": 837
    },
    {
      "epoch": 0.021934493168373845,
      "grad_norm": 12.608865737915039,
      "learning_rate": 9.371368751635698e-06,
      "loss": 0.8076,
      "step": 838
    },
    {
      "epoch": 0.021960667981223935,
      "grad_norm": 22.230405807495117,
      "learning_rate": 9.368228212509815e-06,
      "loss": 0.6271,
      "step": 839
    },
    {
      "epoch": 0.02198684279407402,
      "grad_norm": 17.52166748046875,
      "learning_rate": 9.365087673383931e-06,
      "loss": 0.6983,
      "step": 840
    },
    {
      "epoch": 0.02201301760692411,
      "grad_norm": 14.322993278503418,
      "learning_rate": 9.361947134258049e-06,
      "loss": 0.41,
      "step": 841
    },
    {
      "epoch": 0.022039192419774197,
      "grad_norm": 19.112977981567383,
      "learning_rate": 9.358806595132165e-06,
      "loss": 1.4177,
      "step": 842
    },
    {
      "epoch": 0.022065367232624287,
      "grad_norm": 16.49955940246582,
      "learning_rate": 9.355666056006282e-06,
      "loss": 0.7804,
      "step": 843
    },
    {
      "epoch": 0.022091542045474376,
      "grad_norm": 14.767380714416504,
      "learning_rate": 9.352525516880398e-06,
      "loss": 0.4157,
      "step": 844
    },
    {
      "epoch": 0.022117716858324463,
      "grad_norm": 17.366716384887695,
      "learning_rate": 9.349384977754514e-06,
      "loss": 0.7004,
      "step": 845
    },
    {
      "epoch": 0.022143891671174552,
      "grad_norm": 17.415742874145508,
      "learning_rate": 9.346244438628631e-06,
      "loss": 0.4496,
      "step": 846
    },
    {
      "epoch": 0.02217006648402464,
      "grad_norm": 24.452388763427734,
      "learning_rate": 9.343103899502747e-06,
      "loss": 1.1579,
      "step": 847
    },
    {
      "epoch": 0.022196241296874728,
      "grad_norm": 14.396705627441406,
      "learning_rate": 9.339963360376865e-06,
      "loss": 1.0059,
      "step": 848
    },
    {
      "epoch": 0.022222416109724814,
      "grad_norm": 16.048259735107422,
      "learning_rate": 9.33682282125098e-06,
      "loss": 0.5732,
      "step": 849
    },
    {
      "epoch": 0.022248590922574904,
      "grad_norm": 13.568750381469727,
      "learning_rate": 9.333682282125098e-06,
      "loss": 0.6994,
      "step": 850
    },
    {
      "epoch": 0.02227476573542499,
      "grad_norm": 17.99097442626953,
      "learning_rate": 9.330541742999216e-06,
      "loss": 1.6178,
      "step": 851
    },
    {
      "epoch": 0.02230094054827508,
      "grad_norm": 16.24090003967285,
      "learning_rate": 9.327401203873332e-06,
      "loss": 0.8511,
      "step": 852
    },
    {
      "epoch": 0.02232711536112517,
      "grad_norm": 19.298377990722656,
      "learning_rate": 9.32426066474745e-06,
      "loss": 0.8962,
      "step": 853
    },
    {
      "epoch": 0.022353290173975256,
      "grad_norm": 18.328216552734375,
      "learning_rate": 9.321120125621565e-06,
      "loss": 1.577,
      "step": 854
    },
    {
      "epoch": 0.022379464986825345,
      "grad_norm": 16.088275909423828,
      "learning_rate": 9.317979586495683e-06,
      "loss": 0.5868,
      "step": 855
    },
    {
      "epoch": 0.02240563979967543,
      "grad_norm": 17.16234016418457,
      "learning_rate": 9.314839047369799e-06,
      "loss": 0.5742,
      "step": 856
    },
    {
      "epoch": 0.02243181461252552,
      "grad_norm": 18.589988708496094,
      "learning_rate": 9.311698508243916e-06,
      "loss": 0.8341,
      "step": 857
    },
    {
      "epoch": 0.022457989425375607,
      "grad_norm": 19.9235897064209,
      "learning_rate": 9.308557969118032e-06,
      "loss": 0.9547,
      "step": 858
    },
    {
      "epoch": 0.022484164238225697,
      "grad_norm": 21.933839797973633,
      "learning_rate": 9.30541742999215e-06,
      "loss": 0.2659,
      "step": 859
    },
    {
      "epoch": 0.022510339051075783,
      "grad_norm": 20.53569984436035,
      "learning_rate": 9.302276890866265e-06,
      "loss": 0.798,
      "step": 860
    },
    {
      "epoch": 0.022536513863925873,
      "grad_norm": 10.95396614074707,
      "learning_rate": 9.299136351740381e-06,
      "loss": 0.3219,
      "step": 861
    },
    {
      "epoch": 0.022562688676775963,
      "grad_norm": 11.00860595703125,
      "learning_rate": 9.295995812614499e-06,
      "loss": 0.3292,
      "step": 862
    },
    {
      "epoch": 0.02258886348962605,
      "grad_norm": 14.921107292175293,
      "learning_rate": 9.292855273488615e-06,
      "loss": 1.1664,
      "step": 863
    },
    {
      "epoch": 0.02261503830247614,
      "grad_norm": 24.649066925048828,
      "learning_rate": 9.289714734362732e-06,
      "loss": 1.2226,
      "step": 864
    },
    {
      "epoch": 0.022641213115326225,
      "grad_norm": 22.066865921020508,
      "learning_rate": 9.28657419523685e-06,
      "loss": 1.5929,
      "step": 865
    },
    {
      "epoch": 0.022667387928176314,
      "grad_norm": 23.01406478881836,
      "learning_rate": 9.283433656110966e-06,
      "loss": 1.0643,
      "step": 866
    },
    {
      "epoch": 0.0226935627410264,
      "grad_norm": 18.122507095336914,
      "learning_rate": 9.280293116985083e-06,
      "loss": 1.2052,
      "step": 867
    },
    {
      "epoch": 0.02271973755387649,
      "grad_norm": 25.248008728027344,
      "learning_rate": 9.277152577859199e-06,
      "loss": 1.1317,
      "step": 868
    },
    {
      "epoch": 0.022745912366726576,
      "grad_norm": 19.595775604248047,
      "learning_rate": 9.274012038733317e-06,
      "loss": 0.7689,
      "step": 869
    },
    {
      "epoch": 0.022772087179576666,
      "grad_norm": 14.219436645507812,
      "learning_rate": 9.270871499607432e-06,
      "loss": 0.3689,
      "step": 870
    },
    {
      "epoch": 0.022798261992426756,
      "grad_norm": 11.443872451782227,
      "learning_rate": 9.26773096048155e-06,
      "loss": 0.275,
      "step": 871
    },
    {
      "epoch": 0.022824436805276842,
      "grad_norm": 20.41217041015625,
      "learning_rate": 9.264590421355668e-06,
      "loss": 1.3332,
      "step": 872
    },
    {
      "epoch": 0.02285061161812693,
      "grad_norm": 13.329365730285645,
      "learning_rate": 9.261449882229783e-06,
      "loss": 0.7467,
      "step": 873
    },
    {
      "epoch": 0.022876786430977018,
      "grad_norm": 13.33541488647461,
      "learning_rate": 9.258309343103901e-06,
      "loss": 0.8206,
      "step": 874
    },
    {
      "epoch": 0.022902961243827107,
      "grad_norm": 29.824663162231445,
      "learning_rate": 9.255168803978017e-06,
      "loss": 1.269,
      "step": 875
    },
    {
      "epoch": 0.022929136056677193,
      "grad_norm": 25.781095504760742,
      "learning_rate": 9.252028264852133e-06,
      "loss": 1.1239,
      "step": 876
    },
    {
      "epoch": 0.022955310869527283,
      "grad_norm": 36.34675216674805,
      "learning_rate": 9.24888772572625e-06,
      "loss": 1.8357,
      "step": 877
    },
    {
      "epoch": 0.02298148568237737,
      "grad_norm": 14.29274845123291,
      "learning_rate": 9.245747186600366e-06,
      "loss": 0.4518,
      "step": 878
    },
    {
      "epoch": 0.02300766049522746,
      "grad_norm": 13.736344337463379,
      "learning_rate": 9.242606647474484e-06,
      "loss": 0.331,
      "step": 879
    },
    {
      "epoch": 0.02303383530807755,
      "grad_norm": 16.67820167541504,
      "learning_rate": 9.2394661083486e-06,
      "loss": 0.9536,
      "step": 880
    },
    {
      "epoch": 0.023060010120927635,
      "grad_norm": 16.06148910522461,
      "learning_rate": 9.236325569222717e-06,
      "loss": 0.4296,
      "step": 881
    },
    {
      "epoch": 0.023086184933777724,
      "grad_norm": 17.447650909423828,
      "learning_rate": 9.233185030096833e-06,
      "loss": 1.0124,
      "step": 882
    },
    {
      "epoch": 0.02311235974662781,
      "grad_norm": 18.11922264099121,
      "learning_rate": 9.23004449097095e-06,
      "loss": 0.7357,
      "step": 883
    },
    {
      "epoch": 0.0231385345594779,
      "grad_norm": 15.388917922973633,
      "learning_rate": 9.226903951845066e-06,
      "loss": 0.872,
      "step": 884
    },
    {
      "epoch": 0.023164709372327986,
      "grad_norm": 28.77885627746582,
      "learning_rate": 9.223763412719184e-06,
      "loss": 0.9286,
      "step": 885
    },
    {
      "epoch": 0.023190884185178076,
      "grad_norm": 21.84748077392578,
      "learning_rate": 9.220622873593301e-06,
      "loss": 0.8844,
      "step": 886
    },
    {
      "epoch": 0.023217058998028166,
      "grad_norm": 12.316901206970215,
      "learning_rate": 9.217482334467417e-06,
      "loss": 0.4265,
      "step": 887
    },
    {
      "epoch": 0.023243233810878252,
      "grad_norm": 20.639610290527344,
      "learning_rate": 9.214341795341535e-06,
      "loss": 0.5737,
      "step": 888
    },
    {
      "epoch": 0.02326940862372834,
      "grad_norm": 20.400196075439453,
      "learning_rate": 9.21120125621565e-06,
      "loss": 1.8349,
      "step": 889
    },
    {
      "epoch": 0.023295583436578428,
      "grad_norm": 17.621192932128906,
      "learning_rate": 9.208060717089768e-06,
      "loss": 1.3701,
      "step": 890
    },
    {
      "epoch": 0.023321758249428517,
      "grad_norm": 14.860630989074707,
      "learning_rate": 9.204920177963884e-06,
      "loss": 0.5056,
      "step": 891
    },
    {
      "epoch": 0.023347933062278604,
      "grad_norm": 12.993891716003418,
      "learning_rate": 9.201779638838e-06,
      "loss": 0.4475,
      "step": 892
    },
    {
      "epoch": 0.023374107875128693,
      "grad_norm": 13.648996353149414,
      "learning_rate": 9.198639099712118e-06,
      "loss": 0.8555,
      "step": 893
    },
    {
      "epoch": 0.02340028268797878,
      "grad_norm": 23.163522720336914,
      "learning_rate": 9.195498560586233e-06,
      "loss": 1.0766,
      "step": 894
    },
    {
      "epoch": 0.02342645750082887,
      "grad_norm": 37.671993255615234,
      "learning_rate": 9.192358021460351e-06,
      "loss": 0.6127,
      "step": 895
    },
    {
      "epoch": 0.02345263231367896,
      "grad_norm": 16.284955978393555,
      "learning_rate": 9.189217482334467e-06,
      "loss": 0.6062,
      "step": 896
    },
    {
      "epoch": 0.023478807126529045,
      "grad_norm": 10.86940860748291,
      "learning_rate": 9.186076943208584e-06,
      "loss": 0.853,
      "step": 897
    },
    {
      "epoch": 0.023504981939379135,
      "grad_norm": 20.21002960205078,
      "learning_rate": 9.182936404082702e-06,
      "loss": 0.9093,
      "step": 898
    },
    {
      "epoch": 0.02353115675222922,
      "grad_norm": 14.43177318572998,
      "learning_rate": 9.179795864956818e-06,
      "loss": 0.4658,
      "step": 899
    },
    {
      "epoch": 0.02355733156507931,
      "grad_norm": 15.009284019470215,
      "learning_rate": 9.176655325830935e-06,
      "loss": 0.9047,
      "step": 900
    },
    {
      "epoch": 0.023583506377929397,
      "grad_norm": 13.192595481872559,
      "learning_rate": 9.173514786705051e-06,
      "loss": 0.2976,
      "step": 901
    },
    {
      "epoch": 0.023609681190779486,
      "grad_norm": 18.487560272216797,
      "learning_rate": 9.170374247579169e-06,
      "loss": 1.3448,
      "step": 902
    },
    {
      "epoch": 0.023635856003629573,
      "grad_norm": 15.864055633544922,
      "learning_rate": 9.167233708453285e-06,
      "loss": 0.8958,
      "step": 903
    },
    {
      "epoch": 0.023662030816479662,
      "grad_norm": 23.53626251220703,
      "learning_rate": 9.164093169327402e-06,
      "loss": 1.4936,
      "step": 904
    },
    {
      "epoch": 0.023688205629329752,
      "grad_norm": 32.990623474121094,
      "learning_rate": 9.160952630201518e-06,
      "loss": 1.1259,
      "step": 905
    },
    {
      "epoch": 0.023714380442179838,
      "grad_norm": 37.983612060546875,
      "learning_rate": 9.157812091075634e-06,
      "loss": 0.983,
      "step": 906
    },
    {
      "epoch": 0.023740555255029928,
      "grad_norm": 10.072787284851074,
      "learning_rate": 9.154671551949751e-06,
      "loss": 0.1772,
      "step": 907
    },
    {
      "epoch": 0.023766730067880014,
      "grad_norm": 15.133919715881348,
      "learning_rate": 9.151531012823867e-06,
      "loss": 0.9437,
      "step": 908
    },
    {
      "epoch": 0.023792904880730104,
      "grad_norm": 16.344951629638672,
      "learning_rate": 9.148390473697985e-06,
      "loss": 1.4256,
      "step": 909
    },
    {
      "epoch": 0.02381907969358019,
      "grad_norm": 14.026981353759766,
      "learning_rate": 9.1452499345721e-06,
      "loss": 0.6265,
      "step": 910
    },
    {
      "epoch": 0.02384525450643028,
      "grad_norm": 17.207155227661133,
      "learning_rate": 9.142109395446218e-06,
      "loss": 0.6298,
      "step": 911
    },
    {
      "epoch": 0.023871429319280366,
      "grad_norm": 16.117677688598633,
      "learning_rate": 9.138968856320336e-06,
      "loss": 1.2458,
      "step": 912
    },
    {
      "epoch": 0.023897604132130455,
      "grad_norm": 20.59002113342285,
      "learning_rate": 9.135828317194452e-06,
      "loss": 0.5596,
      "step": 913
    },
    {
      "epoch": 0.023923778944980545,
      "grad_norm": 19.817651748657227,
      "learning_rate": 9.13268777806857e-06,
      "loss": 1.412,
      "step": 914
    },
    {
      "epoch": 0.02394995375783063,
      "grad_norm": 25.94467544555664,
      "learning_rate": 9.129547238942685e-06,
      "loss": 1.0077,
      "step": 915
    },
    {
      "epoch": 0.02397612857068072,
      "grad_norm": 13.771411895751953,
      "learning_rate": 9.126406699816803e-06,
      "loss": 1.6331,
      "step": 916
    },
    {
      "epoch": 0.024002303383530807,
      "grad_norm": 19.249732971191406,
      "learning_rate": 9.123266160690918e-06,
      "loss": 0.5763,
      "step": 917
    },
    {
      "epoch": 0.024028478196380897,
      "grad_norm": 17.815757751464844,
      "learning_rate": 9.120125621565036e-06,
      "loss": 1.3727,
      "step": 918
    },
    {
      "epoch": 0.024054653009230983,
      "grad_norm": 24.883371353149414,
      "learning_rate": 9.116985082439154e-06,
      "loss": 0.8962,
      "step": 919
    },
    {
      "epoch": 0.024080827822081072,
      "grad_norm": 19.292198181152344,
      "learning_rate": 9.11384454331327e-06,
      "loss": 0.9108,
      "step": 920
    },
    {
      "epoch": 0.02410700263493116,
      "grad_norm": 13.042547225952148,
      "learning_rate": 9.110704004187387e-06,
      "loss": 0.7221,
      "step": 921
    },
    {
      "epoch": 0.02413317744778125,
      "grad_norm": 18.20087242126465,
      "learning_rate": 9.107563465061501e-06,
      "loss": 0.607,
      "step": 922
    },
    {
      "epoch": 0.024159352260631338,
      "grad_norm": 28.631694793701172,
      "learning_rate": 9.104422925935619e-06,
      "loss": 1.1953,
      "step": 923
    },
    {
      "epoch": 0.024185527073481424,
      "grad_norm": 19.396739959716797,
      "learning_rate": 9.101282386809736e-06,
      "loss": 0.8718,
      "step": 924
    },
    {
      "epoch": 0.024211701886331514,
      "grad_norm": 13.718029975891113,
      "learning_rate": 9.098141847683852e-06,
      "loss": 0.9772,
      "step": 925
    },
    {
      "epoch": 0.0242378766991816,
      "grad_norm": 18.981956481933594,
      "learning_rate": 9.09500130855797e-06,
      "loss": 0.6103,
      "step": 926
    },
    {
      "epoch": 0.02426405151203169,
      "grad_norm": 14.229768753051758,
      "learning_rate": 9.091860769432086e-06,
      "loss": 0.688,
      "step": 927
    },
    {
      "epoch": 0.024290226324881776,
      "grad_norm": 17.579540252685547,
      "learning_rate": 9.088720230306203e-06,
      "loss": 0.7758,
      "step": 928
    },
    {
      "epoch": 0.024316401137731865,
      "grad_norm": 24.20450782775879,
      "learning_rate": 9.085579691180319e-06,
      "loss": 1.332,
      "step": 929
    },
    {
      "epoch": 0.02434257595058195,
      "grad_norm": 19.184921264648438,
      "learning_rate": 9.082439152054436e-06,
      "loss": 1.3894,
      "step": 930
    },
    {
      "epoch": 0.02436875076343204,
      "grad_norm": 13.487165451049805,
      "learning_rate": 9.079298612928552e-06,
      "loss": 0.569,
      "step": 931
    },
    {
      "epoch": 0.02439492557628213,
      "grad_norm": 17.618600845336914,
      "learning_rate": 9.07615807380267e-06,
      "loss": 1.0034,
      "step": 932
    },
    {
      "epoch": 0.024421100389132217,
      "grad_norm": 20.07601547241211,
      "learning_rate": 9.073017534676787e-06,
      "loss": 1.1376,
      "step": 933
    },
    {
      "epoch": 0.024447275201982307,
      "grad_norm": 14.599526405334473,
      "learning_rate": 9.069876995550903e-06,
      "loss": 0.5347,
      "step": 934
    },
    {
      "epoch": 0.024473450014832393,
      "grad_norm": 12.583362579345703,
      "learning_rate": 9.066736456425021e-06,
      "loss": 0.6666,
      "step": 935
    },
    {
      "epoch": 0.024499624827682483,
      "grad_norm": 16.10581398010254,
      "learning_rate": 9.063595917299137e-06,
      "loss": 0.811,
      "step": 936
    },
    {
      "epoch": 0.02452579964053257,
      "grad_norm": 17.573604583740234,
      "learning_rate": 9.060455378173253e-06,
      "loss": 1.055,
      "step": 937
    },
    {
      "epoch": 0.02455197445338266,
      "grad_norm": 20.570842742919922,
      "learning_rate": 9.05731483904737e-06,
      "loss": 0.5977,
      "step": 938
    },
    {
      "epoch": 0.024578149266232745,
      "grad_norm": 15.86668586730957,
      "learning_rate": 9.054174299921486e-06,
      "loss": 0.6941,
      "step": 939
    },
    {
      "epoch": 0.024604324079082834,
      "grad_norm": 15.788714408874512,
      "learning_rate": 9.051033760795604e-06,
      "loss": 1.3602,
      "step": 940
    },
    {
      "epoch": 0.024630498891932924,
      "grad_norm": 30.584949493408203,
      "learning_rate": 9.04789322166972e-06,
      "loss": 1.1841,
      "step": 941
    },
    {
      "epoch": 0.02465667370478301,
      "grad_norm": 29.247314453125,
      "learning_rate": 9.044752682543837e-06,
      "loss": 1.1479,
      "step": 942
    },
    {
      "epoch": 0.0246828485176331,
      "grad_norm": 38.163150787353516,
      "learning_rate": 9.041612143417953e-06,
      "loss": 0.8425,
      "step": 943
    },
    {
      "epoch": 0.024709023330483186,
      "grad_norm": 18.887632369995117,
      "learning_rate": 9.03847160429207e-06,
      "loss": 0.9595,
      "step": 944
    },
    {
      "epoch": 0.024735198143333276,
      "grad_norm": 21.60202407836914,
      "learning_rate": 9.035331065166188e-06,
      "loss": 1.1404,
      "step": 945
    },
    {
      "epoch": 0.024761372956183362,
      "grad_norm": 13.927895545959473,
      "learning_rate": 9.032190526040304e-06,
      "loss": 0.5938,
      "step": 946
    },
    {
      "epoch": 0.02478754776903345,
      "grad_norm": 12.215208053588867,
      "learning_rate": 9.029049986914421e-06,
      "loss": 0.6235,
      "step": 947
    },
    {
      "epoch": 0.02481372258188354,
      "grad_norm": 27.692947387695312,
      "learning_rate": 9.025909447788537e-06,
      "loss": 0.9193,
      "step": 948
    },
    {
      "epoch": 0.024839897394733627,
      "grad_norm": 18.661052703857422,
      "learning_rate": 9.022768908662655e-06,
      "loss": 0.8735,
      "step": 949
    },
    {
      "epoch": 0.024866072207583717,
      "grad_norm": 18.25432777404785,
      "learning_rate": 9.01962836953677e-06,
      "loss": 0.4867,
      "step": 950
    },
    {
      "epoch": 0.024892247020433803,
      "grad_norm": 22.347694396972656,
      "learning_rate": 9.016487830410888e-06,
      "loss": 0.8666,
      "step": 951
    },
    {
      "epoch": 0.024918421833283893,
      "grad_norm": 20.735393524169922,
      "learning_rate": 9.013347291285004e-06,
      "loss": 0.8297,
      "step": 952
    },
    {
      "epoch": 0.02494459664613398,
      "grad_norm": 17.30999755859375,
      "learning_rate": 9.01020675215912e-06,
      "loss": 0.6298,
      "step": 953
    },
    {
      "epoch": 0.02497077145898407,
      "grad_norm": 15.732367515563965,
      "learning_rate": 9.007066213033237e-06,
      "loss": 1.0287,
      "step": 954
    },
    {
      "epoch": 0.024996946271834155,
      "grad_norm": 17.219409942626953,
      "learning_rate": 9.003925673907353e-06,
      "loss": 0.9774,
      "step": 955
    },
    {
      "epoch": 0.025023121084684245,
      "grad_norm": 20.012283325195312,
      "learning_rate": 9.00078513478147e-06,
      "loss": 0.6432,
      "step": 956
    },
    {
      "epoch": 0.025049295897534334,
      "grad_norm": 19.707353591918945,
      "learning_rate": 8.997644595655587e-06,
      "loss": 1.464,
      "step": 957
    },
    {
      "epoch": 0.02507547071038442,
      "grad_norm": 13.880343437194824,
      "learning_rate": 8.994504056529704e-06,
      "loss": 0.2431,
      "step": 958
    },
    {
      "epoch": 0.02510164552323451,
      "grad_norm": 15.792356491088867,
      "learning_rate": 8.991363517403822e-06,
      "loss": 1.0366,
      "step": 959
    },
    {
      "epoch": 0.025127820336084596,
      "grad_norm": 17.151491165161133,
      "learning_rate": 8.988222978277938e-06,
      "loss": 1.4181,
      "step": 960
    },
    {
      "epoch": 0.025153995148934686,
      "grad_norm": 24.469707489013672,
      "learning_rate": 8.985082439152055e-06,
      "loss": 0.5972,
      "step": 961
    },
    {
      "epoch": 0.025180169961784772,
      "grad_norm": 13.280518531799316,
      "learning_rate": 8.981941900026171e-06,
      "loss": 0.6855,
      "step": 962
    },
    {
      "epoch": 0.025206344774634862,
      "grad_norm": 11.197561264038086,
      "learning_rate": 8.978801360900289e-06,
      "loss": 0.5592,
      "step": 963
    },
    {
      "epoch": 0.025232519587484948,
      "grad_norm": 20.724884033203125,
      "learning_rate": 8.975660821774404e-06,
      "loss": 1.3622,
      "step": 964
    },
    {
      "epoch": 0.025258694400335038,
      "grad_norm": 18.04754638671875,
      "learning_rate": 8.972520282648522e-06,
      "loss": 1.1807,
      "step": 965
    },
    {
      "epoch": 0.025284869213185127,
      "grad_norm": 21.614206314086914,
      "learning_rate": 8.96937974352264e-06,
      "loss": 0.8417,
      "step": 966
    },
    {
      "epoch": 0.025311044026035213,
      "grad_norm": 17.01732063293457,
      "learning_rate": 8.966239204396755e-06,
      "loss": 0.9531,
      "step": 967
    },
    {
      "epoch": 0.025337218838885303,
      "grad_norm": 31.740556716918945,
      "learning_rate": 8.963098665270871e-06,
      "loss": 0.9707,
      "step": 968
    },
    {
      "epoch": 0.02536339365173539,
      "grad_norm": 24.956823348999023,
      "learning_rate": 8.959958126144987e-06,
      "loss": 0.8804,
      "step": 969
    },
    {
      "epoch": 0.02538956846458548,
      "grad_norm": 12.155966758728027,
      "learning_rate": 8.956817587019105e-06,
      "loss": 0.8008,
      "step": 970
    },
    {
      "epoch": 0.025415743277435565,
      "grad_norm": 9.722857475280762,
      "learning_rate": 8.953677047893222e-06,
      "loss": 0.362,
      "step": 971
    },
    {
      "epoch": 0.025441918090285655,
      "grad_norm": 18.809152603149414,
      "learning_rate": 8.950536508767338e-06,
      "loss": 1.1316,
      "step": 972
    },
    {
      "epoch": 0.02546809290313574,
      "grad_norm": 21.56940269470215,
      "learning_rate": 8.947395969641456e-06,
      "loss": 0.9384,
      "step": 973
    },
    {
      "epoch": 0.02549426771598583,
      "grad_norm": 23.18474578857422,
      "learning_rate": 8.944255430515572e-06,
      "loss": 0.6256,
      "step": 974
    },
    {
      "epoch": 0.02552044252883592,
      "grad_norm": 15.431642532348633,
      "learning_rate": 8.941114891389689e-06,
      "loss": 0.8208,
      "step": 975
    },
    {
      "epoch": 0.025546617341686007,
      "grad_norm": 14.796995162963867,
      "learning_rate": 8.937974352263805e-06,
      "loss": 0.7459,
      "step": 976
    },
    {
      "epoch": 0.025572792154536096,
      "grad_norm": 9.691946029663086,
      "learning_rate": 8.934833813137923e-06,
      "loss": 0.2852,
      "step": 977
    },
    {
      "epoch": 0.025598966967386182,
      "grad_norm": 37.812442779541016,
      "learning_rate": 8.931693274012038e-06,
      "loss": 1.1989,
      "step": 978
    },
    {
      "epoch": 0.025625141780236272,
      "grad_norm": 22.795846939086914,
      "learning_rate": 8.928552734886156e-06,
      "loss": 0.7045,
      "step": 979
    },
    {
      "epoch": 0.025651316593086358,
      "grad_norm": 15.780804634094238,
      "learning_rate": 8.925412195760273e-06,
      "loss": 1.3536,
      "step": 980
    },
    {
      "epoch": 0.025677491405936448,
      "grad_norm": 17.481834411621094,
      "learning_rate": 8.92227165663439e-06,
      "loss": 0.9872,
      "step": 981
    },
    {
      "epoch": 0.025703666218786534,
      "grad_norm": 20.693843841552734,
      "learning_rate": 8.919131117508507e-06,
      "loss": 0.7777,
      "step": 982
    },
    {
      "epoch": 0.025729841031636624,
      "grad_norm": 10.231376647949219,
      "learning_rate": 8.915990578382623e-06,
      "loss": 0.2145,
      "step": 983
    },
    {
      "epoch": 0.025756015844486713,
      "grad_norm": 18.13493537902832,
      "learning_rate": 8.912850039256739e-06,
      "loss": 0.7166,
      "step": 984
    },
    {
      "epoch": 0.0257821906573368,
      "grad_norm": 10.336857795715332,
      "learning_rate": 8.909709500130856e-06,
      "loss": 0.4098,
      "step": 985
    },
    {
      "epoch": 0.02580836547018689,
      "grad_norm": 16.242277145385742,
      "learning_rate": 8.906568961004972e-06,
      "loss": 0.7698,
      "step": 986
    },
    {
      "epoch": 0.025834540283036975,
      "grad_norm": 20.488733291625977,
      "learning_rate": 8.90342842187909e-06,
      "loss": 0.8884,
      "step": 987
    },
    {
      "epoch": 0.025860715095887065,
      "grad_norm": 21.018901824951172,
      "learning_rate": 8.900287882753205e-06,
      "loss": 0.9054,
      "step": 988
    },
    {
      "epoch": 0.02588688990873715,
      "grad_norm": 16.938274383544922,
      "learning_rate": 8.897147343627323e-06,
      "loss": 1.0424,
      "step": 989
    },
    {
      "epoch": 0.02591306472158724,
      "grad_norm": 24.921157836914062,
      "learning_rate": 8.894006804501439e-06,
      "loss": 0.8191,
      "step": 990
    },
    {
      "epoch": 0.025939239534437327,
      "grad_norm": 23.72759246826172,
      "learning_rate": 8.890866265375556e-06,
      "loss": 0.7427,
      "step": 991
    },
    {
      "epoch": 0.025965414347287417,
      "grad_norm": 13.402997016906738,
      "learning_rate": 8.887725726249674e-06,
      "loss": 0.6203,
      "step": 992
    },
    {
      "epoch": 0.025991589160137506,
      "grad_norm": 13.776045799255371,
      "learning_rate": 8.88458518712379e-06,
      "loss": 0.9125,
      "step": 993
    },
    {
      "epoch": 0.026017763972987593,
      "grad_norm": 17.09200668334961,
      "learning_rate": 8.881444647997907e-06,
      "loss": 1.339,
      "step": 994
    },
    {
      "epoch": 0.026043938785837682,
      "grad_norm": 19.210105895996094,
      "learning_rate": 8.878304108872023e-06,
      "loss": 0.6543,
      "step": 995
    },
    {
      "epoch": 0.02607011359868777,
      "grad_norm": 12.220773696899414,
      "learning_rate": 8.87516356974614e-06,
      "loss": 0.5568,
      "step": 996
    },
    {
      "epoch": 0.026096288411537858,
      "grad_norm": 17.78321075439453,
      "learning_rate": 8.872023030620257e-06,
      "loss": 0.7973,
      "step": 997
    },
    {
      "epoch": 0.026122463224387944,
      "grad_norm": 25.987051010131836,
      "learning_rate": 8.868882491494374e-06,
      "loss": 0.753,
      "step": 998
    },
    {
      "epoch": 0.026148638037238034,
      "grad_norm": 19.03817367553711,
      "learning_rate": 8.86574195236849e-06,
      "loss": 0.7087,
      "step": 999
    },
    {
      "epoch": 0.026174812850088124,
      "grad_norm": 12.386798858642578,
      "learning_rate": 8.862601413242606e-06,
      "loss": 0.7176,
      "step": 1000
    },
    {
      "epoch": 0.02620098766293821,
      "grad_norm": 31.661161422729492,
      "learning_rate": 8.859460874116723e-06,
      "loss": 0.9982,
      "step": 1001
    },
    {
      "epoch": 0.0262271624757883,
      "grad_norm": 20.21319007873535,
      "learning_rate": 8.85632033499084e-06,
      "loss": 0.8212,
      "step": 1002
    },
    {
      "epoch": 0.026253337288638386,
      "grad_norm": 19.39395523071289,
      "learning_rate": 8.853179795864957e-06,
      "loss": 0.9278,
      "step": 1003
    },
    {
      "epoch": 0.026279512101488475,
      "grad_norm": 17.41468620300293,
      "learning_rate": 8.850039256739073e-06,
      "loss": 0.4466,
      "step": 1004
    },
    {
      "epoch": 0.02630568691433856,
      "grad_norm": 16.003231048583984,
      "learning_rate": 8.84689871761319e-06,
      "loss": 0.8415,
      "step": 1005
    },
    {
      "epoch": 0.02633186172718865,
      "grad_norm": 26.415393829345703,
      "learning_rate": 8.843758178487308e-06,
      "loss": 1.0569,
      "step": 1006
    },
    {
      "epoch": 0.026358036540038737,
      "grad_norm": 23.302183151245117,
      "learning_rate": 8.840617639361424e-06,
      "loss": 0.9145,
      "step": 1007
    },
    {
      "epoch": 0.026384211352888827,
      "grad_norm": 21.894014358520508,
      "learning_rate": 8.837477100235541e-06,
      "loss": 0.5281,
      "step": 1008
    },
    {
      "epoch": 0.026410386165738917,
      "grad_norm": 21.145177841186523,
      "learning_rate": 8.834336561109657e-06,
      "loss": 1.2992,
      "step": 1009
    },
    {
      "epoch": 0.026436560978589003,
      "grad_norm": 14.931975364685059,
      "learning_rate": 8.831196021983775e-06,
      "loss": 1.1661,
      "step": 1010
    },
    {
      "epoch": 0.026462735791439092,
      "grad_norm": 21.843217849731445,
      "learning_rate": 8.82805548285789e-06,
      "loss": 0.8543,
      "step": 1011
    },
    {
      "epoch": 0.02648891060428918,
      "grad_norm": 17.63266372680664,
      "learning_rate": 8.824914943732008e-06,
      "loss": 1.1092,
      "step": 1012
    },
    {
      "epoch": 0.02651508541713927,
      "grad_norm": 26.341777801513672,
      "learning_rate": 8.821774404606126e-06,
      "loss": 0.6214,
      "step": 1013
    },
    {
      "epoch": 0.026541260229989354,
      "grad_norm": 26.317171096801758,
      "learning_rate": 8.818633865480241e-06,
      "loss": 0.9403,
      "step": 1014
    },
    {
      "epoch": 0.026567435042839444,
      "grad_norm": 17.038082122802734,
      "learning_rate": 8.815493326354357e-06,
      "loss": 0.4633,
      "step": 1015
    },
    {
      "epoch": 0.02659360985568953,
      "grad_norm": 25.592071533203125,
      "learning_rate": 8.812352787228473e-06,
      "loss": 0.7419,
      "step": 1016
    },
    {
      "epoch": 0.02661978466853962,
      "grad_norm": 26.74091339111328,
      "learning_rate": 8.80921224810259e-06,
      "loss": 0.7687,
      "step": 1017
    },
    {
      "epoch": 0.02664595948138971,
      "grad_norm": 20.84470558166504,
      "learning_rate": 8.806071708976708e-06,
      "loss": 1.6443,
      "step": 1018
    },
    {
      "epoch": 0.026672134294239796,
      "grad_norm": 13.161638259887695,
      "learning_rate": 8.802931169850824e-06,
      "loss": 0.5368,
      "step": 1019
    },
    {
      "epoch": 0.026698309107089886,
      "grad_norm": 16.118995666503906,
      "learning_rate": 8.799790630724942e-06,
      "loss": 1.2032,
      "step": 1020
    },
    {
      "epoch": 0.02672448391993997,
      "grad_norm": 13.004922866821289,
      "learning_rate": 8.796650091599058e-06,
      "loss": 0.798,
      "step": 1021
    },
    {
      "epoch": 0.02675065873279006,
      "grad_norm": 22.58649253845215,
      "learning_rate": 8.793509552473175e-06,
      "loss": 0.8055,
      "step": 1022
    },
    {
      "epoch": 0.026776833545640148,
      "grad_norm": 21.10123634338379,
      "learning_rate": 8.790369013347291e-06,
      "loss": 1.2528,
      "step": 1023
    },
    {
      "epoch": 0.026803008358490237,
      "grad_norm": 20.990276336669922,
      "learning_rate": 8.787228474221409e-06,
      "loss": 0.7672,
      "step": 1024
    },
    {
      "epoch": 0.026829183171340323,
      "grad_norm": 13.316649436950684,
      "learning_rate": 8.784087935095524e-06,
      "loss": 0.8661,
      "step": 1025
    },
    {
      "epoch": 0.026855357984190413,
      "grad_norm": 24.921159744262695,
      "learning_rate": 8.780947395969642e-06,
      "loss": 1.433,
      "step": 1026
    },
    {
      "epoch": 0.026881532797040503,
      "grad_norm": 23.345951080322266,
      "learning_rate": 8.77780685684376e-06,
      "loss": 0.5345,
      "step": 1027
    },
    {
      "epoch": 0.02690770760989059,
      "grad_norm": 22.206424713134766,
      "learning_rate": 8.774666317717875e-06,
      "loss": 1.4068,
      "step": 1028
    },
    {
      "epoch": 0.02693388242274068,
      "grad_norm": 13.34490966796875,
      "learning_rate": 8.771525778591993e-06,
      "loss": 0.4092,
      "step": 1029
    },
    {
      "epoch": 0.026960057235590765,
      "grad_norm": 17.30352210998535,
      "learning_rate": 8.768385239466107e-06,
      "loss": 0.6091,
      "step": 1030
    },
    {
      "epoch": 0.026986232048440854,
      "grad_norm": 10.58128833770752,
      "learning_rate": 8.765244700340225e-06,
      "loss": 0.202,
      "step": 1031
    },
    {
      "epoch": 0.02701240686129094,
      "grad_norm": 14.250791549682617,
      "learning_rate": 8.762104161214342e-06,
      "loss": 0.4598,
      "step": 1032
    },
    {
      "epoch": 0.02703858167414103,
      "grad_norm": 15.873455047607422,
      "learning_rate": 8.758963622088458e-06,
      "loss": 0.7285,
      "step": 1033
    },
    {
      "epoch": 0.027064756486991116,
      "grad_norm": 16.286922454833984,
      "learning_rate": 8.755823082962576e-06,
      "loss": 0.8312,
      "step": 1034
    },
    {
      "epoch": 0.027090931299841206,
      "grad_norm": 18.8758544921875,
      "learning_rate": 8.752682543836691e-06,
      "loss": 0.9187,
      "step": 1035
    },
    {
      "epoch": 0.027117106112691296,
      "grad_norm": 23.0502986907959,
      "learning_rate": 8.749542004710809e-06,
      "loss": 0.8209,
      "step": 1036
    },
    {
      "epoch": 0.027143280925541382,
      "grad_norm": 12.960359573364258,
      "learning_rate": 8.746401465584925e-06,
      "loss": 0.6758,
      "step": 1037
    },
    {
      "epoch": 0.02716945573839147,
      "grad_norm": 11.770807266235352,
      "learning_rate": 8.743260926459042e-06,
      "loss": 0.4284,
      "step": 1038
    },
    {
      "epoch": 0.027195630551241558,
      "grad_norm": 17.059663772583008,
      "learning_rate": 8.74012038733316e-06,
      "loss": 1.0634,
      "step": 1039
    },
    {
      "epoch": 0.027221805364091647,
      "grad_norm": 19.367183685302734,
      "learning_rate": 8.736979848207276e-06,
      "loss": 0.7698,
      "step": 1040
    },
    {
      "epoch": 0.027247980176941734,
      "grad_norm": 22.40457534790039,
      "learning_rate": 8.733839309081393e-06,
      "loss": 1.3049,
      "step": 1041
    },
    {
      "epoch": 0.027274154989791823,
      "grad_norm": 16.627735137939453,
      "learning_rate": 8.73069876995551e-06,
      "loss": 0.5922,
      "step": 1042
    },
    {
      "epoch": 0.02730032980264191,
      "grad_norm": 14.184876441955566,
      "learning_rate": 8.727558230829627e-06,
      "loss": 0.8558,
      "step": 1043
    },
    {
      "epoch": 0.027326504615492,
      "grad_norm": 18.4725341796875,
      "learning_rate": 8.724417691703743e-06,
      "loss": 0.6219,
      "step": 1044
    },
    {
      "epoch": 0.02735267942834209,
      "grad_norm": 19.5185489654541,
      "learning_rate": 8.72127715257786e-06,
      "loss": 1.2628,
      "step": 1045
    },
    {
      "epoch": 0.027378854241192175,
      "grad_norm": 22.9426212310791,
      "learning_rate": 8.718136613451976e-06,
      "loss": 1.3125,
      "step": 1046
    },
    {
      "epoch": 0.027405029054042265,
      "grad_norm": 13.754690170288086,
      "learning_rate": 8.714996074326092e-06,
      "loss": 0.5608,
      "step": 1047
    },
    {
      "epoch": 0.02743120386689235,
      "grad_norm": 22.93401336669922,
      "learning_rate": 8.71185553520021e-06,
      "loss": 1.2349,
      "step": 1048
    },
    {
      "epoch": 0.02745737867974244,
      "grad_norm": 21.262117385864258,
      "learning_rate": 8.708714996074325e-06,
      "loss": 1.1149,
      "step": 1049
    },
    {
      "epoch": 0.027483553492592527,
      "grad_norm": 17.424516677856445,
      "learning_rate": 8.705574456948443e-06,
      "loss": 0.5403,
      "step": 1050
    },
    {
      "epoch": 0.027509728305442616,
      "grad_norm": 21.398582458496094,
      "learning_rate": 8.702433917822559e-06,
      "loss": 0.6427,
      "step": 1051
    },
    {
      "epoch": 0.027535903118292702,
      "grad_norm": 12.129341125488281,
      "learning_rate": 8.699293378696676e-06,
      "loss": 0.2071,
      "step": 1052
    },
    {
      "epoch": 0.027562077931142792,
      "grad_norm": 23.26810646057129,
      "learning_rate": 8.696152839570794e-06,
      "loss": 0.9383,
      "step": 1053
    },
    {
      "epoch": 0.027588252743992882,
      "grad_norm": 14.787328720092773,
      "learning_rate": 8.69301230044491e-06,
      "loss": 0.5591,
      "step": 1054
    },
    {
      "epoch": 0.027614427556842968,
      "grad_norm": 16.443042755126953,
      "learning_rate": 8.689871761319027e-06,
      "loss": 0.8913,
      "step": 1055
    },
    {
      "epoch": 0.027640602369693058,
      "grad_norm": 22.455860137939453,
      "learning_rate": 8.686731222193143e-06,
      "loss": 0.8008,
      "step": 1056
    },
    {
      "epoch": 0.027666777182543144,
      "grad_norm": 12.400203704833984,
      "learning_rate": 8.68359068306726e-06,
      "loss": 0.5985,
      "step": 1057
    },
    {
      "epoch": 0.027692951995393233,
      "grad_norm": 23.49595069885254,
      "learning_rate": 8.680450143941377e-06,
      "loss": 0.8391,
      "step": 1058
    },
    {
      "epoch": 0.02771912680824332,
      "grad_norm": 26.503957748413086,
      "learning_rate": 8.677309604815494e-06,
      "loss": 0.9271,
      "step": 1059
    },
    {
      "epoch": 0.02774530162109341,
      "grad_norm": 12.038450241088867,
      "learning_rate": 8.674169065689612e-06,
      "loss": 0.7766,
      "step": 1060
    },
    {
      "epoch": 0.0277714764339435,
      "grad_norm": 17.184579849243164,
      "learning_rate": 8.671028526563726e-06,
      "loss": 0.8093,
      "step": 1061
    },
    {
      "epoch": 0.027797651246793585,
      "grad_norm": 20.5596923828125,
      "learning_rate": 8.667887987437843e-06,
      "loss": 0.7478,
      "step": 1062
    },
    {
      "epoch": 0.027823826059643675,
      "grad_norm": 16.137678146362305,
      "learning_rate": 8.66474744831196e-06,
      "loss": 0.5387,
      "step": 1063
    },
    {
      "epoch": 0.02785000087249376,
      "grad_norm": 23.701396942138672,
      "learning_rate": 8.661606909186077e-06,
      "loss": 0.5342,
      "step": 1064
    },
    {
      "epoch": 0.02787617568534385,
      "grad_norm": 23.95350456237793,
      "learning_rate": 8.658466370060194e-06,
      "loss": 0.4558,
      "step": 1065
    },
    {
      "epoch": 0.027902350498193937,
      "grad_norm": 24.890338897705078,
      "learning_rate": 8.65532583093431e-06,
      "loss": 1.1289,
      "step": 1066
    },
    {
      "epoch": 0.027928525311044027,
      "grad_norm": 23.081668853759766,
      "learning_rate": 8.652185291808428e-06,
      "loss": 0.7197,
      "step": 1067
    },
    {
      "epoch": 0.027954700123894113,
      "grad_norm": 21.151029586791992,
      "learning_rate": 8.649044752682544e-06,
      "loss": 1.4057,
      "step": 1068
    },
    {
      "epoch": 0.027980874936744202,
      "grad_norm": 16.886066436767578,
      "learning_rate": 8.645904213556661e-06,
      "loss": 1.0316,
      "step": 1069
    },
    {
      "epoch": 0.028007049749594292,
      "grad_norm": 16.506628036499023,
      "learning_rate": 8.642763674430777e-06,
      "loss": 0.5831,
      "step": 1070
    },
    {
      "epoch": 0.028033224562444378,
      "grad_norm": 14.477432250976562,
      "learning_rate": 8.639623135304895e-06,
      "loss": 0.7674,
      "step": 1071
    },
    {
      "epoch": 0.028059399375294468,
      "grad_norm": 18.04631805419922,
      "learning_rate": 8.63648259617901e-06,
      "loss": 0.5762,
      "step": 1072
    },
    {
      "epoch": 0.028085574188144554,
      "grad_norm": 18.638492584228516,
      "learning_rate": 8.633342057053128e-06,
      "loss": 0.798,
      "step": 1073
    },
    {
      "epoch": 0.028111749000994644,
      "grad_norm": 15.144615173339844,
      "learning_rate": 8.630201517927246e-06,
      "loss": 0.5755,
      "step": 1074
    },
    {
      "epoch": 0.02813792381384473,
      "grad_norm": 20.565866470336914,
      "learning_rate": 8.627060978801361e-06,
      "loss": 0.8479,
      "step": 1075
    },
    {
      "epoch": 0.02816409862669482,
      "grad_norm": 14.001458168029785,
      "learning_rate": 8.623920439675479e-06,
      "loss": 0.6734,
      "step": 1076
    },
    {
      "epoch": 0.028190273439544906,
      "grad_norm": 14.062703132629395,
      "learning_rate": 8.620779900549593e-06,
      "loss": 0.7343,
      "step": 1077
    },
    {
      "epoch": 0.028216448252394995,
      "grad_norm": 24.722597122192383,
      "learning_rate": 8.61763936142371e-06,
      "loss": 0.556,
      "step": 1078
    },
    {
      "epoch": 0.028242623065245085,
      "grad_norm": 17.72638702392578,
      "learning_rate": 8.614498822297828e-06,
      "loss": 0.9352,
      "step": 1079
    },
    {
      "epoch": 0.02826879787809517,
      "grad_norm": 19.15571403503418,
      "learning_rate": 8.611358283171944e-06,
      "loss": 0.5792,
      "step": 1080
    },
    {
      "epoch": 0.02829497269094526,
      "grad_norm": 17.305070877075195,
      "learning_rate": 8.608217744046062e-06,
      "loss": 0.5794,
      "step": 1081
    },
    {
      "epoch": 0.028321147503795347,
      "grad_norm": 12.248754501342773,
      "learning_rate": 8.605077204920178e-06,
      "loss": 0.8311,
      "step": 1082
    },
    {
      "epoch": 0.028347322316645437,
      "grad_norm": 10.051974296569824,
      "learning_rate": 8.601936665794295e-06,
      "loss": 0.3714,
      "step": 1083
    },
    {
      "epoch": 0.028373497129495523,
      "grad_norm": 16.07948112487793,
      "learning_rate": 8.598796126668411e-06,
      "loss": 0.5383,
      "step": 1084
    },
    {
      "epoch": 0.028399671942345613,
      "grad_norm": 15.120184898376465,
      "learning_rate": 8.595655587542528e-06,
      "loss": 0.5718,
      "step": 1085
    },
    {
      "epoch": 0.0284258467551957,
      "grad_norm": 12.277276039123535,
      "learning_rate": 8.592515048416646e-06,
      "loss": 0.5078,
      "step": 1086
    },
    {
      "epoch": 0.02845202156804579,
      "grad_norm": 14.57281494140625,
      "learning_rate": 8.589374509290762e-06,
      "loss": 0.4459,
      "step": 1087
    },
    {
      "epoch": 0.028478196380895878,
      "grad_norm": 12.37181282043457,
      "learning_rate": 8.58623397016488e-06,
      "loss": 0.3174,
      "step": 1088
    },
    {
      "epoch": 0.028504371193745964,
      "grad_norm": 44.744449615478516,
      "learning_rate": 8.583093431038995e-06,
      "loss": 0.9136,
      "step": 1089
    },
    {
      "epoch": 0.028530546006596054,
      "grad_norm": 18.840185165405273,
      "learning_rate": 8.579952891913113e-06,
      "loss": 0.9319,
      "step": 1090
    },
    {
      "epoch": 0.02855672081944614,
      "grad_norm": 13.691912651062012,
      "learning_rate": 8.576812352787229e-06,
      "loss": 0.5228,
      "step": 1091
    },
    {
      "epoch": 0.02858289563229623,
      "grad_norm": 22.378864288330078,
      "learning_rate": 8.573671813661345e-06,
      "loss": 1.1765,
      "step": 1092
    },
    {
      "epoch": 0.028609070445146316,
      "grad_norm": 11.149384498596191,
      "learning_rate": 8.570531274535462e-06,
      "loss": 0.5181,
      "step": 1093
    },
    {
      "epoch": 0.028635245257996406,
      "grad_norm": 17.15279769897461,
      "learning_rate": 8.567390735409578e-06,
      "loss": 0.2792,
      "step": 1094
    },
    {
      "epoch": 0.028661420070846492,
      "grad_norm": 19.948707580566406,
      "learning_rate": 8.564250196283696e-06,
      "loss": 0.6726,
      "step": 1095
    },
    {
      "epoch": 0.02868759488369658,
      "grad_norm": 12.256278991699219,
      "learning_rate": 8.561109657157811e-06,
      "loss": 0.4863,
      "step": 1096
    },
    {
      "epoch": 0.02871376969654667,
      "grad_norm": 31.710067749023438,
      "learning_rate": 8.557969118031929e-06,
      "loss": 0.5402,
      "step": 1097
    },
    {
      "epoch": 0.028739944509396757,
      "grad_norm": 20.61297035217285,
      "learning_rate": 8.554828578906045e-06,
      "loss": 0.7936,
      "step": 1098
    },
    {
      "epoch": 0.028766119322246847,
      "grad_norm": 17.21793556213379,
      "learning_rate": 8.551688039780162e-06,
      "loss": 0.6017,
      "step": 1099
    },
    {
      "epoch": 0.028792294135096933,
      "grad_norm": 11.220561027526855,
      "learning_rate": 8.54854750065428e-06,
      "loss": 0.6992,
      "step": 1100
    },
    {
      "epoch": 0.028818468947947023,
      "grad_norm": 21.484561920166016,
      "learning_rate": 8.545406961528396e-06,
      "loss": 0.983,
      "step": 1101
    },
    {
      "epoch": 0.02884464376079711,
      "grad_norm": 16.25380516052246,
      "learning_rate": 8.542266422402513e-06,
      "loss": 1.1884,
      "step": 1102
    },
    {
      "epoch": 0.0288708185736472,
      "grad_norm": 25.784748077392578,
      "learning_rate": 8.53912588327663e-06,
      "loss": 1.3922,
      "step": 1103
    },
    {
      "epoch": 0.028896993386497285,
      "grad_norm": 18.808609008789062,
      "learning_rate": 8.535985344150747e-06,
      "loss": 0.5598,
      "step": 1104
    },
    {
      "epoch": 0.028923168199347375,
      "grad_norm": 21.099699020385742,
      "learning_rate": 8.532844805024863e-06,
      "loss": 0.8535,
      "step": 1105
    },
    {
      "epoch": 0.028949343012197464,
      "grad_norm": 13.567341804504395,
      "learning_rate": 8.52970426589898e-06,
      "loss": 0.4399,
      "step": 1106
    },
    {
      "epoch": 0.02897551782504755,
      "grad_norm": 14.692951202392578,
      "learning_rate": 8.526563726773098e-06,
      "loss": 0.5651,
      "step": 1107
    },
    {
      "epoch": 0.02900169263789764,
      "grad_norm": 20.135211944580078,
      "learning_rate": 8.523423187647212e-06,
      "loss": 0.886,
      "step": 1108
    },
    {
      "epoch": 0.029027867450747726,
      "grad_norm": 15.79466438293457,
      "learning_rate": 8.52028264852133e-06,
      "loss": 0.6142,
      "step": 1109
    },
    {
      "epoch": 0.029054042263597816,
      "grad_norm": 22.539276123046875,
      "learning_rate": 8.517142109395445e-06,
      "loss": 0.5389,
      "step": 1110
    },
    {
      "epoch": 0.029080217076447902,
      "grad_norm": 17.808666229248047,
      "learning_rate": 8.514001570269563e-06,
      "loss": 0.5496,
      "step": 1111
    },
    {
      "epoch": 0.02910639188929799,
      "grad_norm": 21.386821746826172,
      "learning_rate": 8.51086103114368e-06,
      "loss": 0.5362,
      "step": 1112
    },
    {
      "epoch": 0.02913256670214808,
      "grad_norm": 19.760534286499023,
      "learning_rate": 8.507720492017796e-06,
      "loss": 1.3748,
      "step": 1113
    },
    {
      "epoch": 0.029158741514998168,
      "grad_norm": 18.82125473022461,
      "learning_rate": 8.504579952891914e-06,
      "loss": 1.1346,
      "step": 1114
    },
    {
      "epoch": 0.029184916327848257,
      "grad_norm": 15.346317291259766,
      "learning_rate": 8.50143941376603e-06,
      "loss": 0.7954,
      "step": 1115
    },
    {
      "epoch": 0.029211091140698343,
      "grad_norm": 14.889585494995117,
      "learning_rate": 8.498298874640147e-06,
      "loss": 0.6926,
      "step": 1116
    },
    {
      "epoch": 0.029237265953548433,
      "grad_norm": 13.870555877685547,
      "learning_rate": 8.495158335514263e-06,
      "loss": 0.5275,
      "step": 1117
    },
    {
      "epoch": 0.02926344076639852,
      "grad_norm": 15.321096420288086,
      "learning_rate": 8.49201779638838e-06,
      "loss": 0.784,
      "step": 1118
    },
    {
      "epoch": 0.02928961557924861,
      "grad_norm": 16.731369018554688,
      "learning_rate": 8.488877257262496e-06,
      "loss": 1.0845,
      "step": 1119
    },
    {
      "epoch": 0.029315790392098695,
      "grad_norm": 21.339454650878906,
      "learning_rate": 8.485736718136614e-06,
      "loss": 0.8838,
      "step": 1120
    },
    {
      "epoch": 0.029341965204948785,
      "grad_norm": 18.006811141967773,
      "learning_rate": 8.482596179010732e-06,
      "loss": 0.539,
      "step": 1121
    },
    {
      "epoch": 0.029368140017798874,
      "grad_norm": 21.32979393005371,
      "learning_rate": 8.479455639884847e-06,
      "loss": 0.919,
      "step": 1122
    },
    {
      "epoch": 0.02939431483064896,
      "grad_norm": 21.35277557373047,
      "learning_rate": 8.476315100758963e-06,
      "loss": 0.5432,
      "step": 1123
    },
    {
      "epoch": 0.02942048964349905,
      "grad_norm": 18.464923858642578,
      "learning_rate": 8.47317456163308e-06,
      "loss": 0.737,
      "step": 1124
    },
    {
      "epoch": 0.029446664456349136,
      "grad_norm": 13.816466331481934,
      "learning_rate": 8.470034022507197e-06,
      "loss": 0.493,
      "step": 1125
    },
    {
      "epoch": 0.029472839269199226,
      "grad_norm": 20.707138061523438,
      "learning_rate": 8.466893483381314e-06,
      "loss": 0.6488,
      "step": 1126
    },
    {
      "epoch": 0.029499014082049312,
      "grad_norm": 9.800002098083496,
      "learning_rate": 8.46375294425543e-06,
      "loss": 0.1409,
      "step": 1127
    },
    {
      "epoch": 0.029525188894899402,
      "grad_norm": 12.881309509277344,
      "learning_rate": 8.460612405129548e-06,
      "loss": 0.8813,
      "step": 1128
    },
    {
      "epoch": 0.029551363707749488,
      "grad_norm": 16.126968383789062,
      "learning_rate": 8.457471866003664e-06,
      "loss": 0.7646,
      "step": 1129
    },
    {
      "epoch": 0.029577538520599578,
      "grad_norm": 27.870956420898438,
      "learning_rate": 8.454331326877781e-06,
      "loss": 0.6939,
      "step": 1130
    },
    {
      "epoch": 0.029603713333449667,
      "grad_norm": 10.35516357421875,
      "learning_rate": 8.451190787751897e-06,
      "loss": 0.4844,
      "step": 1131
    },
    {
      "epoch": 0.029629888146299754,
      "grad_norm": 19.119333267211914,
      "learning_rate": 8.448050248626015e-06,
      "loss": 0.4759,
      "step": 1132
    },
    {
      "epoch": 0.029656062959149843,
      "grad_norm": 13.546180725097656,
      "learning_rate": 8.444909709500132e-06,
      "loss": 0.2931,
      "step": 1133
    },
    {
      "epoch": 0.02968223777199993,
      "grad_norm": 13.884352684020996,
      "learning_rate": 8.441769170374248e-06,
      "loss": 0.5421,
      "step": 1134
    },
    {
      "epoch": 0.02970841258485002,
      "grad_norm": 20.500089645385742,
      "learning_rate": 8.438628631248365e-06,
      "loss": 0.9405,
      "step": 1135
    },
    {
      "epoch": 0.029734587397700105,
      "grad_norm": 21.481983184814453,
      "learning_rate": 8.435488092122481e-06,
      "loss": 0.6657,
      "step": 1136
    },
    {
      "epoch": 0.029760762210550195,
      "grad_norm": 12.30611801147461,
      "learning_rate": 8.432347552996599e-06,
      "loss": 0.4874,
      "step": 1137
    },
    {
      "epoch": 0.02978693702340028,
      "grad_norm": 23.186002731323242,
      "learning_rate": 8.429207013870715e-06,
      "loss": 1.1045,
      "step": 1138
    },
    {
      "epoch": 0.02981311183625037,
      "grad_norm": 19.82857322692871,
      "learning_rate": 8.42606647474483e-06,
      "loss": 0.4769,
      "step": 1139
    },
    {
      "epoch": 0.02983928664910046,
      "grad_norm": 8.644344329833984,
      "learning_rate": 8.422925935618948e-06,
      "loss": 0.2814,
      "step": 1140
    },
    {
      "epoch": 0.029865461461950547,
      "grad_norm": 15.507308959960938,
      "learning_rate": 8.419785396493064e-06,
      "loss": 0.3227,
      "step": 1141
    },
    {
      "epoch": 0.029891636274800636,
      "grad_norm": 19.12523651123047,
      "learning_rate": 8.416644857367182e-06,
      "loss": 0.796,
      "step": 1142
    },
    {
      "epoch": 0.029917811087650723,
      "grad_norm": 17.373798370361328,
      "learning_rate": 8.413504318241297e-06,
      "loss": 0.5396,
      "step": 1143
    },
    {
      "epoch": 0.029943985900500812,
      "grad_norm": 21.202472686767578,
      "learning_rate": 8.410363779115415e-06,
      "loss": 0.7738,
      "step": 1144
    },
    {
      "epoch": 0.0299701607133509,
      "grad_norm": 20.175094604492188,
      "learning_rate": 8.407223239989531e-06,
      "loss": 0.6215,
      "step": 1145
    },
    {
      "epoch": 0.029996335526200988,
      "grad_norm": 18.285850524902344,
      "learning_rate": 8.404082700863648e-06,
      "loss": 0.4302,
      "step": 1146
    },
    {
      "epoch": 0.030022510339051074,
      "grad_norm": 12.60654354095459,
      "learning_rate": 8.400942161737766e-06,
      "loss": 0.2594,
      "step": 1147
    },
    {
      "epoch": 0.030048685151901164,
      "grad_norm": 16.586149215698242,
      "learning_rate": 8.397801622611882e-06,
      "loss": 0.599,
      "step": 1148
    },
    {
      "epoch": 0.030074859964751254,
      "grad_norm": 23.544668197631836,
      "learning_rate": 8.394661083486e-06,
      "loss": 0.5283,
      "step": 1149
    },
    {
      "epoch": 0.03010103477760134,
      "grad_norm": 23.43476676940918,
      "learning_rate": 8.391520544360115e-06,
      "loss": 0.8529,
      "step": 1150
    },
    {
      "epoch": 0.03012720959045143,
      "grad_norm": 18.024456024169922,
      "learning_rate": 8.388380005234233e-06,
      "loss": 0.8219,
      "step": 1151
    },
    {
      "epoch": 0.030153384403301516,
      "grad_norm": 23.987863540649414,
      "learning_rate": 8.385239466108349e-06,
      "loss": 0.799,
      "step": 1152
    },
    {
      "epoch": 0.030179559216151605,
      "grad_norm": 17.911882400512695,
      "learning_rate": 8.382098926982466e-06,
      "loss": 0.8065,
      "step": 1153
    },
    {
      "epoch": 0.03020573402900169,
      "grad_norm": 17.784088134765625,
      "learning_rate": 8.378958387856582e-06,
      "loss": 0.379,
      "step": 1154
    },
    {
      "epoch": 0.03023190884185178,
      "grad_norm": 18.194034576416016,
      "learning_rate": 8.375817848730698e-06,
      "loss": 0.6176,
      "step": 1155
    },
    {
      "epoch": 0.030258083654701867,
      "grad_norm": 21.927547454833984,
      "learning_rate": 8.372677309604815e-06,
      "loss": 0.7139,
      "step": 1156
    },
    {
      "epoch": 0.030284258467551957,
      "grad_norm": 12.75497817993164,
      "learning_rate": 8.369536770478931e-06,
      "loss": 0.2147,
      "step": 1157
    },
    {
      "epoch": 0.030310433280402047,
      "grad_norm": 16.065547943115234,
      "learning_rate": 8.366396231353049e-06,
      "loss": 0.9247,
      "step": 1158
    },
    {
      "epoch": 0.030336608093252133,
      "grad_norm": 17.36803436279297,
      "learning_rate": 8.363255692227166e-06,
      "loss": 0.8788,
      "step": 1159
    },
    {
      "epoch": 0.030362782906102222,
      "grad_norm": 17.176185607910156,
      "learning_rate": 8.360115153101282e-06,
      "loss": 0.5058,
      "step": 1160
    },
    {
      "epoch": 0.03038895771895231,
      "grad_norm": 17.31911277770996,
      "learning_rate": 8.3569746139754e-06,
      "loss": 0.5055,
      "step": 1161
    },
    {
      "epoch": 0.030415132531802398,
      "grad_norm": 16.709016799926758,
      "learning_rate": 8.353834074849516e-06,
      "loss": 0.4994,
      "step": 1162
    },
    {
      "epoch": 0.030441307344652484,
      "grad_norm": 11.271753311157227,
      "learning_rate": 8.350693535723633e-06,
      "loss": 0.2866,
      "step": 1163
    },
    {
      "epoch": 0.030467482157502574,
      "grad_norm": 18.358062744140625,
      "learning_rate": 8.347552996597749e-06,
      "loss": 0.4359,
      "step": 1164
    },
    {
      "epoch": 0.03049365697035266,
      "grad_norm": 17.194562911987305,
      "learning_rate": 8.344412457471867e-06,
      "loss": 0.3288,
      "step": 1165
    },
    {
      "epoch": 0.03051983178320275,
      "grad_norm": 17.716796875,
      "learning_rate": 8.341271918345983e-06,
      "loss": 0.5438,
      "step": 1166
    },
    {
      "epoch": 0.03054600659605284,
      "grad_norm": 15.6380033493042,
      "learning_rate": 8.3381313792201e-06,
      "loss": 0.3328,
      "step": 1167
    },
    {
      "epoch": 0.030572181408902926,
      "grad_norm": 12.974352836608887,
      "learning_rate": 8.334990840094218e-06,
      "loss": 0.4339,
      "step": 1168
    },
    {
      "epoch": 0.030598356221753015,
      "grad_norm": 20.544687271118164,
      "learning_rate": 8.331850300968332e-06,
      "loss": 0.5496,
      "step": 1169
    },
    {
      "epoch": 0.0306245310346031,
      "grad_norm": 20.235158920288086,
      "learning_rate": 8.32870976184245e-06,
      "loss": 0.4429,
      "step": 1170
    },
    {
      "epoch": 0.03065070584745319,
      "grad_norm": 20.41921043395996,
      "learning_rate": 8.325569222716565e-06,
      "loss": 0.5181,
      "step": 1171
    },
    {
      "epoch": 0.030676880660303277,
      "grad_norm": 18.73533821105957,
      "learning_rate": 8.322428683590683e-06,
      "loss": 0.8959,
      "step": 1172
    },
    {
      "epoch": 0.030703055473153367,
      "grad_norm": 20.244863510131836,
      "learning_rate": 8.3192881444648e-06,
      "loss": 0.9707,
      "step": 1173
    },
    {
      "epoch": 0.030729230286003457,
      "grad_norm": 39.13740158081055,
      "learning_rate": 8.316147605338916e-06,
      "loss": 0.9684,
      "step": 1174
    },
    {
      "epoch": 0.030755405098853543,
      "grad_norm": 23.43578338623047,
      "learning_rate": 8.313007066213034e-06,
      "loss": 0.3976,
      "step": 1175
    },
    {
      "epoch": 0.030781579911703633,
      "grad_norm": 16.28723907470703,
      "learning_rate": 8.30986652708715e-06,
      "loss": 0.4634,
      "step": 1176
    },
    {
      "epoch": 0.03080775472455372,
      "grad_norm": 28.243955612182617,
      "learning_rate": 8.306725987961267e-06,
      "loss": 0.642,
      "step": 1177
    },
    {
      "epoch": 0.03083392953740381,
      "grad_norm": 15.996028900146484,
      "learning_rate": 8.303585448835383e-06,
      "loss": 0.8636,
      "step": 1178
    },
    {
      "epoch": 0.030860104350253895,
      "grad_norm": 17.182666778564453,
      "learning_rate": 8.3004449097095e-06,
      "loss": 0.903,
      "step": 1179
    },
    {
      "epoch": 0.030886279163103984,
      "grad_norm": 20.0072078704834,
      "learning_rate": 8.297304370583618e-06,
      "loss": 0.7165,
      "step": 1180
    },
    {
      "epoch": 0.03091245397595407,
      "grad_norm": 36.47212219238281,
      "learning_rate": 8.294163831457734e-06,
      "loss": 0.7429,
      "step": 1181
    },
    {
      "epoch": 0.03093862878880416,
      "grad_norm": 31.582866668701172,
      "learning_rate": 8.291023292331852e-06,
      "loss": 0.5796,
      "step": 1182
    },
    {
      "epoch": 0.03096480360165425,
      "grad_norm": 28.1096134185791,
      "learning_rate": 8.287882753205967e-06,
      "loss": 0.7944,
      "step": 1183
    },
    {
      "epoch": 0.030990978414504336,
      "grad_norm": 13.248345375061035,
      "learning_rate": 8.284742214080085e-06,
      "loss": 0.4711,
      "step": 1184
    },
    {
      "epoch": 0.031017153227354426,
      "grad_norm": 24.967899322509766,
      "learning_rate": 8.2816016749542e-06,
      "loss": 0.6011,
      "step": 1185
    },
    {
      "epoch": 0.031043328040204512,
      "grad_norm": 15.102028846740723,
      "learning_rate": 8.278461135828317e-06,
      "loss": 0.9385,
      "step": 1186
    },
    {
      "epoch": 0.0310695028530546,
      "grad_norm": 33.186256408691406,
      "learning_rate": 8.275320596702434e-06,
      "loss": 0.5939,
      "step": 1187
    },
    {
      "epoch": 0.031095677665904688,
      "grad_norm": 27.179737091064453,
      "learning_rate": 8.27218005757655e-06,
      "loss": 0.6041,
      "step": 1188
    },
    {
      "epoch": 0.031121852478754777,
      "grad_norm": 18.57811737060547,
      "learning_rate": 8.269039518450668e-06,
      "loss": 0.8245,
      "step": 1189
    },
    {
      "epoch": 0.031148027291604864,
      "grad_norm": 22.103654861450195,
      "learning_rate": 8.265898979324783e-06,
      "loss": 0.8785,
      "step": 1190
    },
    {
      "epoch": 0.031174202104454953,
      "grad_norm": 17.225234985351562,
      "learning_rate": 8.262758440198901e-06,
      "loss": 0.5336,
      "step": 1191
    },
    {
      "epoch": 0.031200376917305043,
      "grad_norm": 17.45276641845703,
      "learning_rate": 8.259617901073017e-06,
      "loss": 0.5438,
      "step": 1192
    },
    {
      "epoch": 0.03122655173015513,
      "grad_norm": 31.36673355102539,
      "learning_rate": 8.256477361947134e-06,
      "loss": 0.6897,
      "step": 1193
    },
    {
      "epoch": 0.03125272654300522,
      "grad_norm": 13.989961624145508,
      "learning_rate": 8.253336822821252e-06,
      "loss": 0.5771,
      "step": 1194
    },
    {
      "epoch": 0.03127890135585531,
      "grad_norm": 16.098468780517578,
      "learning_rate": 8.250196283695368e-06,
      "loss": 0.5694,
      "step": 1195
    },
    {
      "epoch": 0.03130507616870539,
      "grad_norm": 19.139297485351562,
      "learning_rate": 8.247055744569485e-06,
      "loss": 0.5178,
      "step": 1196
    },
    {
      "epoch": 0.03133125098155548,
      "grad_norm": 19.031688690185547,
      "learning_rate": 8.243915205443601e-06,
      "loss": 0.9542,
      "step": 1197
    },
    {
      "epoch": 0.03135742579440557,
      "grad_norm": 19.0145206451416,
      "learning_rate": 8.240774666317719e-06,
      "loss": 0.6227,
      "step": 1198
    },
    {
      "epoch": 0.03138360060725566,
      "grad_norm": 11.67270278930664,
      "learning_rate": 8.237634127191835e-06,
      "loss": 0.481,
      "step": 1199
    },
    {
      "epoch": 0.03140977542010575,
      "grad_norm": 23.55051612854004,
      "learning_rate": 8.23449358806595e-06,
      "loss": 0.5623,
      "step": 1200
    },
    {
      "epoch": 0.03143595023295583,
      "grad_norm": 14.93101978302002,
      "learning_rate": 8.231353048940068e-06,
      "loss": 0.4639,
      "step": 1201
    },
    {
      "epoch": 0.03146212504580592,
      "grad_norm": 17.78436279296875,
      "learning_rate": 8.228212509814184e-06,
      "loss": 0.6971,
      "step": 1202
    },
    {
      "epoch": 0.03148829985865601,
      "grad_norm": 11.325952529907227,
      "learning_rate": 8.225071970688302e-06,
      "loss": 0.4083,
      "step": 1203
    },
    {
      "epoch": 0.0315144746715061,
      "grad_norm": 26.195804595947266,
      "learning_rate": 8.221931431562417e-06,
      "loss": 0.5792,
      "step": 1204
    },
    {
      "epoch": 0.031540649484356184,
      "grad_norm": 22.585079193115234,
      "learning_rate": 8.218790892436535e-06,
      "loss": 0.5056,
      "step": 1205
    },
    {
      "epoch": 0.031566824297206274,
      "grad_norm": 14.808723449707031,
      "learning_rate": 8.215650353310652e-06,
      "loss": 0.6428,
      "step": 1206
    },
    {
      "epoch": 0.03159299911005636,
      "grad_norm": 11.634310722351074,
      "learning_rate": 8.212509814184768e-06,
      "loss": 0.4184,
      "step": 1207
    },
    {
      "epoch": 0.03161917392290645,
      "grad_norm": 13.626801490783691,
      "learning_rate": 8.209369275058886e-06,
      "loss": 0.4627,
      "step": 1208
    },
    {
      "epoch": 0.03164534873575654,
      "grad_norm": 36.463348388671875,
      "learning_rate": 8.206228735933002e-06,
      "loss": 1.0859,
      "step": 1209
    },
    {
      "epoch": 0.031671523548606625,
      "grad_norm": 23.412649154663086,
      "learning_rate": 8.20308819680712e-06,
      "loss": 0.8266,
      "step": 1210
    },
    {
      "epoch": 0.031697698361456715,
      "grad_norm": 14.014134407043457,
      "learning_rate": 8.199947657681235e-06,
      "loss": 0.5628,
      "step": 1211
    },
    {
      "epoch": 0.031723873174306805,
      "grad_norm": 12.214584350585938,
      "learning_rate": 8.196807118555353e-06,
      "loss": 0.6562,
      "step": 1212
    },
    {
      "epoch": 0.031750047987156894,
      "grad_norm": 22.531726837158203,
      "learning_rate": 8.193666579429469e-06,
      "loss": 0.8606,
      "step": 1213
    },
    {
      "epoch": 0.03177622280000698,
      "grad_norm": 15.791736602783203,
      "learning_rate": 8.190526040303586e-06,
      "loss": 0.4822,
      "step": 1214
    },
    {
      "epoch": 0.03180239761285707,
      "grad_norm": 17.225994110107422,
      "learning_rate": 8.187385501177704e-06,
      "loss": 0.4605,
      "step": 1215
    },
    {
      "epoch": 0.031828572425707156,
      "grad_norm": 20.983505249023438,
      "learning_rate": 8.184244962051818e-06,
      "loss": 1.2678,
      "step": 1216
    },
    {
      "epoch": 0.031854747238557246,
      "grad_norm": 19.484222412109375,
      "learning_rate": 8.181104422925935e-06,
      "loss": 0.4738,
      "step": 1217
    },
    {
      "epoch": 0.031880922051407336,
      "grad_norm": 16.228822708129883,
      "learning_rate": 8.177963883800053e-06,
      "loss": 0.3947,
      "step": 1218
    },
    {
      "epoch": 0.03190709686425742,
      "grad_norm": 12.191750526428223,
      "learning_rate": 8.174823344674169e-06,
      "loss": 0.1781,
      "step": 1219
    },
    {
      "epoch": 0.03193327167710751,
      "grad_norm": 19.48598289489746,
      "learning_rate": 8.171682805548286e-06,
      "loss": 0.5076,
      "step": 1220
    },
    {
      "epoch": 0.0319594464899576,
      "grad_norm": 25.772581100463867,
      "learning_rate": 8.168542266422402e-06,
      "loss": 0.7241,
      "step": 1221
    },
    {
      "epoch": 0.03198562130280769,
      "grad_norm": 16.355422973632812,
      "learning_rate": 8.16540172729652e-06,
      "loss": 0.8785,
      "step": 1222
    },
    {
      "epoch": 0.03201179611565777,
      "grad_norm": 18.22478485107422,
      "learning_rate": 8.162261188170636e-06,
      "loss": 0.8397,
      "step": 1223
    },
    {
      "epoch": 0.03203797092850786,
      "grad_norm": 24.15901756286621,
      "learning_rate": 8.159120649044753e-06,
      "loss": 0.9735,
      "step": 1224
    },
    {
      "epoch": 0.03206414574135795,
      "grad_norm": 21.81362533569336,
      "learning_rate": 8.155980109918869e-06,
      "loss": 0.495,
      "step": 1225
    },
    {
      "epoch": 0.03209032055420804,
      "grad_norm": 14.11471939086914,
      "learning_rate": 8.152839570792987e-06,
      "loss": 0.9597,
      "step": 1226
    },
    {
      "epoch": 0.03211649536705813,
      "grad_norm": 17.33785629272461,
      "learning_rate": 8.149699031667104e-06,
      "loss": 0.6335,
      "step": 1227
    },
    {
      "epoch": 0.03214267017990821,
      "grad_norm": 14.59998893737793,
      "learning_rate": 8.14655849254122e-06,
      "loss": 0.5031,
      "step": 1228
    },
    {
      "epoch": 0.0321688449927583,
      "grad_norm": 22.87152099609375,
      "learning_rate": 8.143417953415338e-06,
      "loss": 0.9242,
      "step": 1229
    },
    {
      "epoch": 0.03219501980560839,
      "grad_norm": 15.980381965637207,
      "learning_rate": 8.140277414289453e-06,
      "loss": 0.3673,
      "step": 1230
    },
    {
      "epoch": 0.03222119461845848,
      "grad_norm": 15.866798400878906,
      "learning_rate": 8.13713687516357e-06,
      "loss": 0.6735,
      "step": 1231
    },
    {
      "epoch": 0.03224736943130856,
      "grad_norm": 15.759687423706055,
      "learning_rate": 8.133996336037687e-06,
      "loss": 0.6618,
      "step": 1232
    },
    {
      "epoch": 0.03227354424415865,
      "grad_norm": 27.058881759643555,
      "learning_rate": 8.130855796911803e-06,
      "loss": 0.9016,
      "step": 1233
    },
    {
      "epoch": 0.03229971905700874,
      "grad_norm": 15.65758228302002,
      "learning_rate": 8.12771525778592e-06,
      "loss": 0.2707,
      "step": 1234
    },
    {
      "epoch": 0.03232589386985883,
      "grad_norm": 20.938926696777344,
      "learning_rate": 8.124574718660036e-06,
      "loss": 0.6057,
      "step": 1235
    },
    {
      "epoch": 0.03235206868270892,
      "grad_norm": 27.787803649902344,
      "learning_rate": 8.121434179534154e-06,
      "loss": 0.5888,
      "step": 1236
    },
    {
      "epoch": 0.032378243495559005,
      "grad_norm": 19.271480560302734,
      "learning_rate": 8.11829364040827e-06,
      "loss": 0.8449,
      "step": 1237
    },
    {
      "epoch": 0.032404418308409094,
      "grad_norm": 20.993968963623047,
      "learning_rate": 8.115153101282387e-06,
      "loss": 0.674,
      "step": 1238
    },
    {
      "epoch": 0.032430593121259184,
      "grad_norm": 18.995145797729492,
      "learning_rate": 8.112012562156503e-06,
      "loss": 0.5012,
      "step": 1239
    },
    {
      "epoch": 0.032456767934109274,
      "grad_norm": 16.363677978515625,
      "learning_rate": 8.10887202303062e-06,
      "loss": 0.6249,
      "step": 1240
    },
    {
      "epoch": 0.032482942746959356,
      "grad_norm": 19.010217666625977,
      "learning_rate": 8.105731483904738e-06,
      "loss": 0.6131,
      "step": 1241
    },
    {
      "epoch": 0.032509117559809446,
      "grad_norm": 16.351497650146484,
      "learning_rate": 8.102590944778854e-06,
      "loss": 0.9537,
      "step": 1242
    },
    {
      "epoch": 0.032535292372659536,
      "grad_norm": 17.567142486572266,
      "learning_rate": 8.099450405652971e-06,
      "loss": 0.4697,
      "step": 1243
    },
    {
      "epoch": 0.032561467185509625,
      "grad_norm": 12.464038848876953,
      "learning_rate": 8.096309866527087e-06,
      "loss": 0.4733,
      "step": 1244
    },
    {
      "epoch": 0.032587641998359715,
      "grad_norm": 18.40229606628418,
      "learning_rate": 8.093169327401205e-06,
      "loss": 0.6218,
      "step": 1245
    },
    {
      "epoch": 0.0326138168112098,
      "grad_norm": 24.34620475769043,
      "learning_rate": 8.09002878827532e-06,
      "loss": 0.5618,
      "step": 1246
    },
    {
      "epoch": 0.03263999162405989,
      "grad_norm": 14.909196853637695,
      "learning_rate": 8.086888249149437e-06,
      "loss": 0.1699,
      "step": 1247
    },
    {
      "epoch": 0.03266616643690998,
      "grad_norm": 14.369684219360352,
      "learning_rate": 8.083747710023554e-06,
      "loss": 0.6847,
      "step": 1248
    },
    {
      "epoch": 0.03269234124976007,
      "grad_norm": 22.71152687072754,
      "learning_rate": 8.08060717089767e-06,
      "loss": 0.6789,
      "step": 1249
    },
    {
      "epoch": 0.03271851606261015,
      "grad_norm": 15.938042640686035,
      "learning_rate": 8.077466631771788e-06,
      "loss": 0.6214,
      "step": 1250
    },
    {
      "epoch": 0.03274469087546024,
      "grad_norm": 13.259037971496582,
      "learning_rate": 8.074326092645903e-06,
      "loss": 0.6184,
      "step": 1251
    },
    {
      "epoch": 0.03277086568831033,
      "grad_norm": 14.734495162963867,
      "learning_rate": 8.071185553520021e-06,
      "loss": 0.8519,
      "step": 1252
    },
    {
      "epoch": 0.03279704050116042,
      "grad_norm": 26.710166931152344,
      "learning_rate": 8.068045014394139e-06,
      "loss": 1.023,
      "step": 1253
    },
    {
      "epoch": 0.03282321531401051,
      "grad_norm": 15.79175853729248,
      "learning_rate": 8.064904475268254e-06,
      "loss": 0.4074,
      "step": 1254
    },
    {
      "epoch": 0.03284939012686059,
      "grad_norm": 16.635238647460938,
      "learning_rate": 8.061763936142372e-06,
      "loss": 0.4321,
      "step": 1255
    },
    {
      "epoch": 0.03287556493971068,
      "grad_norm": 19.640138626098633,
      "learning_rate": 8.058623397016488e-06,
      "loss": 0.4544,
      "step": 1256
    },
    {
      "epoch": 0.03290173975256077,
      "grad_norm": 18.679733276367188,
      "learning_rate": 8.055482857890605e-06,
      "loss": 0.9701,
      "step": 1257
    },
    {
      "epoch": 0.03292791456541086,
      "grad_norm": 11.439140319824219,
      "learning_rate": 8.052342318764721e-06,
      "loss": 0.4093,
      "step": 1258
    },
    {
      "epoch": 0.03295408937826094,
      "grad_norm": 21.076034545898438,
      "learning_rate": 8.049201779638839e-06,
      "loss": 0.5143,
      "step": 1259
    },
    {
      "epoch": 0.03298026419111103,
      "grad_norm": 15.715840339660645,
      "learning_rate": 8.046061240512955e-06,
      "loss": 0.2997,
      "step": 1260
    },
    {
      "epoch": 0.03300643900396112,
      "grad_norm": 17.428903579711914,
      "learning_rate": 8.042920701387072e-06,
      "loss": 0.1823,
      "step": 1261
    },
    {
      "epoch": 0.03303261381681121,
      "grad_norm": 14.643452644348145,
      "learning_rate": 8.039780162261188e-06,
      "loss": 0.5714,
      "step": 1262
    },
    {
      "epoch": 0.0330587886296613,
      "grad_norm": 16.547447204589844,
      "learning_rate": 8.036639623135304e-06,
      "loss": 0.7596,
      "step": 1263
    },
    {
      "epoch": 0.033084963442511384,
      "grad_norm": 16.34348487854004,
      "learning_rate": 8.033499084009421e-06,
      "loss": 0.6656,
      "step": 1264
    },
    {
      "epoch": 0.03311113825536147,
      "grad_norm": 16.112348556518555,
      "learning_rate": 8.030358544883539e-06,
      "loss": 0.5953,
      "step": 1265
    },
    {
      "epoch": 0.03313731306821156,
      "grad_norm": 24.08806800842285,
      "learning_rate": 8.027218005757655e-06,
      "loss": 0.8227,
      "step": 1266
    },
    {
      "epoch": 0.03316348788106165,
      "grad_norm": 15.168018341064453,
      "learning_rate": 8.024077466631772e-06,
      "loss": 0.7166,
      "step": 1267
    },
    {
      "epoch": 0.033189662693911735,
      "grad_norm": 16.91170883178711,
      "learning_rate": 8.020936927505888e-06,
      "loss": 0.6971,
      "step": 1268
    },
    {
      "epoch": 0.033215837506761825,
      "grad_norm": 16.468585968017578,
      "learning_rate": 8.017796388380006e-06,
      "loss": 0.4167,
      "step": 1269
    },
    {
      "epoch": 0.033242012319611915,
      "grad_norm": 16.293603897094727,
      "learning_rate": 8.014655849254122e-06,
      "loss": 0.6306,
      "step": 1270
    },
    {
      "epoch": 0.033268187132462004,
      "grad_norm": 13.426785469055176,
      "learning_rate": 8.01151531012824e-06,
      "loss": 0.5061,
      "step": 1271
    },
    {
      "epoch": 0.033294361945312094,
      "grad_norm": 11.03808307647705,
      "learning_rate": 8.008374771002355e-06,
      "loss": 0.3767,
      "step": 1272
    },
    {
      "epoch": 0.03332053675816218,
      "grad_norm": 19.42428207397461,
      "learning_rate": 8.005234231876473e-06,
      "loss": 0.6803,
      "step": 1273
    },
    {
      "epoch": 0.033346711571012266,
      "grad_norm": 23.124048233032227,
      "learning_rate": 8.00209369275059e-06,
      "loss": 0.8296,
      "step": 1274
    },
    {
      "epoch": 0.033372886383862356,
      "grad_norm": 15.580513954162598,
      "learning_rate": 7.998953153624706e-06,
      "loss": 0.3647,
      "step": 1275
    },
    {
      "epoch": 0.033399061196712446,
      "grad_norm": 13.890544891357422,
      "learning_rate": 7.995812614498824e-06,
      "loss": 0.7299,
      "step": 1276
    },
    {
      "epoch": 0.03342523600956253,
      "grad_norm": 17.880224227905273,
      "learning_rate": 7.99267207537294e-06,
      "loss": 0.547,
      "step": 1277
    },
    {
      "epoch": 0.03345141082241262,
      "grad_norm": 20.92080307006836,
      "learning_rate": 7.989531536247055e-06,
      "loss": 1.0129,
      "step": 1278
    },
    {
      "epoch": 0.03347758563526271,
      "grad_norm": 15.24743938446045,
      "learning_rate": 7.986390997121173e-06,
      "loss": 0.6501,
      "step": 1279
    },
    {
      "epoch": 0.0335037604481128,
      "grad_norm": 22.461286544799805,
      "learning_rate": 7.983250457995289e-06,
      "loss": 0.4712,
      "step": 1280
    },
    {
      "epoch": 0.03352993526096289,
      "grad_norm": 24.02013397216797,
      "learning_rate": 7.980109918869406e-06,
      "loss": 0.7796,
      "step": 1281
    },
    {
      "epoch": 0.03355611007381297,
      "grad_norm": 16.126285552978516,
      "learning_rate": 7.976969379743522e-06,
      "loss": 0.705,
      "step": 1282
    },
    {
      "epoch": 0.03358228488666306,
      "grad_norm": 14.313868522644043,
      "learning_rate": 7.97382884061764e-06,
      "loss": 0.3217,
      "step": 1283
    },
    {
      "epoch": 0.03360845969951315,
      "grad_norm": 24.181190490722656,
      "learning_rate": 7.970688301491756e-06,
      "loss": 1.0965,
      "step": 1284
    },
    {
      "epoch": 0.03363463451236324,
      "grad_norm": 16.900949478149414,
      "learning_rate": 7.967547762365873e-06,
      "loss": 0.8334,
      "step": 1285
    },
    {
      "epoch": 0.03366080932521332,
      "grad_norm": 11.52686882019043,
      "learning_rate": 7.964407223239989e-06,
      "loss": 0.5667,
      "step": 1286
    },
    {
      "epoch": 0.03368698413806341,
      "grad_norm": 18.446598052978516,
      "learning_rate": 7.961266684114107e-06,
      "loss": 0.5453,
      "step": 1287
    },
    {
      "epoch": 0.0337131589509135,
      "grad_norm": 16.6224308013916,
      "learning_rate": 7.958126144988224e-06,
      "loss": 0.4166,
      "step": 1288
    },
    {
      "epoch": 0.03373933376376359,
      "grad_norm": 19.50838851928711,
      "learning_rate": 7.95498560586234e-06,
      "loss": 0.2389,
      "step": 1289
    },
    {
      "epoch": 0.03376550857661368,
      "grad_norm": 21.648534774780273,
      "learning_rate": 7.951845066736457e-06,
      "loss": 0.6897,
      "step": 1290
    },
    {
      "epoch": 0.03379168338946376,
      "grad_norm": 30.21115493774414,
      "learning_rate": 7.948704527610573e-06,
      "loss": 0.6767,
      "step": 1291
    },
    {
      "epoch": 0.03381785820231385,
      "grad_norm": 21.699193954467773,
      "learning_rate": 7.945563988484691e-06,
      "loss": 0.8396,
      "step": 1292
    },
    {
      "epoch": 0.03384403301516394,
      "grad_norm": 13.824298858642578,
      "learning_rate": 7.942423449358807e-06,
      "loss": 0.7253,
      "step": 1293
    },
    {
      "epoch": 0.03387020782801403,
      "grad_norm": 8.179559707641602,
      "learning_rate": 7.939282910232923e-06,
      "loss": 0.2993,
      "step": 1294
    },
    {
      "epoch": 0.033896382640864114,
      "grad_norm": 20.188051223754883,
      "learning_rate": 7.93614237110704e-06,
      "loss": 0.5306,
      "step": 1295
    },
    {
      "epoch": 0.033922557453714204,
      "grad_norm": 13.978476524353027,
      "learning_rate": 7.933001831981156e-06,
      "loss": 0.6395,
      "step": 1296
    },
    {
      "epoch": 0.033948732266564294,
      "grad_norm": 17.256013870239258,
      "learning_rate": 7.929861292855274e-06,
      "loss": 0.5027,
      "step": 1297
    },
    {
      "epoch": 0.03397490707941438,
      "grad_norm": 20.41617202758789,
      "learning_rate": 7.92672075372939e-06,
      "loss": 0.6221,
      "step": 1298
    },
    {
      "epoch": 0.03400108189226447,
      "grad_norm": 12.171501159667969,
      "learning_rate": 7.923580214603507e-06,
      "loss": 0.4859,
      "step": 1299
    },
    {
      "epoch": 0.034027256705114556,
      "grad_norm": 17.15804672241211,
      "learning_rate": 7.920439675477625e-06,
      "loss": 0.6402,
      "step": 1300
    },
    {
      "epoch": 0.034053431517964645,
      "grad_norm": 18.020566940307617,
      "learning_rate": 7.91729913635174e-06,
      "loss": 0.5861,
      "step": 1301
    },
    {
      "epoch": 0.034079606330814735,
      "grad_norm": 9.038763046264648,
      "learning_rate": 7.914158597225858e-06,
      "loss": 0.3585,
      "step": 1302
    },
    {
      "epoch": 0.034105781143664825,
      "grad_norm": 21.747066497802734,
      "learning_rate": 7.911018058099974e-06,
      "loss": 0.6861,
      "step": 1303
    },
    {
      "epoch": 0.03413195595651491,
      "grad_norm": 18.91901206970215,
      "learning_rate": 7.907877518974091e-06,
      "loss": 0.3947,
      "step": 1304
    },
    {
      "epoch": 0.034158130769365,
      "grad_norm": 32.5510368347168,
      "learning_rate": 7.904736979848207e-06,
      "loss": 1.3898,
      "step": 1305
    },
    {
      "epoch": 0.03418430558221509,
      "grad_norm": 22.391338348388672,
      "learning_rate": 7.901596440722325e-06,
      "loss": 0.5096,
      "step": 1306
    },
    {
      "epoch": 0.034210480395065176,
      "grad_norm": 23.6744384765625,
      "learning_rate": 7.89845590159644e-06,
      "loss": 0.4855,
      "step": 1307
    },
    {
      "epoch": 0.034236655207915266,
      "grad_norm": 15.782029151916504,
      "learning_rate": 7.895315362470558e-06,
      "loss": 0.5178,
      "step": 1308
    },
    {
      "epoch": 0.03426283002076535,
      "grad_norm": 18.80600357055664,
      "learning_rate": 7.892174823344674e-06,
      "loss": 0.8294,
      "step": 1309
    },
    {
      "epoch": 0.03428900483361544,
      "grad_norm": 27.747169494628906,
      "learning_rate": 7.88903428421879e-06,
      "loss": 0.5137,
      "step": 1310
    },
    {
      "epoch": 0.03431517964646553,
      "grad_norm": 16.29921531677246,
      "learning_rate": 7.885893745092907e-06,
      "loss": 0.5639,
      "step": 1311
    },
    {
      "epoch": 0.03434135445931562,
      "grad_norm": 22.902124404907227,
      "learning_rate": 7.882753205967025e-06,
      "loss": 0.7483,
      "step": 1312
    },
    {
      "epoch": 0.03436752927216571,
      "grad_norm": 17.05113410949707,
      "learning_rate": 7.879612666841141e-06,
      "loss": 0.4996,
      "step": 1313
    },
    {
      "epoch": 0.03439370408501579,
      "grad_norm": 15.185234069824219,
      "learning_rate": 7.876472127715258e-06,
      "loss": 0.6176,
      "step": 1314
    },
    {
      "epoch": 0.03441987889786588,
      "grad_norm": 23.40630340576172,
      "learning_rate": 7.873331588589374e-06,
      "loss": 0.6632,
      "step": 1315
    },
    {
      "epoch": 0.03444605371071597,
      "grad_norm": 22.47783660888672,
      "learning_rate": 7.870191049463492e-06,
      "loss": 0.8285,
      "step": 1316
    },
    {
      "epoch": 0.03447222852356606,
      "grad_norm": 19.093719482421875,
      "learning_rate": 7.867050510337608e-06,
      "loss": 0.3702,
      "step": 1317
    },
    {
      "epoch": 0.03449840333641614,
      "grad_norm": 20.724027633666992,
      "learning_rate": 7.863909971211725e-06,
      "loss": 0.8617,
      "step": 1318
    },
    {
      "epoch": 0.03452457814926623,
      "grad_norm": 19.681331634521484,
      "learning_rate": 7.860769432085841e-06,
      "loss": 0.503,
      "step": 1319
    },
    {
      "epoch": 0.03455075296211632,
      "grad_norm": 20.066869735717773,
      "learning_rate": 7.857628892959959e-06,
      "loss": 0.6903,
      "step": 1320
    },
    {
      "epoch": 0.03457692777496641,
      "grad_norm": 16.57672691345215,
      "learning_rate": 7.854488353834076e-06,
      "loss": 0.5975,
      "step": 1321
    },
    {
      "epoch": 0.0346031025878165,
      "grad_norm": 15.874419212341309,
      "learning_rate": 7.851347814708192e-06,
      "loss": 0.254,
      "step": 1322
    },
    {
      "epoch": 0.03462927740066658,
      "grad_norm": 19.26849365234375,
      "learning_rate": 7.84820727558231e-06,
      "loss": 0.6495,
      "step": 1323
    },
    {
      "epoch": 0.03465545221351667,
      "grad_norm": 11.576543807983398,
      "learning_rate": 7.845066736456424e-06,
      "loss": 0.2715,
      "step": 1324
    },
    {
      "epoch": 0.03468162702636676,
      "grad_norm": 12.823216438293457,
      "learning_rate": 7.841926197330541e-06,
      "loss": 0.2618,
      "step": 1325
    },
    {
      "epoch": 0.03470780183921685,
      "grad_norm": 18.61086654663086,
      "learning_rate": 7.838785658204659e-06,
      "loss": 0.4962,
      "step": 1326
    },
    {
      "epoch": 0.034733976652066935,
      "grad_norm": 19.859888076782227,
      "learning_rate": 7.835645119078775e-06,
      "loss": 0.471,
      "step": 1327
    },
    {
      "epoch": 0.034760151464917025,
      "grad_norm": 17.080537796020508,
      "learning_rate": 7.832504579952892e-06,
      "loss": 0.3902,
      "step": 1328
    },
    {
      "epoch": 0.034786326277767114,
      "grad_norm": 17.70814323425293,
      "learning_rate": 7.829364040827008e-06,
      "loss": 0.4042,
      "step": 1329
    },
    {
      "epoch": 0.034812501090617204,
      "grad_norm": 19.63416290283203,
      "learning_rate": 7.826223501701126e-06,
      "loss": 0.6017,
      "step": 1330
    },
    {
      "epoch": 0.034838675903467294,
      "grad_norm": 15.477885246276855,
      "learning_rate": 7.823082962575242e-06,
      "loss": 0.3228,
      "step": 1331
    },
    {
      "epoch": 0.034864850716317376,
      "grad_norm": 14.766392707824707,
      "learning_rate": 7.81994242344936e-06,
      "loss": 0.5243,
      "step": 1332
    },
    {
      "epoch": 0.034891025529167466,
      "grad_norm": 19.511646270751953,
      "learning_rate": 7.816801884323477e-06,
      "loss": 0.7817,
      "step": 1333
    },
    {
      "epoch": 0.034917200342017556,
      "grad_norm": 23.270505905151367,
      "learning_rate": 7.813661345197593e-06,
      "loss": 0.8959,
      "step": 1334
    },
    {
      "epoch": 0.034943375154867645,
      "grad_norm": 23.08707618713379,
      "learning_rate": 7.81052080607171e-06,
      "loss": 0.5739,
      "step": 1335
    },
    {
      "epoch": 0.03496954996771773,
      "grad_norm": 31.27659034729004,
      "learning_rate": 7.807380266945826e-06,
      "loss": 0.8297,
      "step": 1336
    },
    {
      "epoch": 0.03499572478056782,
      "grad_norm": 12.797849655151367,
      "learning_rate": 7.804239727819944e-06,
      "loss": 0.2643,
      "step": 1337
    },
    {
      "epoch": 0.03502189959341791,
      "grad_norm": 11.073772430419922,
      "learning_rate": 7.80109918869406e-06,
      "loss": 0.199,
      "step": 1338
    },
    {
      "epoch": 0.035048074406268,
      "grad_norm": 18.169187545776367,
      "learning_rate": 7.797958649568177e-06,
      "loss": 0.46,
      "step": 1339
    },
    {
      "epoch": 0.03507424921911809,
      "grad_norm": 19.734333038330078,
      "learning_rate": 7.794818110442293e-06,
      "loss": 1.0124,
      "step": 1340
    },
    {
      "epoch": 0.03510042403196817,
      "grad_norm": 17.85934066772461,
      "learning_rate": 7.791677571316409e-06,
      "loss": 0.3997,
      "step": 1341
    },
    {
      "epoch": 0.03512659884481826,
      "grad_norm": 22.923633575439453,
      "learning_rate": 7.788537032190526e-06,
      "loss": 0.5123,
      "step": 1342
    },
    {
      "epoch": 0.03515277365766835,
      "grad_norm": 25.790788650512695,
      "learning_rate": 7.785396493064642e-06,
      "loss": 0.7546,
      "step": 1343
    },
    {
      "epoch": 0.03517894847051844,
      "grad_norm": 16.092100143432617,
      "learning_rate": 7.78225595393876e-06,
      "loss": 0.4006,
      "step": 1344
    },
    {
      "epoch": 0.03520512328336852,
      "grad_norm": 23.701419830322266,
      "learning_rate": 7.779115414812875e-06,
      "loss": 0.5325,
      "step": 1345
    },
    {
      "epoch": 0.03523129809621861,
      "grad_norm": 13.217493057250977,
      "learning_rate": 7.775974875686993e-06,
      "loss": 0.5302,
      "step": 1346
    },
    {
      "epoch": 0.0352574729090687,
      "grad_norm": 14.700857162475586,
      "learning_rate": 7.77283433656111e-06,
      "loss": 0.2953,
      "step": 1347
    },
    {
      "epoch": 0.03528364772191879,
      "grad_norm": 17.1502685546875,
      "learning_rate": 7.769693797435226e-06,
      "loss": 0.3497,
      "step": 1348
    },
    {
      "epoch": 0.03530982253476888,
      "grad_norm": 30.61972999572754,
      "learning_rate": 7.766553258309344e-06,
      "loss": 1.0732,
      "step": 1349
    },
    {
      "epoch": 0.03533599734761896,
      "grad_norm": 18.039615631103516,
      "learning_rate": 7.76341271918346e-06,
      "loss": 0.2422,
      "step": 1350
    },
    {
      "epoch": 0.03536217216046905,
      "grad_norm": 25.948238372802734,
      "learning_rate": 7.760272180057577e-06,
      "loss": 0.6422,
      "step": 1351
    },
    {
      "epoch": 0.03538834697331914,
      "grad_norm": 16.510948181152344,
      "learning_rate": 7.757131640931693e-06,
      "loss": 0.3472,
      "step": 1352
    },
    {
      "epoch": 0.03541452178616923,
      "grad_norm": 18.442420959472656,
      "learning_rate": 7.75399110180581e-06,
      "loss": 0.4011,
      "step": 1353
    },
    {
      "epoch": 0.035440696599019314,
      "grad_norm": 22.480772018432617,
      "learning_rate": 7.750850562679927e-06,
      "loss": 0.897,
      "step": 1354
    },
    {
      "epoch": 0.035466871411869404,
      "grad_norm": 49.46504211425781,
      "learning_rate": 7.747710023554043e-06,
      "loss": 0.7781,
      "step": 1355
    },
    {
      "epoch": 0.03549304622471949,
      "grad_norm": 19.405851364135742,
      "learning_rate": 7.74456948442816e-06,
      "loss": 0.6073,
      "step": 1356
    },
    {
      "epoch": 0.03551922103756958,
      "grad_norm": 16.261978149414062,
      "learning_rate": 7.741428945302276e-06,
      "loss": 0.4172,
      "step": 1357
    },
    {
      "epoch": 0.03554539585041967,
      "grad_norm": 15.889765739440918,
      "learning_rate": 7.738288406176394e-06,
      "loss": 0.6333,
      "step": 1358
    },
    {
      "epoch": 0.035571570663269755,
      "grad_norm": 18.155166625976562,
      "learning_rate": 7.735147867050511e-06,
      "loss": 0.4572,
      "step": 1359
    },
    {
      "epoch": 0.035597745476119845,
      "grad_norm": 24.878379821777344,
      "learning_rate": 7.732007327924627e-06,
      "loss": 0.4352,
      "step": 1360
    },
    {
      "epoch": 0.035623920288969935,
      "grad_norm": 16.14891815185547,
      "learning_rate": 7.728866788798744e-06,
      "loss": 0.3003,
      "step": 1361
    },
    {
      "epoch": 0.035650095101820024,
      "grad_norm": 21.342958450317383,
      "learning_rate": 7.72572624967286e-06,
      "loss": 0.584,
      "step": 1362
    },
    {
      "epoch": 0.03567626991467011,
      "grad_norm": 27.541357040405273,
      "learning_rate": 7.722585710546978e-06,
      "loss": 0.5162,
      "step": 1363
    },
    {
      "epoch": 0.0357024447275202,
      "grad_norm": 18.027944564819336,
      "learning_rate": 7.719445171421094e-06,
      "loss": 0.5747,
      "step": 1364
    },
    {
      "epoch": 0.035728619540370286,
      "grad_norm": 13.018811225891113,
      "learning_rate": 7.716304632295211e-06,
      "loss": 0.5648,
      "step": 1365
    },
    {
      "epoch": 0.035754794353220376,
      "grad_norm": 38.16303634643555,
      "learning_rate": 7.713164093169327e-06,
      "loss": 0.6872,
      "step": 1366
    },
    {
      "epoch": 0.035780969166070466,
      "grad_norm": 18.36562728881836,
      "learning_rate": 7.710023554043445e-06,
      "loss": 0.4778,
      "step": 1367
    },
    {
      "epoch": 0.03580714397892055,
      "grad_norm": 15.002086639404297,
      "learning_rate": 7.706883014917562e-06,
      "loss": 0.4369,
      "step": 1368
    },
    {
      "epoch": 0.03583331879177064,
      "grad_norm": 17.472349166870117,
      "learning_rate": 7.703742475791678e-06,
      "loss": 0.4633,
      "step": 1369
    },
    {
      "epoch": 0.03585949360462073,
      "grad_norm": 27.5150089263916,
      "learning_rate": 7.700601936665796e-06,
      "loss": 0.8142,
      "step": 1370
    },
    {
      "epoch": 0.03588566841747082,
      "grad_norm": 18.18723487854004,
      "learning_rate": 7.69746139753991e-06,
      "loss": 0.2918,
      "step": 1371
    },
    {
      "epoch": 0.0359118432303209,
      "grad_norm": 28.390600204467773,
      "learning_rate": 7.694320858414027e-06,
      "loss": 0.899,
      "step": 1372
    },
    {
      "epoch": 0.03593801804317099,
      "grad_norm": 21.234086990356445,
      "learning_rate": 7.691180319288145e-06,
      "loss": 0.6195,
      "step": 1373
    },
    {
      "epoch": 0.03596419285602108,
      "grad_norm": 25.316301345825195,
      "learning_rate": 7.68803978016226e-06,
      "loss": 0.4485,
      "step": 1374
    },
    {
      "epoch": 0.03599036766887117,
      "grad_norm": 16.25037384033203,
      "learning_rate": 7.684899241036378e-06,
      "loss": 0.7611,
      "step": 1375
    },
    {
      "epoch": 0.03601654248172126,
      "grad_norm": 25.999855041503906,
      "learning_rate": 7.681758701910494e-06,
      "loss": 1.1376,
      "step": 1376
    },
    {
      "epoch": 0.03604271729457134,
      "grad_norm": 21.143308639526367,
      "learning_rate": 7.678618162784612e-06,
      "loss": 0.7489,
      "step": 1377
    },
    {
      "epoch": 0.03606889210742143,
      "grad_norm": 16.70027732849121,
      "learning_rate": 7.675477623658728e-06,
      "loss": 0.7943,
      "step": 1378
    },
    {
      "epoch": 0.03609506692027152,
      "grad_norm": 17.25104522705078,
      "learning_rate": 7.672337084532845e-06,
      "loss": 0.4639,
      "step": 1379
    },
    {
      "epoch": 0.03612124173312161,
      "grad_norm": 16.244657516479492,
      "learning_rate": 7.669196545406963e-06,
      "loss": 0.5942,
      "step": 1380
    },
    {
      "epoch": 0.03614741654597169,
      "grad_norm": 22.37664031982422,
      "learning_rate": 7.666056006281079e-06,
      "loss": 0.8282,
      "step": 1381
    },
    {
      "epoch": 0.03617359135882178,
      "grad_norm": 21.173603057861328,
      "learning_rate": 7.662915467155196e-06,
      "loss": 0.6476,
      "step": 1382
    },
    {
      "epoch": 0.03619976617167187,
      "grad_norm": 16.887577056884766,
      "learning_rate": 7.659774928029312e-06,
      "loss": 0.5999,
      "step": 1383
    },
    {
      "epoch": 0.03622594098452196,
      "grad_norm": 27.966081619262695,
      "learning_rate": 7.65663438890343e-06,
      "loss": 0.5188,
      "step": 1384
    },
    {
      "epoch": 0.03625211579737205,
      "grad_norm": 17.79327392578125,
      "learning_rate": 7.653493849777545e-06,
      "loss": 0.4321,
      "step": 1385
    },
    {
      "epoch": 0.036278290610222134,
      "grad_norm": 16.45579719543457,
      "learning_rate": 7.650353310651661e-06,
      "loss": 0.6958,
      "step": 1386
    },
    {
      "epoch": 0.036304465423072224,
      "grad_norm": 19.377819061279297,
      "learning_rate": 7.647212771525779e-06,
      "loss": 1.051,
      "step": 1387
    },
    {
      "epoch": 0.036330640235922314,
      "grad_norm": 27.450275421142578,
      "learning_rate": 7.644072232399895e-06,
      "loss": 0.444,
      "step": 1388
    },
    {
      "epoch": 0.0363568150487724,
      "grad_norm": 9.35712718963623,
      "learning_rate": 7.640931693274012e-06,
      "loss": 0.1279,
      "step": 1389
    },
    {
      "epoch": 0.036382989861622486,
      "grad_norm": 20.082454681396484,
      "learning_rate": 7.637791154148128e-06,
      "loss": 0.6731,
      "step": 1390
    },
    {
      "epoch": 0.036409164674472576,
      "grad_norm": 10.431342124938965,
      "learning_rate": 7.634650615022246e-06,
      "loss": 0.2511,
      "step": 1391
    },
    {
      "epoch": 0.036435339487322665,
      "grad_norm": 18.055721282958984,
      "learning_rate": 7.631510075896362e-06,
      "loss": 0.4269,
      "step": 1392
    },
    {
      "epoch": 0.036461514300172755,
      "grad_norm": 14.912940979003906,
      "learning_rate": 7.628369536770479e-06,
      "loss": 0.4764,
      "step": 1393
    },
    {
      "epoch": 0.036487689113022845,
      "grad_norm": 14.448505401611328,
      "learning_rate": 7.625228997644596e-06,
      "loss": 0.7704,
      "step": 1394
    },
    {
      "epoch": 0.03651386392587293,
      "grad_norm": 13.894495010375977,
      "learning_rate": 7.6220884585187125e-06,
      "loss": 0.9547,
      "step": 1395
    },
    {
      "epoch": 0.03654003873872302,
      "grad_norm": 19.422962188720703,
      "learning_rate": 7.618947919392829e-06,
      "loss": 0.9213,
      "step": 1396
    },
    {
      "epoch": 0.03656621355157311,
      "grad_norm": 17.59050941467285,
      "learning_rate": 7.615807380266947e-06,
      "loss": 0.7612,
      "step": 1397
    },
    {
      "epoch": 0.036592388364423196,
      "grad_norm": 19.853649139404297,
      "learning_rate": 7.6126668411410635e-06,
      "loss": 0.5971,
      "step": 1398
    },
    {
      "epoch": 0.03661856317727328,
      "grad_norm": 20.637800216674805,
      "learning_rate": 7.60952630201518e-06,
      "loss": 0.5155,
      "step": 1399
    },
    {
      "epoch": 0.03664473799012337,
      "grad_norm": 15.478019714355469,
      "learning_rate": 7.606385762889297e-06,
      "loss": 0.5517,
      "step": 1400
    },
    {
      "epoch": 0.03667091280297346,
      "grad_norm": 14.141962051391602,
      "learning_rate": 7.603245223763414e-06,
      "loss": 0.4856,
      "step": 1401
    },
    {
      "epoch": 0.03669708761582355,
      "grad_norm": 23.931318283081055,
      "learning_rate": 7.6001046846375294e-06,
      "loss": 0.5104,
      "step": 1402
    },
    {
      "epoch": 0.03672326242867364,
      "grad_norm": 21.694766998291016,
      "learning_rate": 7.596964145511646e-06,
      "loss": 0.6666,
      "step": 1403
    },
    {
      "epoch": 0.03674943724152372,
      "grad_norm": 14.489320755004883,
      "learning_rate": 7.593823606385763e-06,
      "loss": 0.2401,
      "step": 1404
    },
    {
      "epoch": 0.03677561205437381,
      "grad_norm": 27.71939468383789,
      "learning_rate": 7.5906830672598796e-06,
      "loss": 0.5153,
      "step": 1405
    },
    {
      "epoch": 0.0368017868672239,
      "grad_norm": 17.918306350708008,
      "learning_rate": 7.587542528133996e-06,
      "loss": 0.5716,
      "step": 1406
    },
    {
      "epoch": 0.03682796168007399,
      "grad_norm": 23.813919067382812,
      "learning_rate": 7.584401989008113e-06,
      "loss": 0.453,
      "step": 1407
    },
    {
      "epoch": 0.03685413649292407,
      "grad_norm": 22.07146644592285,
      "learning_rate": 7.58126144988223e-06,
      "loss": 0.8975,
      "step": 1408
    },
    {
      "epoch": 0.03688031130577416,
      "grad_norm": 11.604959487915039,
      "learning_rate": 7.578120910756346e-06,
      "loss": 0.4966,
      "step": 1409
    },
    {
      "epoch": 0.03690648611862425,
      "grad_norm": 24.725427627563477,
      "learning_rate": 7.574980371630464e-06,
      "loss": 0.5359,
      "step": 1410
    },
    {
      "epoch": 0.03693266093147434,
      "grad_norm": 8.68175983428955,
      "learning_rate": 7.571839832504581e-06,
      "loss": 0.1128,
      "step": 1411
    },
    {
      "epoch": 0.03695883574432443,
      "grad_norm": 19.24297523498535,
      "learning_rate": 7.568699293378697e-06,
      "loss": 0.4394,
      "step": 1412
    },
    {
      "epoch": 0.036985010557174514,
      "grad_norm": 18.26197052001953,
      "learning_rate": 7.565558754252814e-06,
      "loss": 0.9472,
      "step": 1413
    },
    {
      "epoch": 0.0370111853700246,
      "grad_norm": 13.033663749694824,
      "learning_rate": 7.562418215126931e-06,
      "loss": 0.3853,
      "step": 1414
    },
    {
      "epoch": 0.03703736018287469,
      "grad_norm": 14.647751808166504,
      "learning_rate": 7.5592776760010475e-06,
      "loss": 0.5707,
      "step": 1415
    },
    {
      "epoch": 0.03706353499572478,
      "grad_norm": 20.48818016052246,
      "learning_rate": 7.556137136875164e-06,
      "loss": 0.8668,
      "step": 1416
    },
    {
      "epoch": 0.037089709808574865,
      "grad_norm": 21.985607147216797,
      "learning_rate": 7.55299659774928e-06,
      "loss": 0.7805,
      "step": 1417
    },
    {
      "epoch": 0.037115884621424955,
      "grad_norm": 12.50190258026123,
      "learning_rate": 7.549856058623397e-06,
      "loss": 0.3437,
      "step": 1418
    },
    {
      "epoch": 0.037142059434275045,
      "grad_norm": 12.195987701416016,
      "learning_rate": 7.5467155194975134e-06,
      "loss": 0.2682,
      "step": 1419
    },
    {
      "epoch": 0.037168234247125134,
      "grad_norm": 25.574716567993164,
      "learning_rate": 7.54357498037163e-06,
      "loss": 0.7044,
      "step": 1420
    },
    {
      "epoch": 0.037194409059975224,
      "grad_norm": 16.818355560302734,
      "learning_rate": 7.540434441245747e-06,
      "loss": 0.3136,
      "step": 1421
    },
    {
      "epoch": 0.03722058387282531,
      "grad_norm": 31.257579803466797,
      "learning_rate": 7.5372939021198636e-06,
      "loss": 0.6915,
      "step": 1422
    },
    {
      "epoch": 0.037246758685675396,
      "grad_norm": 16.990232467651367,
      "learning_rate": 7.534153362993981e-06,
      "loss": 0.2904,
      "step": 1423
    },
    {
      "epoch": 0.037272933498525486,
      "grad_norm": 22.440343856811523,
      "learning_rate": 7.531012823868098e-06,
      "loss": 0.7268,
      "step": 1424
    },
    {
      "epoch": 0.037299108311375576,
      "grad_norm": 21.59572982788086,
      "learning_rate": 7.5278722847422145e-06,
      "loss": 0.515,
      "step": 1425
    },
    {
      "epoch": 0.037325283124225665,
      "grad_norm": 14.902255058288574,
      "learning_rate": 7.524731745616331e-06,
      "loss": 0.508,
      "step": 1426
    },
    {
      "epoch": 0.03735145793707575,
      "grad_norm": 13.750914573669434,
      "learning_rate": 7.521591206490448e-06,
      "loss": 0.2901,
      "step": 1427
    },
    {
      "epoch": 0.03737763274992584,
      "grad_norm": 17.595788955688477,
      "learning_rate": 7.518450667364565e-06,
      "loss": 0.8069,
      "step": 1428
    },
    {
      "epoch": 0.03740380756277593,
      "grad_norm": 21.429454803466797,
      "learning_rate": 7.515310128238681e-06,
      "loss": 0.7075,
      "step": 1429
    },
    {
      "epoch": 0.03742998237562602,
      "grad_norm": 15.598773956298828,
      "learning_rate": 7.512169589112798e-06,
      "loss": 0.3924,
      "step": 1430
    },
    {
      "epoch": 0.0374561571884761,
      "grad_norm": 18.841466903686523,
      "learning_rate": 7.509029049986916e-06,
      "loss": 0.6932,
      "step": 1431
    },
    {
      "epoch": 0.03748233200132619,
      "grad_norm": 16.787240982055664,
      "learning_rate": 7.505888510861031e-06,
      "loss": 0.7044,
      "step": 1432
    },
    {
      "epoch": 0.03750850681417628,
      "grad_norm": 18.73250389099121,
      "learning_rate": 7.502747971735147e-06,
      "loss": 0.5133,
      "step": 1433
    },
    {
      "epoch": 0.03753468162702637,
      "grad_norm": 29.851219177246094,
      "learning_rate": 7.499607432609264e-06,
      "loss": 0.8465,
      "step": 1434
    },
    {
      "epoch": 0.03756085643987646,
      "grad_norm": 19.36211585998535,
      "learning_rate": 7.496466893483381e-06,
      "loss": 0.3958,
      "step": 1435
    },
    {
      "epoch": 0.03758703125272654,
      "grad_norm": 21.833572387695312,
      "learning_rate": 7.493326354357498e-06,
      "loss": 0.8468,
      "step": 1436
    },
    {
      "epoch": 0.03761320606557663,
      "grad_norm": 16.266891479492188,
      "learning_rate": 7.490185815231615e-06,
      "loss": 0.3284,
      "step": 1437
    },
    {
      "epoch": 0.03763938087842672,
      "grad_norm": 29.233888626098633,
      "learning_rate": 7.487045276105732e-06,
      "loss": 0.8143,
      "step": 1438
    },
    {
      "epoch": 0.03766555569127681,
      "grad_norm": 15.55520248413086,
      "learning_rate": 7.483904736979848e-06,
      "loss": 0.2281,
      "step": 1439
    },
    {
      "epoch": 0.03769173050412689,
      "grad_norm": 16.650053024291992,
      "learning_rate": 7.480764197853965e-06,
      "loss": 0.6471,
      "step": 1440
    },
    {
      "epoch": 0.03771790531697698,
      "grad_norm": 18.428329467773438,
      "learning_rate": 7.477623658728082e-06,
      "loss": 0.6192,
      "step": 1441
    },
    {
      "epoch": 0.03774408012982707,
      "grad_norm": 21.165912628173828,
      "learning_rate": 7.4744831196021985e-06,
      "loss": 0.6249,
      "step": 1442
    },
    {
      "epoch": 0.03777025494267716,
      "grad_norm": 26.311777114868164,
      "learning_rate": 7.471342580476315e-06,
      "loss": 0.5509,
      "step": 1443
    },
    {
      "epoch": 0.03779642975552725,
      "grad_norm": 11.426887512207031,
      "learning_rate": 7.468202041350433e-06,
      "loss": 0.29,
      "step": 1444
    },
    {
      "epoch": 0.037822604568377334,
      "grad_norm": 11.62671947479248,
      "learning_rate": 7.4650615022245495e-06,
      "loss": 0.3092,
      "step": 1445
    },
    {
      "epoch": 0.037848779381227424,
      "grad_norm": 18.783369064331055,
      "learning_rate": 7.461920963098666e-06,
      "loss": 0.8567,
      "step": 1446
    },
    {
      "epoch": 0.03787495419407751,
      "grad_norm": 23.087942123413086,
      "learning_rate": 7.458780423972783e-06,
      "loss": 0.6405,
      "step": 1447
    },
    {
      "epoch": 0.0379011290069276,
      "grad_norm": 26.76946258544922,
      "learning_rate": 7.455639884846898e-06,
      "loss": 0.6339,
      "step": 1448
    },
    {
      "epoch": 0.037927303819777686,
      "grad_norm": 22.75250244140625,
      "learning_rate": 7.4524993457210155e-06,
      "loss": 0.6436,
      "step": 1449
    },
    {
      "epoch": 0.037953478632627775,
      "grad_norm": 18.113096237182617,
      "learning_rate": 7.449358806595132e-06,
      "loss": 0.7094,
      "step": 1450
    },
    {
      "epoch": 0.037979653445477865,
      "grad_norm": 21.78819465637207,
      "learning_rate": 7.446218267469249e-06,
      "loss": 0.7182,
      "step": 1451
    },
    {
      "epoch": 0.038005828258327955,
      "grad_norm": 17.43453025817871,
      "learning_rate": 7.443077728343366e-06,
      "loss": 0.3352,
      "step": 1452
    },
    {
      "epoch": 0.038032003071178044,
      "grad_norm": 13.596834182739258,
      "learning_rate": 7.439937189217482e-06,
      "loss": 0.2728,
      "step": 1453
    },
    {
      "epoch": 0.03805817788402813,
      "grad_norm": 37.49578094482422,
      "learning_rate": 7.436796650091599e-06,
      "loss": 0.3714,
      "step": 1454
    },
    {
      "epoch": 0.03808435269687822,
      "grad_norm": 20.94324493408203,
      "learning_rate": 7.433656110965716e-06,
      "loss": 0.5018,
      "step": 1455
    },
    {
      "epoch": 0.038110527509728306,
      "grad_norm": 16.9849796295166,
      "learning_rate": 7.430515571839832e-06,
      "loss": 0.3451,
      "step": 1456
    },
    {
      "epoch": 0.038136702322578396,
      "grad_norm": 18.325794219970703,
      "learning_rate": 7.42737503271395e-06,
      "loss": 0.4887,
      "step": 1457
    },
    {
      "epoch": 0.03816287713542848,
      "grad_norm": 19.92972755432129,
      "learning_rate": 7.424234493588067e-06,
      "loss": 0.5192,
      "step": 1458
    },
    {
      "epoch": 0.03818905194827857,
      "grad_norm": 21.545682907104492,
      "learning_rate": 7.421093954462183e-06,
      "loss": 0.606,
      "step": 1459
    },
    {
      "epoch": 0.03821522676112866,
      "grad_norm": 17.892908096313477,
      "learning_rate": 7.4179534153363e-06,
      "loss": 0.6296,
      "step": 1460
    },
    {
      "epoch": 0.03824140157397875,
      "grad_norm": 15.057950973510742,
      "learning_rate": 7.414812876210417e-06,
      "loss": 0.2417,
      "step": 1461
    },
    {
      "epoch": 0.03826757638682884,
      "grad_norm": 30.626773834228516,
      "learning_rate": 7.4116723370845335e-06,
      "loss": 0.6107,
      "step": 1462
    },
    {
      "epoch": 0.03829375119967892,
      "grad_norm": 18.15503692626953,
      "learning_rate": 7.408531797958649e-06,
      "loss": 0.4327,
      "step": 1463
    },
    {
      "epoch": 0.03831992601252901,
      "grad_norm": 26.8110408782959,
      "learning_rate": 7.405391258832766e-06,
      "loss": 0.3955,
      "step": 1464
    },
    {
      "epoch": 0.0383461008253791,
      "grad_norm": 26.769874572753906,
      "learning_rate": 7.402250719706883e-06,
      "loss": 0.6507,
      "step": 1465
    },
    {
      "epoch": 0.03837227563822919,
      "grad_norm": 23.900054931640625,
      "learning_rate": 7.3991101805809995e-06,
      "loss": 0.4608,
      "step": 1466
    },
    {
      "epoch": 0.03839845045107927,
      "grad_norm": 15.968517303466797,
      "learning_rate": 7.395969641455116e-06,
      "loss": 0.5061,
      "step": 1467
    },
    {
      "epoch": 0.03842462526392936,
      "grad_norm": 24.709274291992188,
      "learning_rate": 7.392829102329233e-06,
      "loss": 0.7406,
      "step": 1468
    },
    {
      "epoch": 0.03845080007677945,
      "grad_norm": 21.211490631103516,
      "learning_rate": 7.38968856320335e-06,
      "loss": 0.4072,
      "step": 1469
    },
    {
      "epoch": 0.03847697488962954,
      "grad_norm": 30.598562240600586,
      "learning_rate": 7.386548024077467e-06,
      "loss": 0.4367,
      "step": 1470
    },
    {
      "epoch": 0.03850314970247963,
      "grad_norm": 20.80502700805664,
      "learning_rate": 7.383407484951584e-06,
      "loss": 0.5234,
      "step": 1471
    },
    {
      "epoch": 0.03852932451532971,
      "grad_norm": 19.792203903198242,
      "learning_rate": 7.3802669458257006e-06,
      "loss": 0.6647,
      "step": 1472
    },
    {
      "epoch": 0.0385554993281798,
      "grad_norm": 9.339995384216309,
      "learning_rate": 7.377126406699817e-06,
      "loss": 0.1764,
      "step": 1473
    },
    {
      "epoch": 0.03858167414102989,
      "grad_norm": 25.601465225219727,
      "learning_rate": 7.373985867573934e-06,
      "loss": 0.488,
      "step": 1474
    },
    {
      "epoch": 0.03860784895387998,
      "grad_norm": 21.86649513244629,
      "learning_rate": 7.370845328448051e-06,
      "loss": 0.7823,
      "step": 1475
    },
    {
      "epoch": 0.038634023766730065,
      "grad_norm": 22.11231803894043,
      "learning_rate": 7.367704789322167e-06,
      "loss": 0.4909,
      "step": 1476
    },
    {
      "epoch": 0.038660198579580154,
      "grad_norm": 18.584230422973633,
      "learning_rate": 7.364564250196284e-06,
      "loss": 0.8305,
      "step": 1477
    },
    {
      "epoch": 0.038686373392430244,
      "grad_norm": 15.03104019165039,
      "learning_rate": 7.361423711070402e-06,
      "loss": 0.4608,
      "step": 1478
    },
    {
      "epoch": 0.038712548205280334,
      "grad_norm": 16.96283721923828,
      "learning_rate": 7.358283171944517e-06,
      "loss": 0.3897,
      "step": 1479
    },
    {
      "epoch": 0.03873872301813042,
      "grad_norm": 17.228872299194336,
      "learning_rate": 7.355142632818633e-06,
      "loss": 0.3274,
      "step": 1480
    },
    {
      "epoch": 0.038764897830980506,
      "grad_norm": 28.528059005737305,
      "learning_rate": 7.35200209369275e-06,
      "loss": 0.3366,
      "step": 1481
    },
    {
      "epoch": 0.038791072643830596,
      "grad_norm": 26.39232063293457,
      "learning_rate": 7.348861554566867e-06,
      "loss": 0.5606,
      "step": 1482
    },
    {
      "epoch": 0.038817247456680685,
      "grad_norm": 15.871273040771484,
      "learning_rate": 7.345721015440984e-06,
      "loss": 0.4716,
      "step": 1483
    },
    {
      "epoch": 0.038843422269530775,
      "grad_norm": 22.237503051757812,
      "learning_rate": 7.342580476315101e-06,
      "loss": 0.686,
      "step": 1484
    },
    {
      "epoch": 0.03886959708238086,
      "grad_norm": 25.250869750976562,
      "learning_rate": 7.339439937189218e-06,
      "loss": 1.0876,
      "step": 1485
    },
    {
      "epoch": 0.03889577189523095,
      "grad_norm": 20.796119689941406,
      "learning_rate": 7.3362993980633345e-06,
      "loss": 0.6586,
      "step": 1486
    },
    {
      "epoch": 0.03892194670808104,
      "grad_norm": 15.70380687713623,
      "learning_rate": 7.333158858937451e-06,
      "loss": 0.4432,
      "step": 1487
    },
    {
      "epoch": 0.03894812152093113,
      "grad_norm": 22.912534713745117,
      "learning_rate": 7.330018319811568e-06,
      "loss": 0.919,
      "step": 1488
    },
    {
      "epoch": 0.038974296333781216,
      "grad_norm": 27.484203338623047,
      "learning_rate": 7.3268777806856846e-06,
      "loss": 0.7464,
      "step": 1489
    },
    {
      "epoch": 0.0390004711466313,
      "grad_norm": 20.035097122192383,
      "learning_rate": 7.323737241559801e-06,
      "loss": 0.9963,
      "step": 1490
    },
    {
      "epoch": 0.03902664595948139,
      "grad_norm": 21.095041275024414,
      "learning_rate": 7.320596702433919e-06,
      "loss": 0.4403,
      "step": 1491
    },
    {
      "epoch": 0.03905282077233148,
      "grad_norm": 27.941272735595703,
      "learning_rate": 7.3174561633080355e-06,
      "loss": 0.3236,
      "step": 1492
    },
    {
      "epoch": 0.03907899558518157,
      "grad_norm": 16.119701385498047,
      "learning_rate": 7.314315624182152e-06,
      "loss": 0.4987,
      "step": 1493
    },
    {
      "epoch": 0.03910517039803165,
      "grad_norm": 18.922319412231445,
      "learning_rate": 7.311175085056267e-06,
      "loss": 0.6455,
      "step": 1494
    },
    {
      "epoch": 0.03913134521088174,
      "grad_norm": 17.422975540161133,
      "learning_rate": 7.308034545930384e-06,
      "loss": 0.5608,
      "step": 1495
    },
    {
      "epoch": 0.03915752002373183,
      "grad_norm": 28.261371612548828,
      "learning_rate": 7.3048940068045015e-06,
      "loss": 0.8546,
      "step": 1496
    },
    {
      "epoch": 0.03918369483658192,
      "grad_norm": 15.347820281982422,
      "learning_rate": 7.301753467678618e-06,
      "loss": 0.4523,
      "step": 1497
    },
    {
      "epoch": 0.03920986964943201,
      "grad_norm": 14.259330749511719,
      "learning_rate": 7.298612928552735e-06,
      "loss": 0.2565,
      "step": 1498
    },
    {
      "epoch": 0.03923604446228209,
      "grad_norm": 13.254655838012695,
      "learning_rate": 7.295472389426852e-06,
      "loss": 0.275,
      "step": 1499
    },
    {
      "epoch": 0.03926221927513218,
      "grad_norm": 20.085859298706055,
      "learning_rate": 7.292331850300968e-06,
      "loss": 0.3967,
      "step": 1500
    },
    {
      "epoch": 0.03928839408798227,
      "grad_norm": 20.417455673217773,
      "learning_rate": 7.289191311175085e-06,
      "loss": 0.4526,
      "step": 1501
    },
    {
      "epoch": 0.03931456890083236,
      "grad_norm": 15.745759963989258,
      "learning_rate": 7.286050772049202e-06,
      "loss": 0.3586,
      "step": 1502
    },
    {
      "epoch": 0.039340743713682444,
      "grad_norm": 11.131237030029297,
      "learning_rate": 7.2829102329233185e-06,
      "loss": 0.1272,
      "step": 1503
    },
    {
      "epoch": 0.039366918526532534,
      "grad_norm": 21.445091247558594,
      "learning_rate": 7.279769693797436e-06,
      "loss": 0.9983,
      "step": 1504
    },
    {
      "epoch": 0.03939309333938262,
      "grad_norm": 12.682723045349121,
      "learning_rate": 7.276629154671553e-06,
      "loss": 0.264,
      "step": 1505
    },
    {
      "epoch": 0.03941926815223271,
      "grad_norm": 14.207160949707031,
      "learning_rate": 7.2734886155456694e-06,
      "loss": 0.3361,
      "step": 1506
    },
    {
      "epoch": 0.0394454429650828,
      "grad_norm": 19.93119239807129,
      "learning_rate": 7.270348076419786e-06,
      "loss": 0.5673,
      "step": 1507
    },
    {
      "epoch": 0.039471617777932885,
      "grad_norm": 24.537784576416016,
      "learning_rate": 7.267207537293903e-06,
      "loss": 0.4526,
      "step": 1508
    },
    {
      "epoch": 0.039497792590782975,
      "grad_norm": 13.335036277770996,
      "learning_rate": 7.2640669981680195e-06,
      "loss": 0.4321,
      "step": 1509
    },
    {
      "epoch": 0.039523967403633065,
      "grad_norm": 23.52393913269043,
      "learning_rate": 7.260926459042135e-06,
      "loss": 0.5907,
      "step": 1510
    },
    {
      "epoch": 0.039550142216483154,
      "grad_norm": 23.500255584716797,
      "learning_rate": 7.257785919916252e-06,
      "loss": 0.8508,
      "step": 1511
    },
    {
      "epoch": 0.03957631702933324,
      "grad_norm": 19.876989364624023,
      "learning_rate": 7.254645380790369e-06,
      "loss": 0.6755,
      "step": 1512
    },
    {
      "epoch": 0.03960249184218333,
      "grad_norm": 17.56342887878418,
      "learning_rate": 7.2515048416644855e-06,
      "loss": 0.3855,
      "step": 1513
    },
    {
      "epoch": 0.039628666655033416,
      "grad_norm": 29.988645553588867,
      "learning_rate": 7.248364302538602e-06,
      "loss": 1.0037,
      "step": 1514
    },
    {
      "epoch": 0.039654841467883506,
      "grad_norm": 19.924352645874023,
      "learning_rate": 7.245223763412719e-06,
      "loss": 0.704,
      "step": 1515
    },
    {
      "epoch": 0.039681016280733596,
      "grad_norm": 26.299592971801758,
      "learning_rate": 7.242083224286836e-06,
      "loss": 0.6431,
      "step": 1516
    },
    {
      "epoch": 0.03970719109358368,
      "grad_norm": 14.917527198791504,
      "learning_rate": 7.238942685160953e-06,
      "loss": 0.3656,
      "step": 1517
    },
    {
      "epoch": 0.03973336590643377,
      "grad_norm": 24.789949417114258,
      "learning_rate": 7.23580214603507e-06,
      "loss": 0.698,
      "step": 1518
    },
    {
      "epoch": 0.03975954071928386,
      "grad_norm": 14.641473770141602,
      "learning_rate": 7.232661606909187e-06,
      "loss": 0.3581,
      "step": 1519
    },
    {
      "epoch": 0.03978571553213395,
      "grad_norm": 11.580229759216309,
      "learning_rate": 7.229521067783303e-06,
      "loss": 0.4992,
      "step": 1520
    },
    {
      "epoch": 0.03981189034498403,
      "grad_norm": 19.23541259765625,
      "learning_rate": 7.22638052865742e-06,
      "loss": 0.5053,
      "step": 1521
    },
    {
      "epoch": 0.03983806515783412,
      "grad_norm": 12.09150505065918,
      "learning_rate": 7.223239989531537e-06,
      "loss": 0.4139,
      "step": 1522
    },
    {
      "epoch": 0.03986423997068421,
      "grad_norm": 16.358095169067383,
      "learning_rate": 7.2200994504056534e-06,
      "loss": 0.5373,
      "step": 1523
    },
    {
      "epoch": 0.0398904147835343,
      "grad_norm": 20.556411743164062,
      "learning_rate": 7.21695891127977e-06,
      "loss": 0.8092,
      "step": 1524
    },
    {
      "epoch": 0.03991658959638439,
      "grad_norm": 31.819297790527344,
      "learning_rate": 7.213818372153886e-06,
      "loss": 0.8352,
      "step": 1525
    },
    {
      "epoch": 0.03994276440923447,
      "grad_norm": 19.40224266052246,
      "learning_rate": 7.210677833028003e-06,
      "loss": 0.3701,
      "step": 1526
    },
    {
      "epoch": 0.03996893922208456,
      "grad_norm": 11.746187210083008,
      "learning_rate": 7.207537293902119e-06,
      "loss": 0.1679,
      "step": 1527
    },
    {
      "epoch": 0.03999511403493465,
      "grad_norm": 15.90229320526123,
      "learning_rate": 7.204396754776236e-06,
      "loss": 0.6186,
      "step": 1528
    },
    {
      "epoch": 0.04002128884778474,
      "grad_norm": 15.919110298156738,
      "learning_rate": 7.201256215650353e-06,
      "loss": 0.2381,
      "step": 1529
    },
    {
      "epoch": 0.04004746366063482,
      "grad_norm": 25.457807540893555,
      "learning_rate": 7.19811567652447e-06,
      "loss": 0.3232,
      "step": 1530
    },
    {
      "epoch": 0.04007363847348491,
      "grad_norm": 21.614940643310547,
      "learning_rate": 7.194975137398587e-06,
      "loss": 0.7257,
      "step": 1531
    },
    {
      "epoch": 0.040099813286335,
      "grad_norm": 19.384973526000977,
      "learning_rate": 7.191834598272704e-06,
      "loss": 0.6483,
      "step": 1532
    },
    {
      "epoch": 0.04012598809918509,
      "grad_norm": 35.22947692871094,
      "learning_rate": 7.1886940591468205e-06,
      "loss": 0.5972,
      "step": 1533
    },
    {
      "epoch": 0.04015216291203518,
      "grad_norm": 17.173900604248047,
      "learning_rate": 7.185553520020937e-06,
      "loss": 0.5226,
      "step": 1534
    },
    {
      "epoch": 0.040178337724885264,
      "grad_norm": 20.028762817382812,
      "learning_rate": 7.182412980895054e-06,
      "loss": 0.8096,
      "step": 1535
    },
    {
      "epoch": 0.040204512537735354,
      "grad_norm": 16.551254272460938,
      "learning_rate": 7.179272441769171e-06,
      "loss": 0.4704,
      "step": 1536
    },
    {
      "epoch": 0.040230687350585444,
      "grad_norm": 16.76205825805664,
      "learning_rate": 7.176131902643287e-06,
      "loss": 0.2969,
      "step": 1537
    },
    {
      "epoch": 0.04025686216343553,
      "grad_norm": 16.00514030456543,
      "learning_rate": 7.172991363517405e-06,
      "loss": 0.4854,
      "step": 1538
    },
    {
      "epoch": 0.04028303697628562,
      "grad_norm": 26.246049880981445,
      "learning_rate": 7.169850824391522e-06,
      "loss": 0.5445,
      "step": 1539
    },
    {
      "epoch": 0.040309211789135706,
      "grad_norm": 21.721416473388672,
      "learning_rate": 7.166710285265638e-06,
      "loss": 0.5438,
      "step": 1540
    },
    {
      "epoch": 0.040335386601985795,
      "grad_norm": 16.250337600708008,
      "learning_rate": 7.163569746139753e-06,
      "loss": 0.3795,
      "step": 1541
    },
    {
      "epoch": 0.040361561414835885,
      "grad_norm": 16.707557678222656,
      "learning_rate": 7.16042920701387e-06,
      "loss": 0.7633,
      "step": 1542
    },
    {
      "epoch": 0.040387736227685975,
      "grad_norm": 23.28533935546875,
      "learning_rate": 7.1572886678879876e-06,
      "loss": 0.4408,
      "step": 1543
    },
    {
      "epoch": 0.04041391104053606,
      "grad_norm": 16.354564666748047,
      "learning_rate": 7.154148128762104e-06,
      "loss": 0.5497,
      "step": 1544
    },
    {
      "epoch": 0.04044008585338615,
      "grad_norm": 17.887359619140625,
      "learning_rate": 7.151007589636221e-06,
      "loss": 0.3604,
      "step": 1545
    },
    {
      "epoch": 0.04046626066623624,
      "grad_norm": 24.26658058166504,
      "learning_rate": 7.147867050510338e-06,
      "loss": 0.6854,
      "step": 1546
    },
    {
      "epoch": 0.040492435479086326,
      "grad_norm": 26.819042205810547,
      "learning_rate": 7.144726511384454e-06,
      "loss": 0.737,
      "step": 1547
    },
    {
      "epoch": 0.040518610291936416,
      "grad_norm": 22.899566650390625,
      "learning_rate": 7.141585972258571e-06,
      "loss": 0.7999,
      "step": 1548
    },
    {
      "epoch": 0.0405447851047865,
      "grad_norm": 16.340356826782227,
      "learning_rate": 7.138445433132688e-06,
      "loss": 0.3367,
      "step": 1549
    },
    {
      "epoch": 0.04057095991763659,
      "grad_norm": 26.436843872070312,
      "learning_rate": 7.1353048940068045e-06,
      "loss": 0.552,
      "step": 1550
    },
    {
      "epoch": 0.04059713473048668,
      "grad_norm": 26.401174545288086,
      "learning_rate": 7.132164354880922e-06,
      "loss": 0.4909,
      "step": 1551
    },
    {
      "epoch": 0.04062330954333677,
      "grad_norm": 19.42303466796875,
      "learning_rate": 7.129023815755039e-06,
      "loss": 0.529,
      "step": 1552
    },
    {
      "epoch": 0.04064948435618685,
      "grad_norm": 14.917716026306152,
      "learning_rate": 7.1258832766291555e-06,
      "loss": 0.3057,
      "step": 1553
    },
    {
      "epoch": 0.04067565916903694,
      "grad_norm": 17.406084060668945,
      "learning_rate": 7.122742737503272e-06,
      "loss": 0.5126,
      "step": 1554
    },
    {
      "epoch": 0.04070183398188703,
      "grad_norm": 20.506393432617188,
      "learning_rate": 7.119602198377389e-06,
      "loss": 0.6362,
      "step": 1555
    },
    {
      "epoch": 0.04072800879473712,
      "grad_norm": 19.33434295654297,
      "learning_rate": 7.116461659251505e-06,
      "loss": 0.3688,
      "step": 1556
    },
    {
      "epoch": 0.04075418360758721,
      "grad_norm": 18.103010177612305,
      "learning_rate": 7.1133211201256214e-06,
      "loss": 0.7706,
      "step": 1557
    },
    {
      "epoch": 0.04078035842043729,
      "grad_norm": 18.411474227905273,
      "learning_rate": 7.110180580999738e-06,
      "loss": 0.8046,
      "step": 1558
    },
    {
      "epoch": 0.04080653323328738,
      "grad_norm": 22.779672622680664,
      "learning_rate": 7.107040041873855e-06,
      "loss": 0.5869,
      "step": 1559
    },
    {
      "epoch": 0.04083270804613747,
      "grad_norm": 17.125019073486328,
      "learning_rate": 7.1038995027479716e-06,
      "loss": 0.5268,
      "step": 1560
    },
    {
      "epoch": 0.04085888285898756,
      "grad_norm": 21.77474594116211,
      "learning_rate": 7.100758963622088e-06,
      "loss": 1.4906,
      "step": 1561
    },
    {
      "epoch": 0.040885057671837644,
      "grad_norm": 16.19542121887207,
      "learning_rate": 7.097618424496205e-06,
      "loss": 0.6025,
      "step": 1562
    },
    {
      "epoch": 0.04091123248468773,
      "grad_norm": 13.3380708694458,
      "learning_rate": 7.094477885370322e-06,
      "loss": 0.278,
      "step": 1563
    },
    {
      "epoch": 0.04093740729753782,
      "grad_norm": 24.506296157836914,
      "learning_rate": 7.091337346244439e-06,
      "loss": 1.184,
      "step": 1564
    },
    {
      "epoch": 0.04096358211038791,
      "grad_norm": 21.887365341186523,
      "learning_rate": 7.088196807118556e-06,
      "loss": 0.5315,
      "step": 1565
    },
    {
      "epoch": 0.040989756923238,
      "grad_norm": 16.260459899902344,
      "learning_rate": 7.085056267992673e-06,
      "loss": 0.34,
      "step": 1566
    },
    {
      "epoch": 0.041015931736088085,
      "grad_norm": 13.498217582702637,
      "learning_rate": 7.081915728866789e-06,
      "loss": 0.4517,
      "step": 1567
    },
    {
      "epoch": 0.041042106548938175,
      "grad_norm": 35.78712844848633,
      "learning_rate": 7.078775189740906e-06,
      "loss": 0.672,
      "step": 1568
    },
    {
      "epoch": 0.041068281361788264,
      "grad_norm": 26.025020599365234,
      "learning_rate": 7.075634650615023e-06,
      "loss": 0.5432,
      "step": 1569
    },
    {
      "epoch": 0.041094456174638354,
      "grad_norm": 19.246278762817383,
      "learning_rate": 7.0724941114891395e-06,
      "loss": 0.4049,
      "step": 1570
    },
    {
      "epoch": 0.04112063098748844,
      "grad_norm": 50.2352409362793,
      "learning_rate": 7.069353572363256e-06,
      "loss": 0.7072,
      "step": 1571
    },
    {
      "epoch": 0.041146805800338526,
      "grad_norm": 23.259685516357422,
      "learning_rate": 7.066213033237372e-06,
      "loss": 0.7076,
      "step": 1572
    },
    {
      "epoch": 0.041172980613188616,
      "grad_norm": 24.470945358276367,
      "learning_rate": 7.063072494111489e-06,
      "loss": 0.7352,
      "step": 1573
    },
    {
      "epoch": 0.041199155426038706,
      "grad_norm": 17.035778045654297,
      "learning_rate": 7.0599319549856054e-06,
      "loss": 0.5027,
      "step": 1574
    },
    {
      "epoch": 0.041225330238888795,
      "grad_norm": 37.40512466430664,
      "learning_rate": 7.056791415859722e-06,
      "loss": 0.6441,
      "step": 1575
    },
    {
      "epoch": 0.04125150505173888,
      "grad_norm": 23.68398094177246,
      "learning_rate": 7.053650876733839e-06,
      "loss": 0.3673,
      "step": 1576
    },
    {
      "epoch": 0.04127767986458897,
      "grad_norm": 20.751331329345703,
      "learning_rate": 7.050510337607956e-06,
      "loss": 0.7636,
      "step": 1577
    },
    {
      "epoch": 0.04130385467743906,
      "grad_norm": 24.84385108947754,
      "learning_rate": 7.047369798482073e-06,
      "loss": 0.5494,
      "step": 1578
    },
    {
      "epoch": 0.04133002949028915,
      "grad_norm": 18.96379280090332,
      "learning_rate": 7.04422925935619e-06,
      "loss": 0.644,
      "step": 1579
    },
    {
      "epoch": 0.04135620430313923,
      "grad_norm": 22.08588218688965,
      "learning_rate": 7.0410887202303065e-06,
      "loss": 0.6097,
      "step": 1580
    },
    {
      "epoch": 0.04138237911598932,
      "grad_norm": 13.562819480895996,
      "learning_rate": 7.037948181104423e-06,
      "loss": 0.3529,
      "step": 1581
    },
    {
      "epoch": 0.04140855392883941,
      "grad_norm": 11.147430419921875,
      "learning_rate": 7.03480764197854e-06,
      "loss": 0.2779,
      "step": 1582
    },
    {
      "epoch": 0.0414347287416895,
      "grad_norm": 19.585811614990234,
      "learning_rate": 7.031667102852657e-06,
      "loss": 0.669,
      "step": 1583
    },
    {
      "epoch": 0.04146090355453959,
      "grad_norm": 18.94293785095215,
      "learning_rate": 7.028526563726773e-06,
      "loss": 0.5527,
      "step": 1584
    },
    {
      "epoch": 0.04148707836738967,
      "grad_norm": 17.71603012084961,
      "learning_rate": 7.025386024600891e-06,
      "loss": 0.3222,
      "step": 1585
    },
    {
      "epoch": 0.04151325318023976,
      "grad_norm": 24.929441452026367,
      "learning_rate": 7.022245485475008e-06,
      "loss": 0.6467,
      "step": 1586
    },
    {
      "epoch": 0.04153942799308985,
      "grad_norm": 24.290382385253906,
      "learning_rate": 7.019104946349123e-06,
      "loss": 0.5777,
      "step": 1587
    },
    {
      "epoch": 0.04156560280593994,
      "grad_norm": 20.127899169921875,
      "learning_rate": 7.015964407223239e-06,
      "loss": 0.4984,
      "step": 1588
    },
    {
      "epoch": 0.04159177761879002,
      "grad_norm": 14.831968307495117,
      "learning_rate": 7.012823868097356e-06,
      "loss": 0.2991,
      "step": 1589
    },
    {
      "epoch": 0.04161795243164011,
      "grad_norm": 41.57707214355469,
      "learning_rate": 7.009683328971474e-06,
      "loss": 0.6375,
      "step": 1590
    },
    {
      "epoch": 0.0416441272444902,
      "grad_norm": 33.108280181884766,
      "learning_rate": 7.00654278984559e-06,
      "loss": 0.7877,
      "step": 1591
    },
    {
      "epoch": 0.04167030205734029,
      "grad_norm": 13.50328254699707,
      "learning_rate": 7.003402250719707e-06,
      "loss": 0.3753,
      "step": 1592
    },
    {
      "epoch": 0.04169647687019038,
      "grad_norm": 25.52310562133789,
      "learning_rate": 7.000261711593824e-06,
      "loss": 0.2816,
      "step": 1593
    },
    {
      "epoch": 0.041722651683040464,
      "grad_norm": 14.572945594787598,
      "learning_rate": 6.99712117246794e-06,
      "loss": 0.3113,
      "step": 1594
    },
    {
      "epoch": 0.041748826495890554,
      "grad_norm": 27.59979248046875,
      "learning_rate": 6.993980633342057e-06,
      "loss": 0.9665,
      "step": 1595
    },
    {
      "epoch": 0.04177500130874064,
      "grad_norm": 14.75218677520752,
      "learning_rate": 6.990840094216174e-06,
      "loss": 0.3952,
      "step": 1596
    },
    {
      "epoch": 0.04180117612159073,
      "grad_norm": 18.661041259765625,
      "learning_rate": 6.9876995550902905e-06,
      "loss": 0.4587,
      "step": 1597
    },
    {
      "epoch": 0.041827350934440816,
      "grad_norm": 24.291776657104492,
      "learning_rate": 6.984559015964408e-06,
      "loss": 0.5395,
      "step": 1598
    },
    {
      "epoch": 0.041853525747290905,
      "grad_norm": 19.072357177734375,
      "learning_rate": 6.981418476838525e-06,
      "loss": 0.6265,
      "step": 1599
    },
    {
      "epoch": 0.041879700560140995,
      "grad_norm": 17.399372100830078,
      "learning_rate": 6.9782779377126415e-06,
      "loss": 0.3327,
      "step": 1600
    },
    {
      "epoch": 0.041905875372991085,
      "grad_norm": 16.511123657226562,
      "learning_rate": 6.975137398586758e-06,
      "loss": 0.4353,
      "step": 1601
    },
    {
      "epoch": 0.041932050185841174,
      "grad_norm": 25.537254333496094,
      "learning_rate": 6.971996859460875e-06,
      "loss": 0.7863,
      "step": 1602
    },
    {
      "epoch": 0.04195822499869126,
      "grad_norm": 24.744041442871094,
      "learning_rate": 6.968856320334991e-06,
      "loss": 0.7921,
      "step": 1603
    },
    {
      "epoch": 0.04198439981154135,
      "grad_norm": 12.644044876098633,
      "learning_rate": 6.9657157812091075e-06,
      "loss": 0.2631,
      "step": 1604
    },
    {
      "epoch": 0.042010574624391436,
      "grad_norm": 14.11544132232666,
      "learning_rate": 6.962575242083224e-06,
      "loss": 0.3785,
      "step": 1605
    },
    {
      "epoch": 0.042036749437241526,
      "grad_norm": 16.891685485839844,
      "learning_rate": 6.959434702957341e-06,
      "loss": 0.426,
      "step": 1606
    },
    {
      "epoch": 0.04206292425009161,
      "grad_norm": 16.004175186157227,
      "learning_rate": 6.956294163831458e-06,
      "loss": 0.312,
      "step": 1607
    },
    {
      "epoch": 0.0420890990629417,
      "grad_norm": 20.601465225219727,
      "learning_rate": 6.953153624705574e-06,
      "loss": 0.5532,
      "step": 1608
    },
    {
      "epoch": 0.04211527387579179,
      "grad_norm": 24.50422477722168,
      "learning_rate": 6.950013085579691e-06,
      "loss": 0.6064,
      "step": 1609
    },
    {
      "epoch": 0.04214144868864188,
      "grad_norm": 16.972768783569336,
      "learning_rate": 6.946872546453808e-06,
      "loss": 0.6054,
      "step": 1610
    },
    {
      "epoch": 0.04216762350149197,
      "grad_norm": 23.216629028320312,
      "learning_rate": 6.943732007327925e-06,
      "loss": 0.369,
      "step": 1611
    },
    {
      "epoch": 0.04219379831434205,
      "grad_norm": 23.014625549316406,
      "learning_rate": 6.940591468202042e-06,
      "loss": 0.4893,
      "step": 1612
    },
    {
      "epoch": 0.04221997312719214,
      "grad_norm": 26.30803108215332,
      "learning_rate": 6.937450929076159e-06,
      "loss": 0.7322,
      "step": 1613
    },
    {
      "epoch": 0.04224614794004223,
      "grad_norm": 22.46484375,
      "learning_rate": 6.934310389950275e-06,
      "loss": 0.6556,
      "step": 1614
    },
    {
      "epoch": 0.04227232275289232,
      "grad_norm": 14.025008201599121,
      "learning_rate": 6.931169850824392e-06,
      "loss": 0.4295,
      "step": 1615
    },
    {
      "epoch": 0.0422984975657424,
      "grad_norm": 16.0169677734375,
      "learning_rate": 6.928029311698509e-06,
      "loss": 0.5473,
      "step": 1616
    },
    {
      "epoch": 0.04232467237859249,
      "grad_norm": 18.321598052978516,
      "learning_rate": 6.9248887725726255e-06,
      "loss": 0.7,
      "step": 1617
    },
    {
      "epoch": 0.04235084719144258,
      "grad_norm": 29.60526466369629,
      "learning_rate": 6.921748233446741e-06,
      "loss": 0.4783,
      "step": 1618
    },
    {
      "epoch": 0.04237702200429267,
      "grad_norm": 23.54730224609375,
      "learning_rate": 6.918607694320858e-06,
      "loss": 0.3803,
      "step": 1619
    },
    {
      "epoch": 0.04240319681714276,
      "grad_norm": 15.319228172302246,
      "learning_rate": 6.915467155194975e-06,
      "loss": 0.6878,
      "step": 1620
    },
    {
      "epoch": 0.04242937162999284,
      "grad_norm": 18.52412223815918,
      "learning_rate": 6.9123266160690915e-06,
      "loss": 0.4184,
      "step": 1621
    },
    {
      "epoch": 0.04245554644284293,
      "grad_norm": 13.506760597229004,
      "learning_rate": 6.909186076943208e-06,
      "loss": 0.3843,
      "step": 1622
    },
    {
      "epoch": 0.04248172125569302,
      "grad_norm": 21.843969345092773,
      "learning_rate": 6.906045537817325e-06,
      "loss": 0.7068,
      "step": 1623
    },
    {
      "epoch": 0.04250789606854311,
      "grad_norm": 12.779033660888672,
      "learning_rate": 6.9029049986914425e-06,
      "loss": 0.5424,
      "step": 1624
    },
    {
      "epoch": 0.042534070881393195,
      "grad_norm": 16.891822814941406,
      "learning_rate": 6.899764459565559e-06,
      "loss": 0.5771,
      "step": 1625
    },
    {
      "epoch": 0.042560245694243284,
      "grad_norm": 12.203009605407715,
      "learning_rate": 6.896623920439676e-06,
      "loss": 0.2686,
      "step": 1626
    },
    {
      "epoch": 0.042586420507093374,
      "grad_norm": 18.242231369018555,
      "learning_rate": 6.8934833813137926e-06,
      "loss": 0.5773,
      "step": 1627
    },
    {
      "epoch": 0.042612595319943464,
      "grad_norm": 19.35545539855957,
      "learning_rate": 6.890342842187909e-06,
      "loss": 0.4991,
      "step": 1628
    },
    {
      "epoch": 0.04263877013279355,
      "grad_norm": 22.183658599853516,
      "learning_rate": 6.887202303062026e-06,
      "loss": 0.6799,
      "step": 1629
    },
    {
      "epoch": 0.042664944945643636,
      "grad_norm": 29.17975425720215,
      "learning_rate": 6.884061763936143e-06,
      "loss": 0.6933,
      "step": 1630
    },
    {
      "epoch": 0.042691119758493726,
      "grad_norm": 18.433395385742188,
      "learning_rate": 6.880921224810259e-06,
      "loss": 0.7572,
      "step": 1631
    },
    {
      "epoch": 0.042717294571343815,
      "grad_norm": 24.27157211303711,
      "learning_rate": 6.877780685684377e-06,
      "loss": 0.7872,
      "step": 1632
    },
    {
      "epoch": 0.042743469384193905,
      "grad_norm": 14.453280448913574,
      "learning_rate": 6.874640146558494e-06,
      "loss": 0.4774,
      "step": 1633
    },
    {
      "epoch": 0.04276964419704399,
      "grad_norm": 30.061294555664062,
      "learning_rate": 6.871499607432609e-06,
      "loss": 0.4781,
      "step": 1634
    },
    {
      "epoch": 0.04279581900989408,
      "grad_norm": 41.66142654418945,
      "learning_rate": 6.868359068306725e-06,
      "loss": 0.5644,
      "step": 1635
    },
    {
      "epoch": 0.04282199382274417,
      "grad_norm": 28.123624801635742,
      "learning_rate": 6.865218529180842e-06,
      "loss": 0.6836,
      "step": 1636
    },
    {
      "epoch": 0.04284816863559426,
      "grad_norm": 22.478628158569336,
      "learning_rate": 6.86207799005496e-06,
      "loss": 0.6129,
      "step": 1637
    },
    {
      "epoch": 0.042874343448444346,
      "grad_norm": 22.82772445678711,
      "learning_rate": 6.858937450929076e-06,
      "loss": 0.5679,
      "step": 1638
    },
    {
      "epoch": 0.04290051826129443,
      "grad_norm": 16.3467960357666,
      "learning_rate": 6.855796911803193e-06,
      "loss": 0.6493,
      "step": 1639
    },
    {
      "epoch": 0.04292669307414452,
      "grad_norm": 19.201677322387695,
      "learning_rate": 6.85265637267731e-06,
      "loss": 0.3877,
      "step": 1640
    },
    {
      "epoch": 0.04295286788699461,
      "grad_norm": 17.967382431030273,
      "learning_rate": 6.8495158335514265e-06,
      "loss": 0.5378,
      "step": 1641
    },
    {
      "epoch": 0.0429790426998447,
      "grad_norm": 19.262195587158203,
      "learning_rate": 6.846375294425543e-06,
      "loss": 0.3812,
      "step": 1642
    },
    {
      "epoch": 0.04300521751269478,
      "grad_norm": 21.059282302856445,
      "learning_rate": 6.84323475529966e-06,
      "loss": 0.5209,
      "step": 1643
    },
    {
      "epoch": 0.04303139232554487,
      "grad_norm": 16.5162410736084,
      "learning_rate": 6.840094216173777e-06,
      "loss": 0.5783,
      "step": 1644
    },
    {
      "epoch": 0.04305756713839496,
      "grad_norm": 17.10845184326172,
      "learning_rate": 6.836953677047894e-06,
      "loss": 0.5538,
      "step": 1645
    },
    {
      "epoch": 0.04308374195124505,
      "grad_norm": 15.206286430358887,
      "learning_rate": 6.833813137922011e-06,
      "loss": 0.3637,
      "step": 1646
    },
    {
      "epoch": 0.04310991676409514,
      "grad_norm": 14.547956466674805,
      "learning_rate": 6.8306725987961275e-06,
      "loss": 0.3945,
      "step": 1647
    },
    {
      "epoch": 0.04313609157694522,
      "grad_norm": 13.490609169006348,
      "learning_rate": 6.827532059670244e-06,
      "loss": 0.3846,
      "step": 1648
    },
    {
      "epoch": 0.04316226638979531,
      "grad_norm": 21.559545516967773,
      "learning_rate": 6.824391520544359e-06,
      "loss": 0.5798,
      "step": 1649
    },
    {
      "epoch": 0.0431884412026454,
      "grad_norm": 22.716554641723633,
      "learning_rate": 6.821250981418477e-06,
      "loss": 0.8034,
      "step": 1650
    },
    {
      "epoch": 0.04321461601549549,
      "grad_norm": 22.185558319091797,
      "learning_rate": 6.8181104422925935e-06,
      "loss": 0.6648,
      "step": 1651
    },
    {
      "epoch": 0.04324079082834558,
      "grad_norm": 22.03158187866211,
      "learning_rate": 6.81496990316671e-06,
      "loss": 0.5414,
      "step": 1652
    },
    {
      "epoch": 0.043266965641195664,
      "grad_norm": 15.67612075805664,
      "learning_rate": 6.811829364040827e-06,
      "loss": 0.4298,
      "step": 1653
    },
    {
      "epoch": 0.04329314045404575,
      "grad_norm": 15.116412162780762,
      "learning_rate": 6.808688824914944e-06,
      "loss": 0.3484,
      "step": 1654
    },
    {
      "epoch": 0.04331931526689584,
      "grad_norm": 18.404340744018555,
      "learning_rate": 6.80554828578906e-06,
      "loss": 0.4363,
      "step": 1655
    },
    {
      "epoch": 0.04334549007974593,
      "grad_norm": 17.632648468017578,
      "learning_rate": 6.802407746663177e-06,
      "loss": 0.4503,
      "step": 1656
    },
    {
      "epoch": 0.043371664892596015,
      "grad_norm": 17.336153030395508,
      "learning_rate": 6.799267207537294e-06,
      "loss": 0.539,
      "step": 1657
    },
    {
      "epoch": 0.043397839705446105,
      "grad_norm": 21.730690002441406,
      "learning_rate": 6.796126668411411e-06,
      "loss": 0.8876,
      "step": 1658
    },
    {
      "epoch": 0.043424014518296195,
      "grad_norm": 11.551973342895508,
      "learning_rate": 6.792986129285528e-06,
      "loss": 0.352,
      "step": 1659
    },
    {
      "epoch": 0.043450189331146284,
      "grad_norm": 20.080446243286133,
      "learning_rate": 6.789845590159645e-06,
      "loss": 0.496,
      "step": 1660
    },
    {
      "epoch": 0.043476364143996374,
      "grad_norm": 22.795211791992188,
      "learning_rate": 6.7867050510337614e-06,
      "loss": 0.444,
      "step": 1661
    },
    {
      "epoch": 0.04350253895684646,
      "grad_norm": 19.992345809936523,
      "learning_rate": 6.783564511907878e-06,
      "loss": 0.552,
      "step": 1662
    },
    {
      "epoch": 0.043528713769696546,
      "grad_norm": 19.50979232788086,
      "learning_rate": 6.780423972781995e-06,
      "loss": 0.4497,
      "step": 1663
    },
    {
      "epoch": 0.043554888582546636,
      "grad_norm": 19.60716438293457,
      "learning_rate": 6.777283433656111e-06,
      "loss": 0.7922,
      "step": 1664
    },
    {
      "epoch": 0.043581063395396726,
      "grad_norm": 17.300325393676758,
      "learning_rate": 6.774142894530227e-06,
      "loss": 0.4319,
      "step": 1665
    },
    {
      "epoch": 0.04360723820824681,
      "grad_norm": 17.776199340820312,
      "learning_rate": 6.771002355404344e-06,
      "loss": 0.3575,
      "step": 1666
    },
    {
      "epoch": 0.0436334130210969,
      "grad_norm": 17.03463363647461,
      "learning_rate": 6.767861816278461e-06,
      "loss": 0.4289,
      "step": 1667
    },
    {
      "epoch": 0.04365958783394699,
      "grad_norm": 14.031790733337402,
      "learning_rate": 6.7647212771525775e-06,
      "loss": 0.2539,
      "step": 1668
    },
    {
      "epoch": 0.04368576264679708,
      "grad_norm": 19.68433952331543,
      "learning_rate": 6.761580738026694e-06,
      "loss": 0.6045,
      "step": 1669
    },
    {
      "epoch": 0.04371193745964717,
      "grad_norm": 16.460479736328125,
      "learning_rate": 6.758440198900811e-06,
      "loss": 0.6343,
      "step": 1670
    },
    {
      "epoch": 0.04373811227249725,
      "grad_norm": 16.76729965209961,
      "learning_rate": 6.7552996597749285e-06,
      "loss": 0.3374,
      "step": 1671
    },
    {
      "epoch": 0.04376428708534734,
      "grad_norm": 15.69357681274414,
      "learning_rate": 6.752159120649045e-06,
      "loss": 0.3822,
      "step": 1672
    },
    {
      "epoch": 0.04379046189819743,
      "grad_norm": 22.493928909301758,
      "learning_rate": 6.749018581523162e-06,
      "loss": 0.5167,
      "step": 1673
    },
    {
      "epoch": 0.04381663671104752,
      "grad_norm": 12.543985366821289,
      "learning_rate": 6.745878042397279e-06,
      "loss": 0.2753,
      "step": 1674
    },
    {
      "epoch": 0.0438428115238976,
      "grad_norm": 29.070297241210938,
      "learning_rate": 6.742737503271395e-06,
      "loss": 0.5518,
      "step": 1675
    },
    {
      "epoch": 0.04386898633674769,
      "grad_norm": 21.78032875061035,
      "learning_rate": 6.739596964145512e-06,
      "loss": 0.3869,
      "step": 1676
    },
    {
      "epoch": 0.04389516114959778,
      "grad_norm": 17.2291202545166,
      "learning_rate": 6.736456425019629e-06,
      "loss": 0.4415,
      "step": 1677
    },
    {
      "epoch": 0.04392133596244787,
      "grad_norm": 16.686180114746094,
      "learning_rate": 6.7333158858937454e-06,
      "loss": 0.538,
      "step": 1678
    },
    {
      "epoch": 0.04394751077529796,
      "grad_norm": 42.567752838134766,
      "learning_rate": 6.730175346767863e-06,
      "loss": 0.6577,
      "step": 1679
    },
    {
      "epoch": 0.04397368558814804,
      "grad_norm": 17.721986770629883,
      "learning_rate": 6.727034807641978e-06,
      "loss": 0.4442,
      "step": 1680
    },
    {
      "epoch": 0.04399986040099813,
      "grad_norm": 16.05643081665039,
      "learning_rate": 6.723894268516095e-06,
      "loss": 0.3927,
      "step": 1681
    },
    {
      "epoch": 0.04402603521384822,
      "grad_norm": 11.313304901123047,
      "learning_rate": 6.720753729390211e-06,
      "loss": 0.2289,
      "step": 1682
    },
    {
      "epoch": 0.04405221002669831,
      "grad_norm": 15.933414459228516,
      "learning_rate": 6.717613190264328e-06,
      "loss": 0.395,
      "step": 1683
    },
    {
      "epoch": 0.044078384839548394,
      "grad_norm": 13.822834014892578,
      "learning_rate": 6.714472651138446e-06,
      "loss": 0.3002,
      "step": 1684
    },
    {
      "epoch": 0.044104559652398484,
      "grad_norm": 23.21363639831543,
      "learning_rate": 6.711332112012562e-06,
      "loss": 0.5733,
      "step": 1685
    },
    {
      "epoch": 0.044130734465248574,
      "grad_norm": 17.761587142944336,
      "learning_rate": 6.708191572886679e-06,
      "loss": 0.4422,
      "step": 1686
    },
    {
      "epoch": 0.04415690927809866,
      "grad_norm": 15.780046463012695,
      "learning_rate": 6.705051033760796e-06,
      "loss": 0.4683,
      "step": 1687
    },
    {
      "epoch": 0.04418308409094875,
      "grad_norm": 18.512514114379883,
      "learning_rate": 6.7019104946349125e-06,
      "loss": 0.4455,
      "step": 1688
    },
    {
      "epoch": 0.044209258903798836,
      "grad_norm": 25.937116622924805,
      "learning_rate": 6.698769955509029e-06,
      "loss": 0.8904,
      "step": 1689
    },
    {
      "epoch": 0.044235433716648925,
      "grad_norm": 21.87374496459961,
      "learning_rate": 6.695629416383146e-06,
      "loss": 0.4242,
      "step": 1690
    },
    {
      "epoch": 0.044261608529499015,
      "grad_norm": 17.366445541381836,
      "learning_rate": 6.692488877257263e-06,
      "loss": 0.3369,
      "step": 1691
    },
    {
      "epoch": 0.044287783342349105,
      "grad_norm": 20.816694259643555,
      "learning_rate": 6.68934833813138e-06,
      "loss": 0.4593,
      "step": 1692
    },
    {
      "epoch": 0.04431395815519919,
      "grad_norm": 18.874021530151367,
      "learning_rate": 6.686207799005497e-06,
      "loss": 0.7509,
      "step": 1693
    },
    {
      "epoch": 0.04434013296804928,
      "grad_norm": 23.495283126831055,
      "learning_rate": 6.683067259879614e-06,
      "loss": 0.4257,
      "step": 1694
    },
    {
      "epoch": 0.04436630778089937,
      "grad_norm": 19.232725143432617,
      "learning_rate": 6.679926720753729e-06,
      "loss": 0.4981,
      "step": 1695
    },
    {
      "epoch": 0.044392482593749456,
      "grad_norm": 19.710052490234375,
      "learning_rate": 6.676786181627845e-06,
      "loss": 0.3513,
      "step": 1696
    },
    {
      "epoch": 0.044418657406599546,
      "grad_norm": 21.478679656982422,
      "learning_rate": 6.673645642501963e-06,
      "loss": 0.4448,
      "step": 1697
    },
    {
      "epoch": 0.04444483221944963,
      "grad_norm": 27.89900779724121,
      "learning_rate": 6.6705051033760796e-06,
      "loss": 0.3563,
      "step": 1698
    },
    {
      "epoch": 0.04447100703229972,
      "grad_norm": 12.505231857299805,
      "learning_rate": 6.667364564250196e-06,
      "loss": 0.2716,
      "step": 1699
    },
    {
      "epoch": 0.04449718184514981,
      "grad_norm": 22.65135955810547,
      "learning_rate": 6.664224025124313e-06,
      "loss": 0.5465,
      "step": 1700
    },
    {
      "epoch": 0.0445233566579999,
      "grad_norm": 14.045134544372559,
      "learning_rate": 6.66108348599843e-06,
      "loss": 0.2475,
      "step": 1701
    },
    {
      "epoch": 0.04454953147084998,
      "grad_norm": 18.442283630371094,
      "learning_rate": 6.657942946872546e-06,
      "loss": 0.352,
      "step": 1702
    },
    {
      "epoch": 0.04457570628370007,
      "grad_norm": 17.553909301757812,
      "learning_rate": 6.654802407746663e-06,
      "loss": 0.2374,
      "step": 1703
    },
    {
      "epoch": 0.04460188109655016,
      "grad_norm": 16.177038192749023,
      "learning_rate": 6.65166186862078e-06,
      "loss": 0.4838,
      "step": 1704
    },
    {
      "epoch": 0.04462805590940025,
      "grad_norm": 15.81316089630127,
      "learning_rate": 6.648521329494897e-06,
      "loss": 0.2373,
      "step": 1705
    },
    {
      "epoch": 0.04465423072225034,
      "grad_norm": 18.334447860717773,
      "learning_rate": 6.645380790369014e-06,
      "loss": 0.5192,
      "step": 1706
    },
    {
      "epoch": 0.04468040553510042,
      "grad_norm": 24.311220169067383,
      "learning_rate": 6.642240251243131e-06,
      "loss": 0.5958,
      "step": 1707
    },
    {
      "epoch": 0.04470658034795051,
      "grad_norm": 15.971857070922852,
      "learning_rate": 6.6390997121172475e-06,
      "loss": 0.39,
      "step": 1708
    },
    {
      "epoch": 0.0447327551608006,
      "grad_norm": 27.879444122314453,
      "learning_rate": 6.635959172991364e-06,
      "loss": 0.4427,
      "step": 1709
    },
    {
      "epoch": 0.04475892997365069,
      "grad_norm": 26.84519386291504,
      "learning_rate": 6.632818633865481e-06,
      "loss": 0.5916,
      "step": 1710
    },
    {
      "epoch": 0.04478510478650077,
      "grad_norm": 15.153976440429688,
      "learning_rate": 6.629678094739597e-06,
      "loss": 0.2291,
      "step": 1711
    },
    {
      "epoch": 0.04481127959935086,
      "grad_norm": 17.044496536254883,
      "learning_rate": 6.6265375556137134e-06,
      "loss": 0.3882,
      "step": 1712
    },
    {
      "epoch": 0.04483745441220095,
      "grad_norm": 21.49253273010254,
      "learning_rate": 6.62339701648783e-06,
      "loss": 0.3392,
      "step": 1713
    },
    {
      "epoch": 0.04486362922505104,
      "grad_norm": 17.5216007232666,
      "learning_rate": 6.620256477361947e-06,
      "loss": 0.7094,
      "step": 1714
    },
    {
      "epoch": 0.04488980403790113,
      "grad_norm": 20.515024185180664,
      "learning_rate": 6.6171159382360636e-06,
      "loss": 0.5833,
      "step": 1715
    },
    {
      "epoch": 0.044915978850751215,
      "grad_norm": 18.340322494506836,
      "learning_rate": 6.61397539911018e-06,
      "loss": 0.4833,
      "step": 1716
    },
    {
      "epoch": 0.044942153663601304,
      "grad_norm": 24.520559310913086,
      "learning_rate": 6.610834859984297e-06,
      "loss": 0.2965,
      "step": 1717
    },
    {
      "epoch": 0.044968328476451394,
      "grad_norm": 22.35999298095703,
      "learning_rate": 6.6076943208584145e-06,
      "loss": 0.4171,
      "step": 1718
    },
    {
      "epoch": 0.044994503289301484,
      "grad_norm": 13.21886157989502,
      "learning_rate": 6.604553781732531e-06,
      "loss": 0.3192,
      "step": 1719
    },
    {
      "epoch": 0.045020678102151566,
      "grad_norm": 22.987323760986328,
      "learning_rate": 6.601413242606648e-06,
      "loss": 0.3003,
      "step": 1720
    },
    {
      "epoch": 0.045046852915001656,
      "grad_norm": 19.725765228271484,
      "learning_rate": 6.598272703480765e-06,
      "loss": 0.4471,
      "step": 1721
    },
    {
      "epoch": 0.045073027727851746,
      "grad_norm": 18.468791961669922,
      "learning_rate": 6.595132164354881e-06,
      "loss": 0.513,
      "step": 1722
    },
    {
      "epoch": 0.045099202540701835,
      "grad_norm": 27.54574966430664,
      "learning_rate": 6.591991625228998e-06,
      "loss": 0.4296,
      "step": 1723
    },
    {
      "epoch": 0.045125377353551925,
      "grad_norm": 17.236135482788086,
      "learning_rate": 6.588851086103115e-06,
      "loss": 0.3207,
      "step": 1724
    },
    {
      "epoch": 0.04515155216640201,
      "grad_norm": 25.08324432373047,
      "learning_rate": 6.5857105469772315e-06,
      "loss": 0.3445,
      "step": 1725
    },
    {
      "epoch": 0.0451777269792521,
      "grad_norm": 27.531036376953125,
      "learning_rate": 6.582570007851347e-06,
      "loss": 0.5932,
      "step": 1726
    },
    {
      "epoch": 0.04520390179210219,
      "grad_norm": 19.761964797973633,
      "learning_rate": 6.579429468725464e-06,
      "loss": 0.2488,
      "step": 1727
    },
    {
      "epoch": 0.04523007660495228,
      "grad_norm": 27.60628890991211,
      "learning_rate": 6.576288929599581e-06,
      "loss": 0.537,
      "step": 1728
    },
    {
      "epoch": 0.04525625141780236,
      "grad_norm": 20.337360382080078,
      "learning_rate": 6.5731483904736975e-06,
      "loss": 0.4893,
      "step": 1729
    },
    {
      "epoch": 0.04528242623065245,
      "grad_norm": 19.424663543701172,
      "learning_rate": 6.570007851347814e-06,
      "loss": 0.285,
      "step": 1730
    },
    {
      "epoch": 0.04530860104350254,
      "grad_norm": 26.21988868713379,
      "learning_rate": 6.566867312221932e-06,
      "loss": 0.3595,
      "step": 1731
    },
    {
      "epoch": 0.04533477585635263,
      "grad_norm": 18.148439407348633,
      "learning_rate": 6.563726773096048e-06,
      "loss": 0.3714,
      "step": 1732
    },
    {
      "epoch": 0.04536095066920272,
      "grad_norm": 26.3016300201416,
      "learning_rate": 6.560586233970165e-06,
      "loss": 0.5748,
      "step": 1733
    },
    {
      "epoch": 0.0453871254820528,
      "grad_norm": 15.891339302062988,
      "learning_rate": 6.557445694844282e-06,
      "loss": 0.3781,
      "step": 1734
    },
    {
      "epoch": 0.04541330029490289,
      "grad_norm": 26.114225387573242,
      "learning_rate": 6.5543051557183985e-06,
      "loss": 0.4129,
      "step": 1735
    },
    {
      "epoch": 0.04543947510775298,
      "grad_norm": 28.872968673706055,
      "learning_rate": 6.551164616592515e-06,
      "loss": 0.4756,
      "step": 1736
    },
    {
      "epoch": 0.04546564992060307,
      "grad_norm": 19.525726318359375,
      "learning_rate": 6.548024077466632e-06,
      "loss": 0.1762,
      "step": 1737
    },
    {
      "epoch": 0.04549182473345315,
      "grad_norm": 9.863844871520996,
      "learning_rate": 6.544883538340749e-06,
      "loss": 0.1901,
      "step": 1738
    },
    {
      "epoch": 0.04551799954630324,
      "grad_norm": 26.41478729248047,
      "learning_rate": 6.541742999214866e-06,
      "loss": 0.829,
      "step": 1739
    },
    {
      "epoch": 0.04554417435915333,
      "grad_norm": 26.661487579345703,
      "learning_rate": 6.538602460088983e-06,
      "loss": 0.415,
      "step": 1740
    },
    {
      "epoch": 0.04557034917200342,
      "grad_norm": 13.486980438232422,
      "learning_rate": 6.5354619209631e-06,
      "loss": 0.382,
      "step": 1741
    },
    {
      "epoch": 0.04559652398485351,
      "grad_norm": 19.94925308227539,
      "learning_rate": 6.532321381837215e-06,
      "loss": 0.7341,
      "step": 1742
    },
    {
      "epoch": 0.045622698797703594,
      "grad_norm": 32.04853057861328,
      "learning_rate": 6.529180842711331e-06,
      "loss": 0.6424,
      "step": 1743
    },
    {
      "epoch": 0.045648873610553684,
      "grad_norm": 9.677535057067871,
      "learning_rate": 6.526040303585449e-06,
      "loss": 0.2679,
      "step": 1744
    },
    {
      "epoch": 0.04567504842340377,
      "grad_norm": 17.988576889038086,
      "learning_rate": 6.522899764459566e-06,
      "loss": 0.5187,
      "step": 1745
    },
    {
      "epoch": 0.04570122323625386,
      "grad_norm": 12.195405006408691,
      "learning_rate": 6.519759225333682e-06,
      "loss": 0.4959,
      "step": 1746
    },
    {
      "epoch": 0.045727398049103946,
      "grad_norm": 29.976871490478516,
      "learning_rate": 6.516618686207799e-06,
      "loss": 0.6346,
      "step": 1747
    },
    {
      "epoch": 0.045753572861954035,
      "grad_norm": 28.849769592285156,
      "learning_rate": 6.513478147081916e-06,
      "loss": 0.5416,
      "step": 1748
    },
    {
      "epoch": 0.045779747674804125,
      "grad_norm": 34.53510284423828,
      "learning_rate": 6.5103376079560324e-06,
      "loss": 0.701,
      "step": 1749
    },
    {
      "epoch": 0.045805922487654215,
      "grad_norm": 41.515316009521484,
      "learning_rate": 6.507197068830149e-06,
      "loss": 0.7443,
      "step": 1750
    },
    {
      "epoch": 0.045832097300504304,
      "grad_norm": 16.461427688598633,
      "learning_rate": 6.504056529704266e-06,
      "loss": 0.3482,
      "step": 1751
    },
    {
      "epoch": 0.04585827211335439,
      "grad_norm": 28.787324905395508,
      "learning_rate": 6.500915990578383e-06,
      "loss": 0.8399,
      "step": 1752
    },
    {
      "epoch": 0.04588444692620448,
      "grad_norm": 32.85402297973633,
      "learning_rate": 6.4977754514525e-06,
      "loss": 0.7017,
      "step": 1753
    },
    {
      "epoch": 0.045910621739054566,
      "grad_norm": 18.567480087280273,
      "learning_rate": 6.494634912326617e-06,
      "loss": 0.5477,
      "step": 1754
    },
    {
      "epoch": 0.045936796551904656,
      "grad_norm": 20.300264358520508,
      "learning_rate": 6.4914943732007335e-06,
      "loss": 0.5715,
      "step": 1755
    },
    {
      "epoch": 0.04596297136475474,
      "grad_norm": 20.508020401000977,
      "learning_rate": 6.48835383407485e-06,
      "loss": 0.4999,
      "step": 1756
    },
    {
      "epoch": 0.04598914617760483,
      "grad_norm": 19.049518585205078,
      "learning_rate": 6.485213294948966e-06,
      "loss": 0.438,
      "step": 1757
    },
    {
      "epoch": 0.04601532099045492,
      "grad_norm": 20.815380096435547,
      "learning_rate": 6.482072755823083e-06,
      "loss": 0.4737,
      "step": 1758
    },
    {
      "epoch": 0.04604149580330501,
      "grad_norm": 15.061354637145996,
      "learning_rate": 6.4789322166971995e-06,
      "loss": 0.6701,
      "step": 1759
    },
    {
      "epoch": 0.0460676706161551,
      "grad_norm": 28.88490104675293,
      "learning_rate": 6.475791677571316e-06,
      "loss": 0.6718,
      "step": 1760
    },
    {
      "epoch": 0.04609384542900518,
      "grad_norm": 16.75082015991211,
      "learning_rate": 6.472651138445433e-06,
      "loss": 0.3792,
      "step": 1761
    },
    {
      "epoch": 0.04612002024185527,
      "grad_norm": 20.554100036621094,
      "learning_rate": 6.46951059931955e-06,
      "loss": 0.4944,
      "step": 1762
    },
    {
      "epoch": 0.04614619505470536,
      "grad_norm": 20.556318283081055,
      "learning_rate": 6.466370060193666e-06,
      "loss": 0.6144,
      "step": 1763
    },
    {
      "epoch": 0.04617236986755545,
      "grad_norm": 18.216205596923828,
      "learning_rate": 6.463229521067783e-06,
      "loss": 0.5348,
      "step": 1764
    },
    {
      "epoch": 0.04619854468040553,
      "grad_norm": 17.813291549682617,
      "learning_rate": 6.4600889819419006e-06,
      "loss": 0.2763,
      "step": 1765
    },
    {
      "epoch": 0.04622471949325562,
      "grad_norm": 40.885986328125,
      "learning_rate": 6.456948442816017e-06,
      "loss": 0.8567,
      "step": 1766
    },
    {
      "epoch": 0.04625089430610571,
      "grad_norm": 15.528423309326172,
      "learning_rate": 6.453807903690134e-06,
      "loss": 0.3401,
      "step": 1767
    },
    {
      "epoch": 0.0462770691189558,
      "grad_norm": 21.32138442993164,
      "learning_rate": 6.450667364564251e-06,
      "loss": 0.7681,
      "step": 1768
    },
    {
      "epoch": 0.04630324393180589,
      "grad_norm": 26.177709579467773,
      "learning_rate": 6.447526825438367e-06,
      "loss": 0.5174,
      "step": 1769
    },
    {
      "epoch": 0.04632941874465597,
      "grad_norm": 18.40180015563965,
      "learning_rate": 6.444386286312484e-06,
      "loss": 0.3086,
      "step": 1770
    },
    {
      "epoch": 0.04635559355750606,
      "grad_norm": 29.29621124267578,
      "learning_rate": 6.441245747186601e-06,
      "loss": 0.5961,
      "step": 1771
    },
    {
      "epoch": 0.04638176837035615,
      "grad_norm": 23.477174758911133,
      "learning_rate": 6.4381052080607175e-06,
      "loss": 0.3461,
      "step": 1772
    },
    {
      "epoch": 0.04640794318320624,
      "grad_norm": 12.780303001403809,
      "learning_rate": 6.434964668934833e-06,
      "loss": 0.363,
      "step": 1773
    },
    {
      "epoch": 0.04643411799605633,
      "grad_norm": 19.009124755859375,
      "learning_rate": 6.43182412980895e-06,
      "loss": 0.223,
      "step": 1774
    },
    {
      "epoch": 0.046460292808906414,
      "grad_norm": 22.319992065429688,
      "learning_rate": 6.428683590683067e-06,
      "loss": 0.5023,
      "step": 1775
    },
    {
      "epoch": 0.046486467621756504,
      "grad_norm": 22.431541442871094,
      "learning_rate": 6.4255430515571835e-06,
      "loss": 0.6029,
      "step": 1776
    },
    {
      "epoch": 0.046512642434606594,
      "grad_norm": 17.202951431274414,
      "learning_rate": 6.4224025124313e-06,
      "loss": 0.5451,
      "step": 1777
    },
    {
      "epoch": 0.04653881724745668,
      "grad_norm": 25.560394287109375,
      "learning_rate": 6.419261973305418e-06,
      "loss": 0.6164,
      "step": 1778
    },
    {
      "epoch": 0.046564992060306766,
      "grad_norm": 17.311355590820312,
      "learning_rate": 6.4161214341795345e-06,
      "loss": 0.3657,
      "step": 1779
    },
    {
      "epoch": 0.046591166873156856,
      "grad_norm": 18.197486877441406,
      "learning_rate": 6.412980895053651e-06,
      "loss": 0.7238,
      "step": 1780
    },
    {
      "epoch": 0.046617341686006945,
      "grad_norm": 21.268207550048828,
      "learning_rate": 6.409840355927768e-06,
      "loss": 0.3738,
      "step": 1781
    },
    {
      "epoch": 0.046643516498857035,
      "grad_norm": 21.35114097595215,
      "learning_rate": 6.406699816801885e-06,
      "loss": 0.6853,
      "step": 1782
    },
    {
      "epoch": 0.046669691311707125,
      "grad_norm": 21.582212448120117,
      "learning_rate": 6.403559277676001e-06,
      "loss": 0.289,
      "step": 1783
    },
    {
      "epoch": 0.04669586612455721,
      "grad_norm": 10.24063491821289,
      "learning_rate": 6.400418738550118e-06,
      "loss": 0.2586,
      "step": 1784
    },
    {
      "epoch": 0.0467220409374073,
      "grad_norm": 19.553749084472656,
      "learning_rate": 6.397278199424235e-06,
      "loss": 0.3634,
      "step": 1785
    },
    {
      "epoch": 0.04674821575025739,
      "grad_norm": 20.234750747680664,
      "learning_rate": 6.394137660298352e-06,
      "loss": 0.5169,
      "step": 1786
    },
    {
      "epoch": 0.046774390563107476,
      "grad_norm": 18.00937271118164,
      "learning_rate": 6.390997121172469e-06,
      "loss": 0.2551,
      "step": 1787
    },
    {
      "epoch": 0.04680056537595756,
      "grad_norm": 27.53620147705078,
      "learning_rate": 6.387856582046584e-06,
      "loss": 0.4251,
      "step": 1788
    },
    {
      "epoch": 0.04682674018880765,
      "grad_norm": 20.54437828063965,
      "learning_rate": 6.384716042920701e-06,
      "loss": 0.3713,
      "step": 1789
    },
    {
      "epoch": 0.04685291500165774,
      "grad_norm": 18.622783660888672,
      "learning_rate": 6.381575503794817e-06,
      "loss": 0.3438,
      "step": 1790
    },
    {
      "epoch": 0.04687908981450783,
      "grad_norm": 17.350894927978516,
      "learning_rate": 6.378434964668935e-06,
      "loss": 0.5309,
      "step": 1791
    },
    {
      "epoch": 0.04690526462735792,
      "grad_norm": 22.68339729309082,
      "learning_rate": 6.375294425543052e-06,
      "loss": 0.6348,
      "step": 1792
    },
    {
      "epoch": 0.046931439440208,
      "grad_norm": 20.66924285888672,
      "learning_rate": 6.372153886417168e-06,
      "loss": 0.419,
      "step": 1793
    },
    {
      "epoch": 0.04695761425305809,
      "grad_norm": 22.286006927490234,
      "learning_rate": 6.369013347291285e-06,
      "loss": 0.3793,
      "step": 1794
    },
    {
      "epoch": 0.04698378906590818,
      "grad_norm": 26.354549407958984,
      "learning_rate": 6.365872808165402e-06,
      "loss": 0.3412,
      "step": 1795
    },
    {
      "epoch": 0.04700996387875827,
      "grad_norm": 29.538564682006836,
      "learning_rate": 6.3627322690395185e-06,
      "loss": 0.6,
      "step": 1796
    },
    {
      "epoch": 0.04703613869160835,
      "grad_norm": 13.749777793884277,
      "learning_rate": 6.359591729913635e-06,
      "loss": 0.3061,
      "step": 1797
    },
    {
      "epoch": 0.04706231350445844,
      "grad_norm": 26.346084594726562,
      "learning_rate": 6.356451190787752e-06,
      "loss": 0.8349,
      "step": 1798
    },
    {
      "epoch": 0.04708848831730853,
      "grad_norm": 21.264202117919922,
      "learning_rate": 6.3533106516618694e-06,
      "loss": 0.6284,
      "step": 1799
    },
    {
      "epoch": 0.04711466313015862,
      "grad_norm": 19.56787872314453,
      "learning_rate": 6.350170112535986e-06,
      "loss": 0.4608,
      "step": 1800
    },
    {
      "epoch": 0.04714083794300871,
      "grad_norm": 21.894548416137695,
      "learning_rate": 6.347029573410103e-06,
      "loss": 0.6758,
      "step": 1801
    },
    {
      "epoch": 0.04716701275585879,
      "grad_norm": 33.68335723876953,
      "learning_rate": 6.3438890342842196e-06,
      "loss": 0.5966,
      "step": 1802
    },
    {
      "epoch": 0.04719318756870888,
      "grad_norm": 23.5930233001709,
      "learning_rate": 6.340748495158336e-06,
      "loss": 0.4153,
      "step": 1803
    },
    {
      "epoch": 0.04721936238155897,
      "grad_norm": 25.791425704956055,
      "learning_rate": 6.337607956032452e-06,
      "loss": 0.595,
      "step": 1804
    },
    {
      "epoch": 0.04724553719440906,
      "grad_norm": 17.527841567993164,
      "learning_rate": 6.334467416906569e-06,
      "loss": 0.5436,
      "step": 1805
    },
    {
      "epoch": 0.047271712007259145,
      "grad_norm": 17.68760871887207,
      "learning_rate": 6.3313268777806855e-06,
      "loss": 0.2937,
      "step": 1806
    },
    {
      "epoch": 0.047297886820109235,
      "grad_norm": 18.121618270874023,
      "learning_rate": 6.328186338654802e-06,
      "loss": 0.3882,
      "step": 1807
    },
    {
      "epoch": 0.047324061632959324,
      "grad_norm": 27.594627380371094,
      "learning_rate": 6.325045799528919e-06,
      "loss": 0.7588,
      "step": 1808
    },
    {
      "epoch": 0.047350236445809414,
      "grad_norm": 19.87004280090332,
      "learning_rate": 6.321905260403036e-06,
      "loss": 0.3545,
      "step": 1809
    },
    {
      "epoch": 0.047376411258659504,
      "grad_norm": 23.849550247192383,
      "learning_rate": 6.318764721277152e-06,
      "loss": 0.4467,
      "step": 1810
    },
    {
      "epoch": 0.047402586071509586,
      "grad_norm": 18.41518211364746,
      "learning_rate": 6.315624182151269e-06,
      "loss": 0.2116,
      "step": 1811
    },
    {
      "epoch": 0.047428760884359676,
      "grad_norm": 29.568086624145508,
      "learning_rate": 6.312483643025387e-06,
      "loss": 0.8594,
      "step": 1812
    },
    {
      "epoch": 0.047454935697209766,
      "grad_norm": 18.056278228759766,
      "learning_rate": 6.309343103899503e-06,
      "loss": 0.5006,
      "step": 1813
    },
    {
      "epoch": 0.047481110510059855,
      "grad_norm": 30.499340057373047,
      "learning_rate": 6.30620256477362e-06,
      "loss": 0.5797,
      "step": 1814
    },
    {
      "epoch": 0.04750728532290994,
      "grad_norm": 16.621728897094727,
      "learning_rate": 6.303062025647737e-06,
      "loss": 0.325,
      "step": 1815
    },
    {
      "epoch": 0.04753346013576003,
      "grad_norm": 21.25823211669922,
      "learning_rate": 6.2999214865218534e-06,
      "loss": 0.552,
      "step": 1816
    },
    {
      "epoch": 0.04755963494861012,
      "grad_norm": 11.833663940429688,
      "learning_rate": 6.29678094739597e-06,
      "loss": 0.2772,
      "step": 1817
    },
    {
      "epoch": 0.04758580976146021,
      "grad_norm": 17.3842716217041,
      "learning_rate": 6.293640408270087e-06,
      "loss": 0.2846,
      "step": 1818
    },
    {
      "epoch": 0.0476119845743103,
      "grad_norm": 12.912117958068848,
      "learning_rate": 6.290499869144203e-06,
      "loss": 0.2078,
      "step": 1819
    },
    {
      "epoch": 0.04763815938716038,
      "grad_norm": 21.432798385620117,
      "learning_rate": 6.287359330018319e-06,
      "loss": 0.4319,
      "step": 1820
    },
    {
      "epoch": 0.04766433420001047,
      "grad_norm": 12.669456481933594,
      "learning_rate": 6.284218790892436e-06,
      "loss": 0.3721,
      "step": 1821
    },
    {
      "epoch": 0.04769050901286056,
      "grad_norm": 12.681093215942383,
      "learning_rate": 6.281078251766553e-06,
      "loss": 0.3959,
      "step": 1822
    },
    {
      "epoch": 0.04771668382571065,
      "grad_norm": 19.55695152282715,
      "learning_rate": 6.2779377126406695e-06,
      "loss": 0.5156,
      "step": 1823
    },
    {
      "epoch": 0.04774285863856073,
      "grad_norm": 14.966981887817383,
      "learning_rate": 6.274797173514786e-06,
      "loss": 0.4215,
      "step": 1824
    },
    {
      "epoch": 0.04776903345141082,
      "grad_norm": 14.616561889648438,
      "learning_rate": 6.271656634388904e-06,
      "loss": 0.3552,
      "step": 1825
    },
    {
      "epoch": 0.04779520826426091,
      "grad_norm": 18.904003143310547,
      "learning_rate": 6.2685160952630205e-06,
      "loss": 0.9028,
      "step": 1826
    },
    {
      "epoch": 0.047821383077111,
      "grad_norm": 25.196205139160156,
      "learning_rate": 6.265375556137137e-06,
      "loss": 0.6635,
      "step": 1827
    },
    {
      "epoch": 0.04784755788996109,
      "grad_norm": 26.524694442749023,
      "learning_rate": 6.262235017011254e-06,
      "loss": 0.6916,
      "step": 1828
    },
    {
      "epoch": 0.04787373270281117,
      "grad_norm": 16.92926025390625,
      "learning_rate": 6.259094477885371e-06,
      "loss": 0.4794,
      "step": 1829
    },
    {
      "epoch": 0.04789990751566126,
      "grad_norm": 20.66340446472168,
      "learning_rate": 6.255953938759487e-06,
      "loss": 0.6742,
      "step": 1830
    },
    {
      "epoch": 0.04792608232851135,
      "grad_norm": 16.326017379760742,
      "learning_rate": 6.252813399633604e-06,
      "loss": 0.3529,
      "step": 1831
    },
    {
      "epoch": 0.04795225714136144,
      "grad_norm": 21.74686050415039,
      "learning_rate": 6.249672860507721e-06,
      "loss": 0.4755,
      "step": 1832
    },
    {
      "epoch": 0.047978431954211524,
      "grad_norm": 20.26451301574707,
      "learning_rate": 6.246532321381838e-06,
      "loss": 0.4113,
      "step": 1833
    },
    {
      "epoch": 0.048004606767061614,
      "grad_norm": 19.881282806396484,
      "learning_rate": 6.243391782255955e-06,
      "loss": 0.4955,
      "step": 1834
    },
    {
      "epoch": 0.048030781579911704,
      "grad_norm": 29.852529525756836,
      "learning_rate": 6.24025124313007e-06,
      "loss": 0.3732,
      "step": 1835
    },
    {
      "epoch": 0.04805695639276179,
      "grad_norm": 34.76167678833008,
      "learning_rate": 6.237110704004187e-06,
      "loss": 0.8395,
      "step": 1836
    },
    {
      "epoch": 0.04808313120561188,
      "grad_norm": 30.08296012878418,
      "learning_rate": 6.233970164878304e-06,
      "loss": 0.5047,
      "step": 1837
    },
    {
      "epoch": 0.048109306018461966,
      "grad_norm": 23.906007766723633,
      "learning_rate": 6.230829625752421e-06,
      "loss": 0.5033,
      "step": 1838
    },
    {
      "epoch": 0.048135480831312055,
      "grad_norm": 15.087021827697754,
      "learning_rate": 6.227689086626538e-06,
      "loss": 0.3652,
      "step": 1839
    },
    {
      "epoch": 0.048161655644162145,
      "grad_norm": 19.94037437438965,
      "learning_rate": 6.224548547500654e-06,
      "loss": 0.6708,
      "step": 1840
    },
    {
      "epoch": 0.048187830457012235,
      "grad_norm": 14.821314811706543,
      "learning_rate": 6.221408008374771e-06,
      "loss": 0.3206,
      "step": 1841
    },
    {
      "epoch": 0.04821400526986232,
      "grad_norm": 13.727360725402832,
      "learning_rate": 6.218267469248888e-06,
      "loss": 0.3515,
      "step": 1842
    },
    {
      "epoch": 0.04824018008271241,
      "grad_norm": 19.614986419677734,
      "learning_rate": 6.2151269301230045e-06,
      "loss": 0.6392,
      "step": 1843
    },
    {
      "epoch": 0.0482663548955625,
      "grad_norm": 12.120353698730469,
      "learning_rate": 6.211986390997121e-06,
      "loss": 0.1543,
      "step": 1844
    },
    {
      "epoch": 0.048292529708412586,
      "grad_norm": 26.940898895263672,
      "learning_rate": 6.208845851871238e-06,
      "loss": 0.4013,
      "step": 1845
    },
    {
      "epoch": 0.048318704521262676,
      "grad_norm": 18.462343215942383,
      "learning_rate": 6.2057053127453555e-06,
      "loss": 0.3417,
      "step": 1846
    },
    {
      "epoch": 0.04834487933411276,
      "grad_norm": 23.768470764160156,
      "learning_rate": 6.202564773619472e-06,
      "loss": 0.59,
      "step": 1847
    },
    {
      "epoch": 0.04837105414696285,
      "grad_norm": 16.164400100708008,
      "learning_rate": 6.199424234493589e-06,
      "loss": 0.3434,
      "step": 1848
    },
    {
      "epoch": 0.04839722895981294,
      "grad_norm": 13.516417503356934,
      "learning_rate": 6.196283695367706e-06,
      "loss": 0.2981,
      "step": 1849
    },
    {
      "epoch": 0.04842340377266303,
      "grad_norm": 12.776290893554688,
      "learning_rate": 6.1931431562418214e-06,
      "loss": 0.367,
      "step": 1850
    },
    {
      "epoch": 0.04844957858551311,
      "grad_norm": 21.78403091430664,
      "learning_rate": 6.190002617115938e-06,
      "loss": 0.4291,
      "step": 1851
    },
    {
      "epoch": 0.0484757533983632,
      "grad_norm": 13.928585052490234,
      "learning_rate": 6.186862077990055e-06,
      "loss": 0.5425,
      "step": 1852
    },
    {
      "epoch": 0.04850192821121329,
      "grad_norm": 29.622968673706055,
      "learning_rate": 6.1837215388641716e-06,
      "loss": 0.4357,
      "step": 1853
    },
    {
      "epoch": 0.04852810302406338,
      "grad_norm": 28.131620407104492,
      "learning_rate": 6.180580999738288e-06,
      "loss": 0.5304,
      "step": 1854
    },
    {
      "epoch": 0.04855427783691347,
      "grad_norm": 14.910948753356934,
      "learning_rate": 6.177440460612405e-06,
      "loss": 0.3983,
      "step": 1855
    },
    {
      "epoch": 0.04858045264976355,
      "grad_norm": 25.571208953857422,
      "learning_rate": 6.174299921486522e-06,
      "loss": 0.5901,
      "step": 1856
    },
    {
      "epoch": 0.04860662746261364,
      "grad_norm": 23.14423370361328,
      "learning_rate": 6.171159382360638e-06,
      "loss": 0.7142,
      "step": 1857
    },
    {
      "epoch": 0.04863280227546373,
      "grad_norm": 18.91185188293457,
      "learning_rate": 6.168018843234755e-06,
      "loss": 0.569,
      "step": 1858
    },
    {
      "epoch": 0.04865897708831382,
      "grad_norm": 33.62759017944336,
      "learning_rate": 6.164878304108873e-06,
      "loss": 0.5257,
      "step": 1859
    },
    {
      "epoch": 0.0486851519011639,
      "grad_norm": 22.139812469482422,
      "learning_rate": 6.161737764982989e-06,
      "loss": 0.2892,
      "step": 1860
    },
    {
      "epoch": 0.04871132671401399,
      "grad_norm": 11.809596061706543,
      "learning_rate": 6.158597225857106e-06,
      "loss": 0.3049,
      "step": 1861
    },
    {
      "epoch": 0.04873750152686408,
      "grad_norm": 22.68961524963379,
      "learning_rate": 6.155456686731223e-06,
      "loss": 0.3946,
      "step": 1862
    },
    {
      "epoch": 0.04876367633971417,
      "grad_norm": 14.15758991241455,
      "learning_rate": 6.1523161476053395e-06,
      "loss": 0.419,
      "step": 1863
    },
    {
      "epoch": 0.04878985115256426,
      "grad_norm": 21.744443893432617,
      "learning_rate": 6.149175608479456e-06,
      "loss": 0.7646,
      "step": 1864
    },
    {
      "epoch": 0.048816025965414345,
      "grad_norm": 15.109386444091797,
      "learning_rate": 6.146035069353573e-06,
      "loss": 0.5512,
      "step": 1865
    },
    {
      "epoch": 0.048842200778264434,
      "grad_norm": 18.736827850341797,
      "learning_rate": 6.142894530227689e-06,
      "loss": 0.5042,
      "step": 1866
    },
    {
      "epoch": 0.048868375591114524,
      "grad_norm": 28.879104614257812,
      "learning_rate": 6.1397539911018055e-06,
      "loss": 0.3734,
      "step": 1867
    },
    {
      "epoch": 0.048894550403964614,
      "grad_norm": 24.503995895385742,
      "learning_rate": 6.136613451975922e-06,
      "loss": 0.4926,
      "step": 1868
    },
    {
      "epoch": 0.048920725216814696,
      "grad_norm": 18.27947998046875,
      "learning_rate": 6.133472912850039e-06,
      "loss": 0.3308,
      "step": 1869
    },
    {
      "epoch": 0.048946900029664786,
      "grad_norm": 20.58304786682129,
      "learning_rate": 6.1303323737241556e-06,
      "loss": 0.569,
      "step": 1870
    },
    {
      "epoch": 0.048973074842514876,
      "grad_norm": 16.689777374267578,
      "learning_rate": 6.127191834598272e-06,
      "loss": 0.2242,
      "step": 1871
    },
    {
      "epoch": 0.048999249655364965,
      "grad_norm": 15.344295501708984,
      "learning_rate": 6.12405129547239e-06,
      "loss": 0.2179,
      "step": 1872
    },
    {
      "epoch": 0.049025424468215055,
      "grad_norm": 10.384450912475586,
      "learning_rate": 6.1209107563465065e-06,
      "loss": 0.3149,
      "step": 1873
    },
    {
      "epoch": 0.04905159928106514,
      "grad_norm": 23.300443649291992,
      "learning_rate": 6.117770217220623e-06,
      "loss": 0.4681,
      "step": 1874
    },
    {
      "epoch": 0.04907777409391523,
      "grad_norm": 21.16755485534668,
      "learning_rate": 6.11462967809474e-06,
      "loss": 0.4859,
      "step": 1875
    },
    {
      "epoch": 0.04910394890676532,
      "grad_norm": 28.940296173095703,
      "learning_rate": 6.111489138968857e-06,
      "loss": 0.4066,
      "step": 1876
    },
    {
      "epoch": 0.04913012371961541,
      "grad_norm": 28.1897029876709,
      "learning_rate": 6.108348599842973e-06,
      "loss": 1.0701,
      "step": 1877
    },
    {
      "epoch": 0.04915629853246549,
      "grad_norm": 25.434425354003906,
      "learning_rate": 6.10520806071709e-06,
      "loss": 0.5103,
      "step": 1878
    },
    {
      "epoch": 0.04918247334531558,
      "grad_norm": 26.27858543395996,
      "learning_rate": 6.102067521591207e-06,
      "loss": 0.3271,
      "step": 1879
    },
    {
      "epoch": 0.04920864815816567,
      "grad_norm": 26.23378562927246,
      "learning_rate": 6.098926982465324e-06,
      "loss": 0.6632,
      "step": 1880
    },
    {
      "epoch": 0.04923482297101576,
      "grad_norm": 22.501707077026367,
      "learning_rate": 6.095786443339439e-06,
      "loss": 0.4924,
      "step": 1881
    },
    {
      "epoch": 0.04926099778386585,
      "grad_norm": 18.798492431640625,
      "learning_rate": 6.092645904213556e-06,
      "loss": 0.3107,
      "step": 1882
    },
    {
      "epoch": 0.04928717259671593,
      "grad_norm": 31.845190048217773,
      "learning_rate": 6.089505365087673e-06,
      "loss": 0.5814,
      "step": 1883
    },
    {
      "epoch": 0.04931334740956602,
      "grad_norm": 32.05630111694336,
      "learning_rate": 6.08636482596179e-06,
      "loss": 0.5524,
      "step": 1884
    },
    {
      "epoch": 0.04933952222241611,
      "grad_norm": 27.330278396606445,
      "learning_rate": 6.083224286835907e-06,
      "loss": 0.8292,
      "step": 1885
    },
    {
      "epoch": 0.0493656970352662,
      "grad_norm": 13.933496475219727,
      "learning_rate": 6.080083747710024e-06,
      "loss": 0.3806,
      "step": 1886
    },
    {
      "epoch": 0.04939187184811629,
      "grad_norm": 15.35069465637207,
      "learning_rate": 6.0769432085841404e-06,
      "loss": 0.3576,
      "step": 1887
    },
    {
      "epoch": 0.04941804666096637,
      "grad_norm": 22.96913719177246,
      "learning_rate": 6.073802669458257e-06,
      "loss": 0.4427,
      "step": 1888
    },
    {
      "epoch": 0.04944422147381646,
      "grad_norm": 18.454082489013672,
      "learning_rate": 6.070662130332374e-06,
      "loss": 0.3521,
      "step": 1889
    },
    {
      "epoch": 0.04947039628666655,
      "grad_norm": 27.389324188232422,
      "learning_rate": 6.0675215912064905e-06,
      "loss": 0.574,
      "step": 1890
    },
    {
      "epoch": 0.04949657109951664,
      "grad_norm": 26.634817123413086,
      "learning_rate": 6.064381052080607e-06,
      "loss": 0.4697,
      "step": 1891
    },
    {
      "epoch": 0.049522745912366724,
      "grad_norm": 30.7195987701416,
      "learning_rate": 6.061240512954724e-06,
      "loss": 0.3436,
      "step": 1892
    },
    {
      "epoch": 0.04954892072521681,
      "grad_norm": 15.968905448913574,
      "learning_rate": 6.0580999738288415e-06,
      "loss": 0.3729,
      "step": 1893
    },
    {
      "epoch": 0.0495750955380669,
      "grad_norm": 27.332460403442383,
      "learning_rate": 6.054959434702958e-06,
      "loss": 0.4127,
      "step": 1894
    },
    {
      "epoch": 0.04960127035091699,
      "grad_norm": 15.229080200195312,
      "learning_rate": 6.051818895577075e-06,
      "loss": 0.3811,
      "step": 1895
    },
    {
      "epoch": 0.04962744516376708,
      "grad_norm": 22.047008514404297,
      "learning_rate": 6.048678356451192e-06,
      "loss": 0.6427,
      "step": 1896
    },
    {
      "epoch": 0.049653619976617165,
      "grad_norm": 19.317222595214844,
      "learning_rate": 6.0455378173253075e-06,
      "loss": 0.4339,
      "step": 1897
    },
    {
      "epoch": 0.049679794789467255,
      "grad_norm": 21.702919006347656,
      "learning_rate": 6.042397278199424e-06,
      "loss": 0.5047,
      "step": 1898
    },
    {
      "epoch": 0.049705969602317344,
      "grad_norm": 34.45197296142578,
      "learning_rate": 6.039256739073541e-06,
      "loss": 0.4785,
      "step": 1899
    },
    {
      "epoch": 0.049732144415167434,
      "grad_norm": 23.90642547607422,
      "learning_rate": 6.036116199947658e-06,
      "loss": 0.4303,
      "step": 1900
    }
  ],
  "logging_steps": 1,
  "max_steps": 3821,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2855610348079104.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
