{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.10001395990018672,
  "eval_steps": 500,
  "global_step": 3821,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 2.6174812850088122e-05,
      "grad_norm": 30.22783851623535,
      "learning_rate": 1.2e-05,
      "loss": 7.5026,
      "step": 1
    },
    {
      "epoch": 5.2349625700176245e-05,
      "grad_norm": 29.683502197265625,
      "learning_rate": 1.1996859460874118e-05,
      "loss": 7.443,
      "step": 2
    },
    {
      "epoch": 7.852443855026436e-05,
      "grad_norm": 27.453813552856445,
      "learning_rate": 1.1993718921748234e-05,
      "loss": 7.9002,
      "step": 3
    },
    {
      "epoch": 0.00010469925140035249,
      "grad_norm": 24.240352630615234,
      "learning_rate": 1.1990578382622351e-05,
      "loss": 7.3203,
      "step": 4
    },
    {
      "epoch": 0.00013087406425044061,
      "grad_norm": 16.31001853942871,
      "learning_rate": 1.1987437843496467e-05,
      "loss": 7.0234,
      "step": 5
    },
    {
      "epoch": 0.00015704887710052873,
      "grad_norm": 25.663909912109375,
      "learning_rate": 1.1984297304370585e-05,
      "loss": 7.9986,
      "step": 6
    },
    {
      "epoch": 0.00018322368995061687,
      "grad_norm": 26.023481369018555,
      "learning_rate": 1.19811567652447e-05,
      "loss": 7.2023,
      "step": 7
    },
    {
      "epoch": 0.00020939850280070498,
      "grad_norm": 20.31952667236328,
      "learning_rate": 1.1978016226118818e-05,
      "loss": 7.9735,
      "step": 8
    },
    {
      "epoch": 0.0002355733156507931,
      "grad_norm": 25.408531188964844,
      "learning_rate": 1.1974875686992934e-05,
      "loss": 7.825,
      "step": 9
    },
    {
      "epoch": 0.00026174812850088123,
      "grad_norm": 29.22234344482422,
      "learning_rate": 1.197173514786705e-05,
      "loss": 8.8159,
      "step": 10
    },
    {
      "epoch": 0.0002879229413509693,
      "grad_norm": 27.097631454467773,
      "learning_rate": 1.1968594608741167e-05,
      "loss": 6.9501,
      "step": 11
    },
    {
      "epoch": 0.00031409775420105745,
      "grad_norm": 26.209110260009766,
      "learning_rate": 1.1965454069615283e-05,
      "loss": 8.0336,
      "step": 12
    },
    {
      "epoch": 0.0003402725670511456,
      "grad_norm": 23.530088424682617,
      "learning_rate": 1.19623135304894e-05,
      "loss": 7.3187,
      "step": 13
    },
    {
      "epoch": 0.00036644737990123373,
      "grad_norm": 22.233415603637695,
      "learning_rate": 1.1959172991363518e-05,
      "loss": 7.5003,
      "step": 14
    },
    {
      "epoch": 0.0003926221927513218,
      "grad_norm": 20.459087371826172,
      "learning_rate": 1.1956032452237634e-05,
      "loss": 6.8246,
      "step": 15
    },
    {
      "epoch": 0.00041879700560140996,
      "grad_norm": 24.221656799316406,
      "learning_rate": 1.1952891913111752e-05,
      "loss": 6.913,
      "step": 16
    },
    {
      "epoch": 0.0004449718184514981,
      "grad_norm": 15.340860366821289,
      "learning_rate": 1.1949751373985868e-05,
      "loss": 6.4441,
      "step": 17
    },
    {
      "epoch": 0.0004711466313015862,
      "grad_norm": 17.736690521240234,
      "learning_rate": 1.1946610834859985e-05,
      "loss": 7.4307,
      "step": 18
    },
    {
      "epoch": 0.0004973214441516743,
      "grad_norm": 24.10845184326172,
      "learning_rate": 1.1943470295734101e-05,
      "loss": 7.4598,
      "step": 19
    },
    {
      "epoch": 0.0005234962570017625,
      "grad_norm": 19.871971130371094,
      "learning_rate": 1.1940329756608219e-05,
      "loss": 6.3105,
      "step": 20
    },
    {
      "epoch": 0.0005496710698518506,
      "grad_norm": 30.47126579284668,
      "learning_rate": 1.1937189217482334e-05,
      "loss": 7.0865,
      "step": 21
    },
    {
      "epoch": 0.0005758458827019386,
      "grad_norm": 18.690874099731445,
      "learning_rate": 1.1934048678356452e-05,
      "loss": 6.3588,
      "step": 22
    },
    {
      "epoch": 0.0006020206955520268,
      "grad_norm": 19.9085750579834,
      "learning_rate": 1.193090813923057e-05,
      "loss": 6.2853,
      "step": 23
    },
    {
      "epoch": 0.0006281955084021149,
      "grad_norm": 22.348464965820312,
      "learning_rate": 1.1927767600104685e-05,
      "loss": 7.0539,
      "step": 24
    },
    {
      "epoch": 0.000654370321252203,
      "grad_norm": 17.024673461914062,
      "learning_rate": 1.1924627060978801e-05,
      "loss": 6.9062,
      "step": 25
    },
    {
      "epoch": 0.0006805451341022912,
      "grad_norm": 23.76247215270996,
      "learning_rate": 1.1921486521852917e-05,
      "loss": 7.154,
      "step": 26
    },
    {
      "epoch": 0.0007067199469523793,
      "grad_norm": 19.746639251708984,
      "learning_rate": 1.1918345982727035e-05,
      "loss": 6.2787,
      "step": 27
    },
    {
      "epoch": 0.0007328947598024675,
      "grad_norm": 19.664539337158203,
      "learning_rate": 1.1915205443601152e-05,
      "loss": 7.0984,
      "step": 28
    },
    {
      "epoch": 0.0007590695726525555,
      "grad_norm": 15.368788719177246,
      "learning_rate": 1.1912064904475268e-05,
      "loss": 6.8251,
      "step": 29
    },
    {
      "epoch": 0.0007852443855026436,
      "grad_norm": 14.586758613586426,
      "learning_rate": 1.1908924365349386e-05,
      "loss": 5.9218,
      "step": 30
    },
    {
      "epoch": 0.0008114191983527318,
      "grad_norm": 20.014503479003906,
      "learning_rate": 1.1905783826223501e-05,
      "loss": 6.4127,
      "step": 31
    },
    {
      "epoch": 0.0008375940112028199,
      "grad_norm": 16.258520126342773,
      "learning_rate": 1.1902643287097619e-05,
      "loss": 5.542,
      "step": 32
    },
    {
      "epoch": 0.000863768824052908,
      "grad_norm": 15.960464477539062,
      "learning_rate": 1.1899502747971735e-05,
      "loss": 5.913,
      "step": 33
    },
    {
      "epoch": 0.0008899436369029962,
      "grad_norm": 15.393472671508789,
      "learning_rate": 1.1896362208845852e-05,
      "loss": 5.518,
      "step": 34
    },
    {
      "epoch": 0.0009161184497530842,
      "grad_norm": 22.119590759277344,
      "learning_rate": 1.1893221669719968e-05,
      "loss": 6.6795,
      "step": 35
    },
    {
      "epoch": 0.0009422932626031724,
      "grad_norm": 22.641530990600586,
      "learning_rate": 1.1890081130594086e-05,
      "loss": 6.7796,
      "step": 36
    },
    {
      "epoch": 0.0009684680754532605,
      "grad_norm": 15.211116790771484,
      "learning_rate": 1.1886940591468203e-05,
      "loss": 6.2389,
      "step": 37
    },
    {
      "epoch": 0.0009946428883033486,
      "grad_norm": 17.28351593017578,
      "learning_rate": 1.188380005234232e-05,
      "loss": 5.4961,
      "step": 38
    },
    {
      "epoch": 0.0010208177011534368,
      "grad_norm": 14.06243896484375,
      "learning_rate": 1.1880659513216437e-05,
      "loss": 5.426,
      "step": 39
    },
    {
      "epoch": 0.001046992514003525,
      "grad_norm": 15.821523666381836,
      "learning_rate": 1.1877518974090553e-05,
      "loss": 6.035,
      "step": 40
    },
    {
      "epoch": 0.001073167326853613,
      "grad_norm": 17.898828506469727,
      "learning_rate": 1.1874378434964669e-05,
      "loss": 5.6535,
      "step": 41
    },
    {
      "epoch": 0.0010993421397037012,
      "grad_norm": 21.517593383789062,
      "learning_rate": 1.1871237895838786e-05,
      "loss": 5.962,
      "step": 42
    },
    {
      "epoch": 0.0011255169525537893,
      "grad_norm": 16.947418212890625,
      "learning_rate": 1.1868097356712902e-05,
      "loss": 6.4819,
      "step": 43
    },
    {
      "epoch": 0.0011516917654038773,
      "grad_norm": 17.106189727783203,
      "learning_rate": 1.186495681758702e-05,
      "loss": 5.1147,
      "step": 44
    },
    {
      "epoch": 0.0011778665782539654,
      "grad_norm": 32.797977447509766,
      "learning_rate": 1.1861816278461135e-05,
      "loss": 6.9105,
      "step": 45
    },
    {
      "epoch": 0.0012040413911040535,
      "grad_norm": 21.497854232788086,
      "learning_rate": 1.1858675739335253e-05,
      "loss": 6.4169,
      "step": 46
    },
    {
      "epoch": 0.0012302162039541417,
      "grad_norm": 17.993343353271484,
      "learning_rate": 1.1855535200209369e-05,
      "loss": 5.1963,
      "step": 47
    },
    {
      "epoch": 0.0012563910168042298,
      "grad_norm": 23.032163619995117,
      "learning_rate": 1.1852394661083486e-05,
      "loss": 6.4351,
      "step": 48
    },
    {
      "epoch": 0.001282565829654318,
      "grad_norm": 20.04737091064453,
      "learning_rate": 1.1849254121957604e-05,
      "loss": 4.7691,
      "step": 49
    },
    {
      "epoch": 0.001308740642504406,
      "grad_norm": 17.606029510498047,
      "learning_rate": 1.184611358283172e-05,
      "loss": 4.9135,
      "step": 50
    },
    {
      "epoch": 0.0013349154553544942,
      "grad_norm": 22.45792007446289,
      "learning_rate": 1.1842973043705837e-05,
      "loss": 4.2721,
      "step": 51
    },
    {
      "epoch": 0.0013610902682045824,
      "grad_norm": 23.00812530517578,
      "learning_rate": 1.1839832504579953e-05,
      "loss": 4.598,
      "step": 52
    },
    {
      "epoch": 0.0013872650810546705,
      "grad_norm": 26.22503089904785,
      "learning_rate": 1.183669196545407e-05,
      "loss": 4.6337,
      "step": 53
    },
    {
      "epoch": 0.0014134398939047586,
      "grad_norm": 27.360754013061523,
      "learning_rate": 1.1833551426328187e-05,
      "loss": 3.7572,
      "step": 54
    },
    {
      "epoch": 0.0014396147067548468,
      "grad_norm": 20.84589385986328,
      "learning_rate": 1.1830410887202304e-05,
      "loss": 5.1462,
      "step": 55
    },
    {
      "epoch": 0.001465789519604935,
      "grad_norm": 25.76931381225586,
      "learning_rate": 1.182727034807642e-05,
      "loss": 4.3577,
      "step": 56
    },
    {
      "epoch": 0.0014919643324550228,
      "grad_norm": 21.385438919067383,
      "learning_rate": 1.1824129808950536e-05,
      "loss": 5.1476,
      "step": 57
    },
    {
      "epoch": 0.001518139145305111,
      "grad_norm": 33.590694427490234,
      "learning_rate": 1.1820989269824653e-05,
      "loss": 3.8346,
      "step": 58
    },
    {
      "epoch": 0.0015443139581551991,
      "grad_norm": 25.63347625732422,
      "learning_rate": 1.181784873069877e-05,
      "loss": 3.3297,
      "step": 59
    },
    {
      "epoch": 0.0015704887710052873,
      "grad_norm": 25.36574363708496,
      "learning_rate": 1.1814708191572887e-05,
      "loss": 4.9581,
      "step": 60
    },
    {
      "epoch": 0.0015966635838553754,
      "grad_norm": 31.0465030670166,
      "learning_rate": 1.1811567652447004e-05,
      "loss": 4.6948,
      "step": 61
    },
    {
      "epoch": 0.0016228383967054635,
      "grad_norm": 29.258100509643555,
      "learning_rate": 1.180842711332112e-05,
      "loss": 4.7846,
      "step": 62
    },
    {
      "epoch": 0.0016490132095555517,
      "grad_norm": 23.9012451171875,
      "learning_rate": 1.1805286574195238e-05,
      "loss": 4.0025,
      "step": 63
    },
    {
      "epoch": 0.0016751880224056398,
      "grad_norm": 29.01807975769043,
      "learning_rate": 1.1802146035069354e-05,
      "loss": 4.4877,
      "step": 64
    },
    {
      "epoch": 0.001701362835255728,
      "grad_norm": 28.187484741210938,
      "learning_rate": 1.1799005495943471e-05,
      "loss": 3.0827,
      "step": 65
    },
    {
      "epoch": 0.001727537648105816,
      "grad_norm": 35.91884231567383,
      "learning_rate": 1.1795864956817587e-05,
      "loss": 2.6421,
      "step": 66
    },
    {
      "epoch": 0.0017537124609559042,
      "grad_norm": 28.194583892822266,
      "learning_rate": 1.1792724417691705e-05,
      "loss": 3.4314,
      "step": 67
    },
    {
      "epoch": 0.0017798872738059924,
      "grad_norm": 23.687419891357422,
      "learning_rate": 1.178958387856582e-05,
      "loss": 3.691,
      "step": 68
    },
    {
      "epoch": 0.0018060620866560803,
      "grad_norm": 20.045509338378906,
      "learning_rate": 1.1786443339439938e-05,
      "loss": 3.7732,
      "step": 69
    },
    {
      "epoch": 0.0018322368995061684,
      "grad_norm": 20.962646484375,
      "learning_rate": 1.1783302800314056e-05,
      "loss": 4.0607,
      "step": 70
    },
    {
      "epoch": 0.0018584117123562566,
      "grad_norm": 30.48273277282715,
      "learning_rate": 1.178016226118817e-05,
      "loss": 2.9529,
      "step": 71
    },
    {
      "epoch": 0.0018845865252063447,
      "grad_norm": 11.651561737060547,
      "learning_rate": 1.1777021722062287e-05,
      "loss": 1.394,
      "step": 72
    },
    {
      "epoch": 0.0019107613380564329,
      "grad_norm": 20.1381893157959,
      "learning_rate": 1.1773881182936403e-05,
      "loss": 2.4774,
      "step": 73
    },
    {
      "epoch": 0.001936936150906521,
      "grad_norm": 17.626211166381836,
      "learning_rate": 1.177074064381052e-05,
      "loss": 3.911,
      "step": 74
    },
    {
      "epoch": 0.0019631109637566094,
      "grad_norm": 14.677168846130371,
      "learning_rate": 1.1767600104684638e-05,
      "loss": 2.1644,
      "step": 75
    },
    {
      "epoch": 0.0019892857766066973,
      "grad_norm": 24.24646759033203,
      "learning_rate": 1.1764459565558754e-05,
      "loss": 4.2923,
      "step": 76
    },
    {
      "epoch": 0.002015460589456785,
      "grad_norm": 15.711065292358398,
      "learning_rate": 1.1761319026432872e-05,
      "loss": 2.2434,
      "step": 77
    },
    {
      "epoch": 0.0020416354023068736,
      "grad_norm": 19.13817024230957,
      "learning_rate": 1.1758178487306988e-05,
      "loss": 2.7628,
      "step": 78
    },
    {
      "epoch": 0.0020678102151569615,
      "grad_norm": 18.389123916625977,
      "learning_rate": 1.1755037948181105e-05,
      "loss": 2.8062,
      "step": 79
    },
    {
      "epoch": 0.00209398502800705,
      "grad_norm": 17.289493560791016,
      "learning_rate": 1.1751897409055221e-05,
      "loss": 3.033,
      "step": 80
    },
    {
      "epoch": 0.0021201598408571378,
      "grad_norm": 18.462589263916016,
      "learning_rate": 1.1748756869929338e-05,
      "loss": 2.6751,
      "step": 81
    },
    {
      "epoch": 0.002146334653707226,
      "grad_norm": 19.505393981933594,
      "learning_rate": 1.1745616330803456e-05,
      "loss": 1.9767,
      "step": 82
    },
    {
      "epoch": 0.002172509466557314,
      "grad_norm": 14.440767288208008,
      "learning_rate": 1.1742475791677572e-05,
      "loss": 2.1662,
      "step": 83
    },
    {
      "epoch": 0.0021986842794074024,
      "grad_norm": 14.605199813842773,
      "learning_rate": 1.173933525255169e-05,
      "loss": 2.809,
      "step": 84
    },
    {
      "epoch": 0.0022248590922574903,
      "grad_norm": 14.445960998535156,
      "learning_rate": 1.1736194713425805e-05,
      "loss": 2.0437,
      "step": 85
    },
    {
      "epoch": 0.0022510339051075787,
      "grad_norm": 19.794342041015625,
      "learning_rate": 1.1733054174299923e-05,
      "loss": 2.2116,
      "step": 86
    },
    {
      "epoch": 0.0022772087179576666,
      "grad_norm": 18.496395111083984,
      "learning_rate": 1.1729913635174039e-05,
      "loss": 2.4008,
      "step": 87
    },
    {
      "epoch": 0.0023033835308077545,
      "grad_norm": 18.180294036865234,
      "learning_rate": 1.1726773096048155e-05,
      "loss": 1.5929,
      "step": 88
    },
    {
      "epoch": 0.002329558343657843,
      "grad_norm": 16.627222061157227,
      "learning_rate": 1.1723632556922272e-05,
      "loss": 3.528,
      "step": 89
    },
    {
      "epoch": 0.002355733156507931,
      "grad_norm": 14.420065879821777,
      "learning_rate": 1.1720492017796388e-05,
      "loss": 1.469,
      "step": 90
    },
    {
      "epoch": 0.002381907969358019,
      "grad_norm": 25.988649368286133,
      "learning_rate": 1.1717351478670506e-05,
      "loss": 2.8033,
      "step": 91
    },
    {
      "epoch": 0.002408082782208107,
      "grad_norm": 24.650802612304688,
      "learning_rate": 1.1714210939544621e-05,
      "loss": 2.6106,
      "step": 92
    },
    {
      "epoch": 0.0024342575950581954,
      "grad_norm": 22.823925018310547,
      "learning_rate": 1.1711070400418739e-05,
      "loss": 3.2885,
      "step": 93
    },
    {
      "epoch": 0.0024604324079082833,
      "grad_norm": 25.272377014160156,
      "learning_rate": 1.1707929861292855e-05,
      "loss": 4.0585,
      "step": 94
    },
    {
      "epoch": 0.0024866072207583717,
      "grad_norm": 19.218107223510742,
      "learning_rate": 1.1704789322166972e-05,
      "loss": 1.5222,
      "step": 95
    },
    {
      "epoch": 0.0025127820336084596,
      "grad_norm": 12.57055377960205,
      "learning_rate": 1.170164878304109e-05,
      "loss": 1.4616,
      "step": 96
    },
    {
      "epoch": 0.002538956846458548,
      "grad_norm": 21.43596076965332,
      "learning_rate": 1.1698508243915206e-05,
      "loss": 3.5691,
      "step": 97
    },
    {
      "epoch": 0.002565131659308636,
      "grad_norm": 15.383261680603027,
      "learning_rate": 1.1695367704789323e-05,
      "loss": 2.7256,
      "step": 98
    },
    {
      "epoch": 0.0025913064721587243,
      "grad_norm": 16.71521759033203,
      "learning_rate": 1.169222716566344e-05,
      "loss": 2.692,
      "step": 99
    },
    {
      "epoch": 0.002617481285008812,
      "grad_norm": 23.095685958862305,
      "learning_rate": 1.1689086626537557e-05,
      "loss": 2.3494,
      "step": 100
    },
    {
      "epoch": 0.0026436560978589,
      "grad_norm": 11.330353736877441,
      "learning_rate": 1.1685946087411673e-05,
      "loss": 0.8398,
      "step": 101
    },
    {
      "epoch": 0.0026698309107089885,
      "grad_norm": 13.569964408874512,
      "learning_rate": 1.1682805548285788e-05,
      "loss": 1.9375,
      "step": 102
    },
    {
      "epoch": 0.0026960057235590764,
      "grad_norm": 20.64599609375,
      "learning_rate": 1.1679665009159906e-05,
      "loss": 3.5732,
      "step": 103
    },
    {
      "epoch": 0.0027221805364091647,
      "grad_norm": 16.05816650390625,
      "learning_rate": 1.1676524470034022e-05,
      "loss": 2.4012,
      "step": 104
    },
    {
      "epoch": 0.0027483553492592527,
      "grad_norm": 21.189777374267578,
      "learning_rate": 1.167338393090814e-05,
      "loss": 2.0351,
      "step": 105
    },
    {
      "epoch": 0.002774530162109341,
      "grad_norm": 14.837639808654785,
      "learning_rate": 1.1670243391782255e-05,
      "loss": 2.0259,
      "step": 106
    },
    {
      "epoch": 0.002800704974959429,
      "grad_norm": 15.209829330444336,
      "learning_rate": 1.1667102852656373e-05,
      "loss": 0.9615,
      "step": 107
    },
    {
      "epoch": 0.0028268797878095173,
      "grad_norm": 13.797417640686035,
      "learning_rate": 1.166396231353049e-05,
      "loss": 2.6921,
      "step": 108
    },
    {
      "epoch": 0.0028530546006596052,
      "grad_norm": 16.877059936523438,
      "learning_rate": 1.1660821774404606e-05,
      "loss": 2.4156,
      "step": 109
    },
    {
      "epoch": 0.0028792294135096936,
      "grad_norm": 15.073802947998047,
      "learning_rate": 1.1657681235278724e-05,
      "loss": 1.5567,
      "step": 110
    },
    {
      "epoch": 0.0029054042263597815,
      "grad_norm": 22.69402503967285,
      "learning_rate": 1.165454069615284e-05,
      "loss": 1.9831,
      "step": 111
    },
    {
      "epoch": 0.00293157903920987,
      "grad_norm": 13.091713905334473,
      "learning_rate": 1.1651400157026957e-05,
      "loss": 1.7704,
      "step": 112
    },
    {
      "epoch": 0.0029577538520599578,
      "grad_norm": 16.164756774902344,
      "learning_rate": 1.1648259617901073e-05,
      "loss": 2.1245,
      "step": 113
    },
    {
      "epoch": 0.0029839286649100457,
      "grad_norm": 11.855355262756348,
      "learning_rate": 1.164511907877519e-05,
      "loss": 1.4096,
      "step": 114
    },
    {
      "epoch": 0.003010103477760134,
      "grad_norm": 17.004735946655273,
      "learning_rate": 1.1641978539649307e-05,
      "loss": 3.1814,
      "step": 115
    },
    {
      "epoch": 0.003036278290610222,
      "grad_norm": 16.72195053100586,
      "learning_rate": 1.1638838000523424e-05,
      "loss": 3.4206,
      "step": 116
    },
    {
      "epoch": 0.0030624531034603103,
      "grad_norm": 16.99317169189453,
      "learning_rate": 1.1635697461397542e-05,
      "loss": 3.1969,
      "step": 117
    },
    {
      "epoch": 0.0030886279163103983,
      "grad_norm": 14.970856666564941,
      "learning_rate": 1.1632556922271656e-05,
      "loss": 1.8483,
      "step": 118
    },
    {
      "epoch": 0.0031148027291604866,
      "grad_norm": 16.661405563354492,
      "learning_rate": 1.1629416383145773e-05,
      "loss": 2.4612,
      "step": 119
    },
    {
      "epoch": 0.0031409775420105745,
      "grad_norm": 15.496171951293945,
      "learning_rate": 1.162627584401989e-05,
      "loss": 2.1451,
      "step": 120
    },
    {
      "epoch": 0.003167152354860663,
      "grad_norm": 14.659793853759766,
      "learning_rate": 1.1623135304894007e-05,
      "loss": 3.0658,
      "step": 121
    },
    {
      "epoch": 0.003193327167710751,
      "grad_norm": 20.552831649780273,
      "learning_rate": 1.1619994765768124e-05,
      "loss": 2.1227,
      "step": 122
    },
    {
      "epoch": 0.003219501980560839,
      "grad_norm": 15.305253028869629,
      "learning_rate": 1.161685422664224e-05,
      "loss": 1.8505,
      "step": 123
    },
    {
      "epoch": 0.003245676793410927,
      "grad_norm": 10.772724151611328,
      "learning_rate": 1.1613713687516358e-05,
      "loss": 0.7333,
      "step": 124
    },
    {
      "epoch": 0.0032718516062610154,
      "grad_norm": 11.5306978225708,
      "learning_rate": 1.1610573148390474e-05,
      "loss": 1.6245,
      "step": 125
    },
    {
      "epoch": 0.0032980264191111034,
      "grad_norm": 15.563087463378906,
      "learning_rate": 1.1607432609264591e-05,
      "loss": 2.5764,
      "step": 126
    },
    {
      "epoch": 0.0033242012319611913,
      "grad_norm": 13.194500923156738,
      "learning_rate": 1.1604292070138707e-05,
      "loss": 3.5995,
      "step": 127
    },
    {
      "epoch": 0.0033503760448112796,
      "grad_norm": 20.081491470336914,
      "learning_rate": 1.1601151531012825e-05,
      "loss": 3.2537,
      "step": 128
    },
    {
      "epoch": 0.0033765508576613676,
      "grad_norm": 10.320838928222656,
      "learning_rate": 1.1598010991886942e-05,
      "loss": 1.1903,
      "step": 129
    },
    {
      "epoch": 0.003402725670511456,
      "grad_norm": 15.799105644226074,
      "learning_rate": 1.1594870452761058e-05,
      "loss": 2.5996,
      "step": 130
    },
    {
      "epoch": 0.003428900483361544,
      "grad_norm": 11.129196166992188,
      "learning_rate": 1.1591729913635176e-05,
      "loss": 1.8434,
      "step": 131
    },
    {
      "epoch": 0.003455075296211632,
      "grad_norm": 20.662139892578125,
      "learning_rate": 1.1588589374509291e-05,
      "loss": 1.8442,
      "step": 132
    },
    {
      "epoch": 0.00348125010906172,
      "grad_norm": 12.548161506652832,
      "learning_rate": 1.1585448835383407e-05,
      "loss": 2.7647,
      "step": 133
    },
    {
      "epoch": 0.0035074249219118085,
      "grad_norm": 15.477767944335938,
      "learning_rate": 1.1582308296257525e-05,
      "loss": 1.2986,
      "step": 134
    },
    {
      "epoch": 0.0035335997347618964,
      "grad_norm": 13.959481239318848,
      "learning_rate": 1.157916775713164e-05,
      "loss": 1.9339,
      "step": 135
    },
    {
      "epoch": 0.0035597745476119848,
      "grad_norm": 11.074966430664062,
      "learning_rate": 1.1576027218005758e-05,
      "loss": 0.7969,
      "step": 136
    },
    {
      "epoch": 0.0035859493604620727,
      "grad_norm": 17.44649314880371,
      "learning_rate": 1.1572886678879874e-05,
      "loss": 3.6024,
      "step": 137
    },
    {
      "epoch": 0.0036121241733121606,
      "grad_norm": 19.50830078125,
      "learning_rate": 1.1569746139753992e-05,
      "loss": 2.5693,
      "step": 138
    },
    {
      "epoch": 0.003638298986162249,
      "grad_norm": 16.172344207763672,
      "learning_rate": 1.1566605600628107e-05,
      "loss": 3.5693,
      "step": 139
    },
    {
      "epoch": 0.003664473799012337,
      "grad_norm": 9.623823165893555,
      "learning_rate": 1.1563465061502225e-05,
      "loss": 1.3644,
      "step": 140
    },
    {
      "epoch": 0.0036906486118624252,
      "grad_norm": 17.274850845336914,
      "learning_rate": 1.1560324522376341e-05,
      "loss": 2.3273,
      "step": 141
    },
    {
      "epoch": 0.003716823424712513,
      "grad_norm": 10.902981758117676,
      "learning_rate": 1.1557183983250458e-05,
      "loss": 0.7059,
      "step": 142
    },
    {
      "epoch": 0.0037429982375626015,
      "grad_norm": 13.711991310119629,
      "learning_rate": 1.1554043444124576e-05,
      "loss": 2.1896,
      "step": 143
    },
    {
      "epoch": 0.0037691730504126894,
      "grad_norm": 12.256510734558105,
      "learning_rate": 1.1550902904998692e-05,
      "loss": 1.0948,
      "step": 144
    },
    {
      "epoch": 0.003795347863262778,
      "grad_norm": 13.120607376098633,
      "learning_rate": 1.154776236587281e-05,
      "loss": 2.105,
      "step": 145
    },
    {
      "epoch": 0.0038215226761128657,
      "grad_norm": 14.109667778015137,
      "learning_rate": 1.1544621826746925e-05,
      "loss": 2.0919,
      "step": 146
    },
    {
      "epoch": 0.003847697488962954,
      "grad_norm": 9.945662498474121,
      "learning_rate": 1.1541481287621043e-05,
      "loss": 2.181,
      "step": 147
    },
    {
      "epoch": 0.003873872301813042,
      "grad_norm": 17.676101684570312,
      "learning_rate": 1.1538340748495159e-05,
      "loss": 2.1376,
      "step": 148
    },
    {
      "epoch": 0.0039000471146631304,
      "grad_norm": 12.687335014343262,
      "learning_rate": 1.1535200209369275e-05,
      "loss": 1.5024,
      "step": 149
    },
    {
      "epoch": 0.003926221927513219,
      "grad_norm": 13.829209327697754,
      "learning_rate": 1.1532059670243392e-05,
      "loss": 1.347,
      "step": 150
    },
    {
      "epoch": 0.003952396740363307,
      "grad_norm": 15.27659797668457,
      "learning_rate": 1.1528919131117508e-05,
      "loss": 3.4027,
      "step": 151
    },
    {
      "epoch": 0.0039785715532133946,
      "grad_norm": 19.26668930053711,
      "learning_rate": 1.1525778591991625e-05,
      "loss": 2.7934,
      "step": 152
    },
    {
      "epoch": 0.0040047463660634825,
      "grad_norm": 12.895267486572266,
      "learning_rate": 1.1522638052865741e-05,
      "loss": 2.0696,
      "step": 153
    },
    {
      "epoch": 0.00403092117891357,
      "grad_norm": 13.362541198730469,
      "learning_rate": 1.1519497513739859e-05,
      "loss": 1.9762,
      "step": 154
    },
    {
      "epoch": 0.004057095991763659,
      "grad_norm": 17.350093841552734,
      "learning_rate": 1.1516356974613976e-05,
      "loss": 3.0854,
      "step": 155
    },
    {
      "epoch": 0.004083270804613747,
      "grad_norm": 11.796960830688477,
      "learning_rate": 1.1513216435488092e-05,
      "loss": 0.8029,
      "step": 156
    },
    {
      "epoch": 0.004109445617463835,
      "grad_norm": 17.074609756469727,
      "learning_rate": 1.151007589636221e-05,
      "loss": 2.0252,
      "step": 157
    },
    {
      "epoch": 0.004135620430313923,
      "grad_norm": 14.722773551940918,
      "learning_rate": 1.1506935357236326e-05,
      "loss": 2.4514,
      "step": 158
    },
    {
      "epoch": 0.004161795243164012,
      "grad_norm": 8.824915885925293,
      "learning_rate": 1.1503794818110443e-05,
      "loss": 1.6378,
      "step": 159
    },
    {
      "epoch": 0.0041879700560141,
      "grad_norm": 9.449216842651367,
      "learning_rate": 1.1500654278984559e-05,
      "loss": 0.9903,
      "step": 160
    },
    {
      "epoch": 0.004214144868864188,
      "grad_norm": 15.854833602905273,
      "learning_rate": 1.1497513739858677e-05,
      "loss": 2.7349,
      "step": 161
    },
    {
      "epoch": 0.0042403196817142755,
      "grad_norm": 11.094951629638672,
      "learning_rate": 1.1494373200732793e-05,
      "loss": 2.4186,
      "step": 162
    },
    {
      "epoch": 0.0042664944945643634,
      "grad_norm": 16.610332489013672,
      "learning_rate": 1.149123266160691e-05,
      "loss": 2.261,
      "step": 163
    },
    {
      "epoch": 0.004292669307414452,
      "grad_norm": 11.434932708740234,
      "learning_rate": 1.1488092122481026e-05,
      "loss": 1.9423,
      "step": 164
    },
    {
      "epoch": 0.00431884412026454,
      "grad_norm": 11.406320571899414,
      "learning_rate": 1.1484951583355142e-05,
      "loss": 1.8591,
      "step": 165
    },
    {
      "epoch": 0.004345018933114628,
      "grad_norm": 17.76449966430664,
      "learning_rate": 1.148181104422926e-05,
      "loss": 2.4188,
      "step": 166
    },
    {
      "epoch": 0.004371193745964716,
      "grad_norm": 13.17193603515625,
      "learning_rate": 1.1478670505103375e-05,
      "loss": 1.3708,
      "step": 167
    },
    {
      "epoch": 0.004397368558814805,
      "grad_norm": 10.870563507080078,
      "learning_rate": 1.1475529965977493e-05,
      "loss": 0.8146,
      "step": 168
    },
    {
      "epoch": 0.004423543371664893,
      "grad_norm": 15.043729782104492,
      "learning_rate": 1.147238942685161e-05,
      "loss": 1.1763,
      "step": 169
    },
    {
      "epoch": 0.004449718184514981,
      "grad_norm": 17.411113739013672,
      "learning_rate": 1.1469248887725726e-05,
      "loss": 2.6194,
      "step": 170
    },
    {
      "epoch": 0.0044758929973650685,
      "grad_norm": 17.68866729736328,
      "learning_rate": 1.1466108348599844e-05,
      "loss": 2.6683,
      "step": 171
    },
    {
      "epoch": 0.004502067810215157,
      "grad_norm": 14.316873550415039,
      "learning_rate": 1.146296780947396e-05,
      "loss": 1.4204,
      "step": 172
    },
    {
      "epoch": 0.004528242623065245,
      "grad_norm": 16.87318992614746,
      "learning_rate": 1.1459827270348077e-05,
      "loss": 2.0249,
      "step": 173
    },
    {
      "epoch": 0.004554417435915333,
      "grad_norm": 14.415238380432129,
      "learning_rate": 1.1456686731222193e-05,
      "loss": 1.6838,
      "step": 174
    },
    {
      "epoch": 0.004580592248765421,
      "grad_norm": 12.448020935058594,
      "learning_rate": 1.145354619209631e-05,
      "loss": 2.0115,
      "step": 175
    },
    {
      "epoch": 0.004606767061615509,
      "grad_norm": 19.25944709777832,
      "learning_rate": 1.1450405652970428e-05,
      "loss": 1.3213,
      "step": 176
    },
    {
      "epoch": 0.004632941874465598,
      "grad_norm": 21.150325775146484,
      "learning_rate": 1.1447265113844544e-05,
      "loss": 1.7329,
      "step": 177
    },
    {
      "epoch": 0.004659116687315686,
      "grad_norm": 12.001667976379395,
      "learning_rate": 1.1444124574718662e-05,
      "loss": 1.2908,
      "step": 178
    },
    {
      "epoch": 0.004685291500165774,
      "grad_norm": 11.877140045166016,
      "learning_rate": 1.1440984035592776e-05,
      "loss": 0.9327,
      "step": 179
    },
    {
      "epoch": 0.004711466313015862,
      "grad_norm": 16.84221649169922,
      "learning_rate": 1.1437843496466893e-05,
      "loss": 2.0072,
      "step": 180
    },
    {
      "epoch": 0.00473764112586595,
      "grad_norm": 14.974508285522461,
      "learning_rate": 1.143470295734101e-05,
      "loss": 2.6614,
      "step": 181
    },
    {
      "epoch": 0.004763815938716038,
      "grad_norm": 14.474237442016602,
      "learning_rate": 1.1431562418215127e-05,
      "loss": 1.7737,
      "step": 182
    },
    {
      "epoch": 0.004789990751566126,
      "grad_norm": 11.452691078186035,
      "learning_rate": 1.1428421879089244e-05,
      "loss": 0.6434,
      "step": 183
    },
    {
      "epoch": 0.004816165564416214,
      "grad_norm": 18.34880256652832,
      "learning_rate": 1.142528133996336e-05,
      "loss": 1.1745,
      "step": 184
    },
    {
      "epoch": 0.004842340377266303,
      "grad_norm": 16.266033172607422,
      "learning_rate": 1.1422140800837478e-05,
      "loss": 1.9531,
      "step": 185
    },
    {
      "epoch": 0.004868515190116391,
      "grad_norm": 13.216902732849121,
      "learning_rate": 1.1419000261711593e-05,
      "loss": 1.1746,
      "step": 186
    },
    {
      "epoch": 0.004894690002966479,
      "grad_norm": 15.451672554016113,
      "learning_rate": 1.1415859722585711e-05,
      "loss": 2.0025,
      "step": 187
    },
    {
      "epoch": 0.004920864815816567,
      "grad_norm": 27.117507934570312,
      "learning_rate": 1.1412719183459827e-05,
      "loss": 1.8598,
      "step": 188
    },
    {
      "epoch": 0.004947039628666655,
      "grad_norm": 12.628796577453613,
      "learning_rate": 1.1409578644333944e-05,
      "loss": 1.6837,
      "step": 189
    },
    {
      "epoch": 0.004973214441516743,
      "grad_norm": 9.43991756439209,
      "learning_rate": 1.1406438105208062e-05,
      "loss": 1.1251,
      "step": 190
    },
    {
      "epoch": 0.004999389254366831,
      "grad_norm": 15.370447158813477,
      "learning_rate": 1.1403297566082178e-05,
      "loss": 1.5517,
      "step": 191
    },
    {
      "epoch": 0.005025564067216919,
      "grad_norm": 16.280874252319336,
      "learning_rate": 1.1400157026956295e-05,
      "loss": 1.4637,
      "step": 192
    },
    {
      "epoch": 0.005051738880067007,
      "grad_norm": 13.504317283630371,
      "learning_rate": 1.1397016487830411e-05,
      "loss": 2.3779,
      "step": 193
    },
    {
      "epoch": 0.005077913692917096,
      "grad_norm": 16.56597328186035,
      "learning_rate": 1.1393875948704529e-05,
      "loss": 1.8118,
      "step": 194
    },
    {
      "epoch": 0.005104088505767184,
      "grad_norm": 11.969481468200684,
      "learning_rate": 1.1390735409578645e-05,
      "loss": 1.333,
      "step": 195
    },
    {
      "epoch": 0.005130263318617272,
      "grad_norm": 15.130587577819824,
      "learning_rate": 1.138759487045276e-05,
      "loss": 2.3335,
      "step": 196
    },
    {
      "epoch": 0.00515643813146736,
      "grad_norm": 12.055906295776367,
      "learning_rate": 1.1384454331326878e-05,
      "loss": 1.386,
      "step": 197
    },
    {
      "epoch": 0.0051826129443174485,
      "grad_norm": 13.507920265197754,
      "learning_rate": 1.1381313792200994e-05,
      "loss": 2.3502,
      "step": 198
    },
    {
      "epoch": 0.0052087877571675364,
      "grad_norm": 13.850314140319824,
      "learning_rate": 1.1378173253075112e-05,
      "loss": 2.3893,
      "step": 199
    },
    {
      "epoch": 0.005234962570017624,
      "grad_norm": 12.840747833251953,
      "learning_rate": 1.1375032713949227e-05,
      "loss": 2.0314,
      "step": 200
    },
    {
      "epoch": 0.005261137382867712,
      "grad_norm": 13.096674919128418,
      "learning_rate": 1.1371892174823345e-05,
      "loss": 1.7229,
      "step": 201
    },
    {
      "epoch": 0.0052873121957178,
      "grad_norm": 15.769506454467773,
      "learning_rate": 1.1368751635697462e-05,
      "loss": 1.7678,
      "step": 202
    },
    {
      "epoch": 0.005313487008567889,
      "grad_norm": 13.398155212402344,
      "learning_rate": 1.1365611096571578e-05,
      "loss": 0.7023,
      "step": 203
    },
    {
      "epoch": 0.005339661821417977,
      "grad_norm": 14.75139045715332,
      "learning_rate": 1.1362470557445696e-05,
      "loss": 1.9156,
      "step": 204
    },
    {
      "epoch": 0.005365836634268065,
      "grad_norm": 14.622982025146484,
      "learning_rate": 1.1359330018319812e-05,
      "loss": 1.3304,
      "step": 205
    },
    {
      "epoch": 0.005392011447118153,
      "grad_norm": 15.189102172851562,
      "learning_rate": 1.135618947919393e-05,
      "loss": 0.9287,
      "step": 206
    },
    {
      "epoch": 0.0054181862599682416,
      "grad_norm": 19.454303741455078,
      "learning_rate": 1.1353048940068045e-05,
      "loss": 1.6403,
      "step": 207
    },
    {
      "epoch": 0.0054443610728183295,
      "grad_norm": 17.01598358154297,
      "learning_rate": 1.1349908400942163e-05,
      "loss": 2.7376,
      "step": 208
    },
    {
      "epoch": 0.005470535885668417,
      "grad_norm": 15.76163101196289,
      "learning_rate": 1.1346767861816279e-05,
      "loss": 2.3374,
      "step": 209
    },
    {
      "epoch": 0.005496710698518505,
      "grad_norm": 14.11268424987793,
      "learning_rate": 1.1343627322690394e-05,
      "loss": 1.655,
      "step": 210
    },
    {
      "epoch": 0.005522885511368594,
      "grad_norm": 20.920007705688477,
      "learning_rate": 1.1340486783564512e-05,
      "loss": 0.767,
      "step": 211
    },
    {
      "epoch": 0.005549060324218682,
      "grad_norm": 9.898805618286133,
      "learning_rate": 1.1337346244438628e-05,
      "loss": 1.6528,
      "step": 212
    },
    {
      "epoch": 0.00557523513706877,
      "grad_norm": 12.516654014587402,
      "learning_rate": 1.1334205705312745e-05,
      "loss": 1.4345,
      "step": 213
    },
    {
      "epoch": 0.005601409949918858,
      "grad_norm": 18.73650550842285,
      "learning_rate": 1.1331065166186861e-05,
      "loss": 1.4362,
      "step": 214
    },
    {
      "epoch": 0.005627584762768946,
      "grad_norm": 15.803741455078125,
      "learning_rate": 1.1327924627060979e-05,
      "loss": 1.7166,
      "step": 215
    },
    {
      "epoch": 0.005653759575619035,
      "grad_norm": 14.517212867736816,
      "learning_rate": 1.1324784087935096e-05,
      "loss": 1.7294,
      "step": 216
    },
    {
      "epoch": 0.0056799343884691225,
      "grad_norm": 16.518701553344727,
      "learning_rate": 1.1321643548809212e-05,
      "loss": 1.2964,
      "step": 217
    },
    {
      "epoch": 0.0057061092013192104,
      "grad_norm": 13.22188663482666,
      "learning_rate": 1.131850300968333e-05,
      "loss": 1.4885,
      "step": 218
    },
    {
      "epoch": 0.005732284014169298,
      "grad_norm": 12.286432266235352,
      "learning_rate": 1.1315362470557446e-05,
      "loss": 1.4088,
      "step": 219
    },
    {
      "epoch": 0.005758458827019387,
      "grad_norm": 11.964247703552246,
      "learning_rate": 1.1312221931431563e-05,
      "loss": 0.9841,
      "step": 220
    },
    {
      "epoch": 0.005784633639869475,
      "grad_norm": 15.483153343200684,
      "learning_rate": 1.1309081392305679e-05,
      "loss": 1.1353,
      "step": 221
    },
    {
      "epoch": 0.005810808452719563,
      "grad_norm": 13.376920700073242,
      "learning_rate": 1.1305940853179797e-05,
      "loss": 1.7308,
      "step": 222
    },
    {
      "epoch": 0.005836983265569651,
      "grad_norm": 15.010568618774414,
      "learning_rate": 1.1302800314053914e-05,
      "loss": 1.1283,
      "step": 223
    },
    {
      "epoch": 0.00586315807841974,
      "grad_norm": 15.958560943603516,
      "learning_rate": 1.129965977492803e-05,
      "loss": 1.5827,
      "step": 224
    },
    {
      "epoch": 0.005889332891269828,
      "grad_norm": 16.578712463378906,
      "learning_rate": 1.1296519235802148e-05,
      "loss": 1.458,
      "step": 225
    },
    {
      "epoch": 0.0059155077041199156,
      "grad_norm": 15.302216529846191,
      "learning_rate": 1.1293378696676262e-05,
      "loss": 1.1604,
      "step": 226
    },
    {
      "epoch": 0.0059416825169700035,
      "grad_norm": 15.29163646697998,
      "learning_rate": 1.129023815755038e-05,
      "loss": 0.7297,
      "step": 227
    },
    {
      "epoch": 0.005967857329820091,
      "grad_norm": 12.139481544494629,
      "learning_rate": 1.1287097618424497e-05,
      "loss": 0.9888,
      "step": 228
    },
    {
      "epoch": 0.00599403214267018,
      "grad_norm": 19.516498565673828,
      "learning_rate": 1.1283957079298613e-05,
      "loss": 2.5996,
      "step": 229
    },
    {
      "epoch": 0.006020206955520268,
      "grad_norm": 12.15183162689209,
      "learning_rate": 1.128081654017273e-05,
      "loss": 2.5289,
      "step": 230
    },
    {
      "epoch": 0.006046381768370356,
      "grad_norm": 12.720767974853516,
      "learning_rate": 1.1277676001046846e-05,
      "loss": 0.967,
      "step": 231
    },
    {
      "epoch": 0.006072556581220444,
      "grad_norm": 14.053345680236816,
      "learning_rate": 1.1274535461920964e-05,
      "loss": 1.1607,
      "step": 232
    },
    {
      "epoch": 0.006098731394070533,
      "grad_norm": 12.142253875732422,
      "learning_rate": 1.127139492279508e-05,
      "loss": 1.0286,
      "step": 233
    },
    {
      "epoch": 0.006124906206920621,
      "grad_norm": 13.495305061340332,
      "learning_rate": 1.1268254383669197e-05,
      "loss": 1.8786,
      "step": 234
    },
    {
      "epoch": 0.006151081019770709,
      "grad_norm": 15.427663803100586,
      "learning_rate": 1.1265113844543313e-05,
      "loss": 1.2116,
      "step": 235
    },
    {
      "epoch": 0.0061772558326207965,
      "grad_norm": 16.25979995727539,
      "learning_rate": 1.126197330541743e-05,
      "loss": 2.0223,
      "step": 236
    },
    {
      "epoch": 0.006203430645470885,
      "grad_norm": 16.369291305541992,
      "learning_rate": 1.1258832766291548e-05,
      "loss": 3.3264,
      "step": 237
    },
    {
      "epoch": 0.006229605458320973,
      "grad_norm": 10.123537063598633,
      "learning_rate": 1.1255692227165664e-05,
      "loss": 1.6257,
      "step": 238
    },
    {
      "epoch": 0.006255780271171061,
      "grad_norm": 12.045762062072754,
      "learning_rate": 1.1252551688039781e-05,
      "loss": 1.8353,
      "step": 239
    },
    {
      "epoch": 0.006281955084021149,
      "grad_norm": 12.249852180480957,
      "learning_rate": 1.1249411148913897e-05,
      "loss": 1.5886,
      "step": 240
    },
    {
      "epoch": 0.006308129896871237,
      "grad_norm": 21.565555572509766,
      "learning_rate": 1.1246270609788013e-05,
      "loss": 1.772,
      "step": 241
    },
    {
      "epoch": 0.006334304709721326,
      "grad_norm": 14.761540412902832,
      "learning_rate": 1.124313007066213e-05,
      "loss": 2.3544,
      "step": 242
    },
    {
      "epoch": 0.006360479522571414,
      "grad_norm": 14.064984321594238,
      "learning_rate": 1.1239989531536247e-05,
      "loss": 0.4517,
      "step": 243
    },
    {
      "epoch": 0.006386654335421502,
      "grad_norm": 15.47919750213623,
      "learning_rate": 1.1236848992410364e-05,
      "loss": 1.3596,
      "step": 244
    },
    {
      "epoch": 0.0064128291482715895,
      "grad_norm": 15.840648651123047,
      "learning_rate": 1.123370845328448e-05,
      "loss": 1.7255,
      "step": 245
    },
    {
      "epoch": 0.006439003961121678,
      "grad_norm": 13.832386016845703,
      "learning_rate": 1.1230567914158598e-05,
      "loss": 2.2063,
      "step": 246
    },
    {
      "epoch": 0.006465178773971766,
      "grad_norm": 14.32754135131836,
      "learning_rate": 1.1227427375032713e-05,
      "loss": 1.3076,
      "step": 247
    },
    {
      "epoch": 0.006491353586821854,
      "grad_norm": 13.14931869506836,
      "learning_rate": 1.1224286835906831e-05,
      "loss": 2.1726,
      "step": 248
    },
    {
      "epoch": 0.006517528399671942,
      "grad_norm": 18.986228942871094,
      "learning_rate": 1.1221146296780949e-05,
      "loss": 1.9514,
      "step": 249
    },
    {
      "epoch": 0.006543703212522031,
      "grad_norm": 18.050270080566406,
      "learning_rate": 1.1218005757655064e-05,
      "loss": 2.3808,
      "step": 250
    },
    {
      "epoch": 0.006569878025372119,
      "grad_norm": 12.211296081542969,
      "learning_rate": 1.1214865218529182e-05,
      "loss": 2.079,
      "step": 251
    },
    {
      "epoch": 0.006596052838222207,
      "grad_norm": 22.59772491455078,
      "learning_rate": 1.1211724679403298e-05,
      "loss": 1.7225,
      "step": 252
    },
    {
      "epoch": 0.006622227651072295,
      "grad_norm": 16.774682998657227,
      "learning_rate": 1.1208584140277415e-05,
      "loss": 1.1822,
      "step": 253
    },
    {
      "epoch": 0.006648402463922383,
      "grad_norm": 14.39283275604248,
      "learning_rate": 1.1205443601151531e-05,
      "loss": 3.0094,
      "step": 254
    },
    {
      "epoch": 0.006674577276772471,
      "grad_norm": 12.797060012817383,
      "learning_rate": 1.1202303062025649e-05,
      "loss": 1.1131,
      "step": 255
    },
    {
      "epoch": 0.006700752089622559,
      "grad_norm": 11.716316223144531,
      "learning_rate": 1.1199162522899765e-05,
      "loss": 0.7453,
      "step": 256
    },
    {
      "epoch": 0.006726926902472647,
      "grad_norm": 17.063180923461914,
      "learning_rate": 1.119602198377388e-05,
      "loss": 2.3483,
      "step": 257
    },
    {
      "epoch": 0.006753101715322735,
      "grad_norm": 11.267877578735352,
      "learning_rate": 1.1192881444647998e-05,
      "loss": 1.8233,
      "step": 258
    },
    {
      "epoch": 0.006779276528172824,
      "grad_norm": 10.438720703125,
      "learning_rate": 1.1189740905522114e-05,
      "loss": 1.3407,
      "step": 259
    },
    {
      "epoch": 0.006805451341022912,
      "grad_norm": 13.037179946899414,
      "learning_rate": 1.1186600366396231e-05,
      "loss": 1.2294,
      "step": 260
    },
    {
      "epoch": 0.006831626153873,
      "grad_norm": 14.77631664276123,
      "learning_rate": 1.1183459827270347e-05,
      "loss": 2.1465,
      "step": 261
    },
    {
      "epoch": 0.006857800966723088,
      "grad_norm": 12.245406150817871,
      "learning_rate": 1.1180319288144465e-05,
      "loss": 0.8685,
      "step": 262
    },
    {
      "epoch": 0.006883975779573176,
      "grad_norm": 17.907636642456055,
      "learning_rate": 1.1177178749018582e-05,
      "loss": 2.0669,
      "step": 263
    },
    {
      "epoch": 0.006910150592423264,
      "grad_norm": 13.983388900756836,
      "learning_rate": 1.1174038209892698e-05,
      "loss": 1.9092,
      "step": 264
    },
    {
      "epoch": 0.006936325405273352,
      "grad_norm": 15.62343692779541,
      "learning_rate": 1.1170897670766816e-05,
      "loss": 1.1995,
      "step": 265
    },
    {
      "epoch": 0.00696250021812344,
      "grad_norm": 9.424201011657715,
      "learning_rate": 1.1167757131640932e-05,
      "loss": 1.1862,
      "step": 266
    },
    {
      "epoch": 0.006988675030973528,
      "grad_norm": 14.142909049987793,
      "learning_rate": 1.116461659251505e-05,
      "loss": 1.8591,
      "step": 267
    },
    {
      "epoch": 0.007014849843823617,
      "grad_norm": 12.401169776916504,
      "learning_rate": 1.1161476053389165e-05,
      "loss": 1.1515,
      "step": 268
    },
    {
      "epoch": 0.007041024656673705,
      "grad_norm": 18.058483123779297,
      "learning_rate": 1.1158335514263283e-05,
      "loss": 1.1207,
      "step": 269
    },
    {
      "epoch": 0.007067199469523793,
      "grad_norm": 20.262357711791992,
      "learning_rate": 1.11551949751374e-05,
      "loss": 2.8084,
      "step": 270
    },
    {
      "epoch": 0.007093374282373881,
      "grad_norm": 14.646797180175781,
      "learning_rate": 1.1152054436011516e-05,
      "loss": 1.5655,
      "step": 271
    },
    {
      "epoch": 0.0071195490952239695,
      "grad_norm": 13.775413513183594,
      "learning_rate": 1.1148913896885632e-05,
      "loss": 1.8068,
      "step": 272
    },
    {
      "epoch": 0.0071457239080740574,
      "grad_norm": 14.789746284484863,
      "learning_rate": 1.1145773357759748e-05,
      "loss": 1.3959,
      "step": 273
    },
    {
      "epoch": 0.007171898720924145,
      "grad_norm": 18.808486938476562,
      "learning_rate": 1.1142632818633865e-05,
      "loss": 1.3654,
      "step": 274
    },
    {
      "epoch": 0.007198073533774233,
      "grad_norm": 12.5117826461792,
      "learning_rate": 1.1139492279507983e-05,
      "loss": 1.236,
      "step": 275
    },
    {
      "epoch": 0.007224248346624321,
      "grad_norm": 14.488447189331055,
      "learning_rate": 1.1136351740382099e-05,
      "loss": 2.2505,
      "step": 276
    },
    {
      "epoch": 0.00725042315947441,
      "grad_norm": 14.731329917907715,
      "learning_rate": 1.1133211201256216e-05,
      "loss": 2.1046,
      "step": 277
    },
    {
      "epoch": 0.007276597972324498,
      "grad_norm": 17.36949920654297,
      "learning_rate": 1.1130070662130332e-05,
      "loss": 1.9663,
      "step": 278
    },
    {
      "epoch": 0.007302772785174586,
      "grad_norm": 15.755168914794922,
      "learning_rate": 1.112693012300445e-05,
      "loss": 1.1911,
      "step": 279
    },
    {
      "epoch": 0.007328947598024674,
      "grad_norm": 19.237802505493164,
      "learning_rate": 1.1123789583878566e-05,
      "loss": 0.9595,
      "step": 280
    },
    {
      "epoch": 0.0073551224108747626,
      "grad_norm": 15.116825103759766,
      "learning_rate": 1.1120649044752683e-05,
      "loss": 1.5026,
      "step": 281
    },
    {
      "epoch": 0.0073812972237248505,
      "grad_norm": 13.805624008178711,
      "learning_rate": 1.1117508505626799e-05,
      "loss": 2.2041,
      "step": 282
    },
    {
      "epoch": 0.007407472036574938,
      "grad_norm": 23.11675453186035,
      "learning_rate": 1.1114367966500917e-05,
      "loss": 2.1789,
      "step": 283
    },
    {
      "epoch": 0.007433646849425026,
      "grad_norm": 18.663558959960938,
      "learning_rate": 1.1111227427375034e-05,
      "loss": 1.7239,
      "step": 284
    },
    {
      "epoch": 0.007459821662275115,
      "grad_norm": 10.9806547164917,
      "learning_rate": 1.110808688824915e-05,
      "loss": 0.6113,
      "step": 285
    },
    {
      "epoch": 0.007485996475125203,
      "grad_norm": 10.12496566772461,
      "learning_rate": 1.1104946349123268e-05,
      "loss": 1.1653,
      "step": 286
    },
    {
      "epoch": 0.007512171287975291,
      "grad_norm": 13.140800476074219,
      "learning_rate": 1.1101805809997383e-05,
      "loss": 0.5907,
      "step": 287
    },
    {
      "epoch": 0.007538346100825379,
      "grad_norm": 19.366743087768555,
      "learning_rate": 1.10986652708715e-05,
      "loss": 0.9592,
      "step": 288
    },
    {
      "epoch": 0.007564520913675467,
      "grad_norm": 12.549296379089355,
      "learning_rate": 1.1095524731745617e-05,
      "loss": 2.4665,
      "step": 289
    },
    {
      "epoch": 0.007590695726525556,
      "grad_norm": 16.548173904418945,
      "learning_rate": 1.1092384192619733e-05,
      "loss": 1.3756,
      "step": 290
    },
    {
      "epoch": 0.0076168705393756435,
      "grad_norm": 12.961524963378906,
      "learning_rate": 1.108924365349385e-05,
      "loss": 1.2124,
      "step": 291
    },
    {
      "epoch": 0.0076430453522257314,
      "grad_norm": 17.963848114013672,
      "learning_rate": 1.1086103114367966e-05,
      "loss": 2.061,
      "step": 292
    },
    {
      "epoch": 0.007669220165075819,
      "grad_norm": 19.636781692504883,
      "learning_rate": 1.1082962575242084e-05,
      "loss": 1.3615,
      "step": 293
    },
    {
      "epoch": 0.007695394977925908,
      "grad_norm": 15.720267295837402,
      "learning_rate": 1.10798220361162e-05,
      "loss": 1.5583,
      "step": 294
    },
    {
      "epoch": 0.007721569790775996,
      "grad_norm": 15.742332458496094,
      "learning_rate": 1.1076681496990317e-05,
      "loss": 1.7538,
      "step": 295
    },
    {
      "epoch": 0.007747744603626084,
      "grad_norm": 19.154739379882812,
      "learning_rate": 1.1073540957864435e-05,
      "loss": 1.7403,
      "step": 296
    },
    {
      "epoch": 0.007773919416476172,
      "grad_norm": 23.00737953186035,
      "learning_rate": 1.107040041873855e-05,
      "loss": 1.6915,
      "step": 297
    },
    {
      "epoch": 0.007800094229326261,
      "grad_norm": 15.868215560913086,
      "learning_rate": 1.1067259879612668e-05,
      "loss": 1.1833,
      "step": 298
    },
    {
      "epoch": 0.007826269042176348,
      "grad_norm": 22.073097229003906,
      "learning_rate": 1.1064119340486784e-05,
      "loss": 1.1888,
      "step": 299
    },
    {
      "epoch": 0.007852443855026437,
      "grad_norm": 11.909801483154297,
      "learning_rate": 1.1060978801360901e-05,
      "loss": 1.605,
      "step": 300
    },
    {
      "epoch": 0.007878618667876525,
      "grad_norm": 16.42983627319336,
      "learning_rate": 1.1057838262235017e-05,
      "loss": 1.3688,
      "step": 301
    },
    {
      "epoch": 0.007904793480726613,
      "grad_norm": 19.567707061767578,
      "learning_rate": 1.1054697723109135e-05,
      "loss": 1.2214,
      "step": 302
    },
    {
      "epoch": 0.007930968293576701,
      "grad_norm": 11.74425220489502,
      "learning_rate": 1.105155718398325e-05,
      "loss": 0.7944,
      "step": 303
    },
    {
      "epoch": 0.007957143106426789,
      "grad_norm": 14.515366554260254,
      "learning_rate": 1.1048416644857367e-05,
      "loss": 1.0281,
      "step": 304
    },
    {
      "epoch": 0.007983317919276877,
      "grad_norm": 20.552743911743164,
      "learning_rate": 1.1045276105731484e-05,
      "loss": 1.5882,
      "step": 305
    },
    {
      "epoch": 0.008009492732126965,
      "grad_norm": 21.663604736328125,
      "learning_rate": 1.10421355666056e-05,
      "loss": 1.8039,
      "step": 306
    },
    {
      "epoch": 0.008035667544977053,
      "grad_norm": 20.33736228942871,
      "learning_rate": 1.1038995027479717e-05,
      "loss": 1.1212,
      "step": 307
    },
    {
      "epoch": 0.00806184235782714,
      "grad_norm": 14.70441722869873,
      "learning_rate": 1.1035854488353833e-05,
      "loss": 2.4091,
      "step": 308
    },
    {
      "epoch": 0.00808801717067723,
      "grad_norm": 14.672015190124512,
      "learning_rate": 1.1032713949227951e-05,
      "loss": 2.106,
      "step": 309
    },
    {
      "epoch": 0.008114191983527318,
      "grad_norm": 14.566289901733398,
      "learning_rate": 1.1029573410102068e-05,
      "loss": 2.0872,
      "step": 310
    },
    {
      "epoch": 0.008140366796377406,
      "grad_norm": 15.018850326538086,
      "learning_rate": 1.1026432870976184e-05,
      "loss": 2.0676,
      "step": 311
    },
    {
      "epoch": 0.008166541609227494,
      "grad_norm": 18.827930450439453,
      "learning_rate": 1.1023292331850302e-05,
      "loss": 1.7748,
      "step": 312
    },
    {
      "epoch": 0.008192716422077582,
      "grad_norm": 14.074217796325684,
      "learning_rate": 1.1020151792724418e-05,
      "loss": 2.1583,
      "step": 313
    },
    {
      "epoch": 0.00821889123492767,
      "grad_norm": 10.189136505126953,
      "learning_rate": 1.1017011253598535e-05,
      "loss": 1.6949,
      "step": 314
    },
    {
      "epoch": 0.008245066047777758,
      "grad_norm": 18.352317810058594,
      "learning_rate": 1.1013870714472651e-05,
      "loss": 0.4191,
      "step": 315
    },
    {
      "epoch": 0.008271240860627846,
      "grad_norm": 15.7222318649292,
      "learning_rate": 1.1010730175346769e-05,
      "loss": 0.5871,
      "step": 316
    },
    {
      "epoch": 0.008297415673477934,
      "grad_norm": 13.767792701721191,
      "learning_rate": 1.1007589636220886e-05,
      "loss": 1.2207,
      "step": 317
    },
    {
      "epoch": 0.008323590486328023,
      "grad_norm": 12.374411582946777,
      "learning_rate": 1.1004449097095002e-05,
      "loss": 1.0025,
      "step": 318
    },
    {
      "epoch": 0.008349765299178111,
      "grad_norm": 13.591221809387207,
      "learning_rate": 1.1001308557969118e-05,
      "loss": 1.5102,
      "step": 319
    },
    {
      "epoch": 0.0083759401120282,
      "grad_norm": 16.649709701538086,
      "learning_rate": 1.0998168018843234e-05,
      "loss": 0.6206,
      "step": 320
    },
    {
      "epoch": 0.008402114924878287,
      "grad_norm": 11.719310760498047,
      "learning_rate": 1.0995027479717351e-05,
      "loss": 1.6413,
      "step": 321
    },
    {
      "epoch": 0.008428289737728375,
      "grad_norm": 16.285432815551758,
      "learning_rate": 1.0991886940591469e-05,
      "loss": 2.1585,
      "step": 322
    },
    {
      "epoch": 0.008454464550578463,
      "grad_norm": 16.335044860839844,
      "learning_rate": 1.0988746401465585e-05,
      "loss": 1.5047,
      "step": 323
    },
    {
      "epoch": 0.008480639363428551,
      "grad_norm": 30.429353713989258,
      "learning_rate": 1.0985605862339702e-05,
      "loss": 0.864,
      "step": 324
    },
    {
      "epoch": 0.008506814176278639,
      "grad_norm": 22.31454086303711,
      "learning_rate": 1.0982465323213818e-05,
      "loss": 1.1125,
      "step": 325
    },
    {
      "epoch": 0.008532988989128727,
      "grad_norm": 17.5647029876709,
      "learning_rate": 1.0979324784087936e-05,
      "loss": 1.7887,
      "step": 326
    },
    {
      "epoch": 0.008559163801978817,
      "grad_norm": 15.903203964233398,
      "learning_rate": 1.0976184244962052e-05,
      "loss": 1.343,
      "step": 327
    },
    {
      "epoch": 0.008585338614828904,
      "grad_norm": 18.91958999633789,
      "learning_rate": 1.097304370583617e-05,
      "loss": 1.2594,
      "step": 328
    },
    {
      "epoch": 0.008611513427678992,
      "grad_norm": 14.548200607299805,
      "learning_rate": 1.0969903166710285e-05,
      "loss": 0.9094,
      "step": 329
    },
    {
      "epoch": 0.00863768824052908,
      "grad_norm": 11.368436813354492,
      "learning_rate": 1.0966762627584403e-05,
      "loss": 1.8223,
      "step": 330
    },
    {
      "epoch": 0.008663863053379168,
      "grad_norm": 11.939844131469727,
      "learning_rate": 1.096362208845852e-05,
      "loss": 0.6597,
      "step": 331
    },
    {
      "epoch": 0.008690037866229256,
      "grad_norm": 17.506649017333984,
      "learning_rate": 1.0960481549332636e-05,
      "loss": 1.6236,
      "step": 332
    },
    {
      "epoch": 0.008716212679079344,
      "grad_norm": 17.037145614624023,
      "learning_rate": 1.0957341010206754e-05,
      "loss": 2.3782,
      "step": 333
    },
    {
      "epoch": 0.008742387491929432,
      "grad_norm": 20.199565887451172,
      "learning_rate": 1.0954200471080868e-05,
      "loss": 0.6111,
      "step": 334
    },
    {
      "epoch": 0.008768562304779522,
      "grad_norm": 15.771493911743164,
      "learning_rate": 1.0951059931954985e-05,
      "loss": 1.3612,
      "step": 335
    },
    {
      "epoch": 0.00879473711762961,
      "grad_norm": 13.405550956726074,
      "learning_rate": 1.0947919392829103e-05,
      "loss": 2.2104,
      "step": 336
    },
    {
      "epoch": 0.008820911930479697,
      "grad_norm": 13.113595008850098,
      "learning_rate": 1.0944778853703219e-05,
      "loss": 1.4872,
      "step": 337
    },
    {
      "epoch": 0.008847086743329785,
      "grad_norm": 13.193557739257812,
      "learning_rate": 1.0941638314577336e-05,
      "loss": 1.818,
      "step": 338
    },
    {
      "epoch": 0.008873261556179873,
      "grad_norm": 22.578781127929688,
      "learning_rate": 1.0938497775451452e-05,
      "loss": 1.2875,
      "step": 339
    },
    {
      "epoch": 0.008899436369029961,
      "grad_norm": 10.925480842590332,
      "learning_rate": 1.093535723632557e-05,
      "loss": 1.06,
      "step": 340
    },
    {
      "epoch": 0.00892561118188005,
      "grad_norm": 16.95890998840332,
      "learning_rate": 1.0932216697199686e-05,
      "loss": 2.0116,
      "step": 341
    },
    {
      "epoch": 0.008951785994730137,
      "grad_norm": 20.626190185546875,
      "learning_rate": 1.0929076158073803e-05,
      "loss": 1.916,
      "step": 342
    },
    {
      "epoch": 0.008977960807580225,
      "grad_norm": 14.161541938781738,
      "learning_rate": 1.092593561894792e-05,
      "loss": 1.8118,
      "step": 343
    },
    {
      "epoch": 0.009004135620430315,
      "grad_norm": 17.72511100769043,
      "learning_rate": 1.0922795079822036e-05,
      "loss": 2.0665,
      "step": 344
    },
    {
      "epoch": 0.009030310433280403,
      "grad_norm": 17.07662582397461,
      "learning_rate": 1.0919654540696154e-05,
      "loss": 1.3605,
      "step": 345
    },
    {
      "epoch": 0.00905648524613049,
      "grad_norm": 19.506410598754883,
      "learning_rate": 1.091651400157027e-05,
      "loss": 0.4042,
      "step": 346
    },
    {
      "epoch": 0.009082660058980578,
      "grad_norm": 14.735677719116211,
      "learning_rate": 1.0913373462444387e-05,
      "loss": 1.6496,
      "step": 347
    },
    {
      "epoch": 0.009108834871830666,
      "grad_norm": 16.169328689575195,
      "learning_rate": 1.0910232923318503e-05,
      "loss": 2.5778,
      "step": 348
    },
    {
      "epoch": 0.009135009684680754,
      "grad_norm": 18.068321228027344,
      "learning_rate": 1.090709238419262e-05,
      "loss": 1.8639,
      "step": 349
    },
    {
      "epoch": 0.009161184497530842,
      "grad_norm": 25.43743324279785,
      "learning_rate": 1.0903951845066737e-05,
      "loss": 1.4617,
      "step": 350
    },
    {
      "epoch": 0.00918735931038093,
      "grad_norm": 15.793428421020508,
      "learning_rate": 1.0900811305940853e-05,
      "loss": 1.403,
      "step": 351
    },
    {
      "epoch": 0.009213534123231018,
      "grad_norm": 17.325904846191406,
      "learning_rate": 1.089767076681497e-05,
      "loss": 2.0604,
      "step": 352
    },
    {
      "epoch": 0.009239708936081108,
      "grad_norm": 15.80048942565918,
      "learning_rate": 1.0894530227689086e-05,
      "loss": 2.7016,
      "step": 353
    },
    {
      "epoch": 0.009265883748931196,
      "grad_norm": 10.43928337097168,
      "learning_rate": 1.0891389688563204e-05,
      "loss": 2.0932,
      "step": 354
    },
    {
      "epoch": 0.009292058561781284,
      "grad_norm": 11.128812789916992,
      "learning_rate": 1.088824914943732e-05,
      "loss": 0.7772,
      "step": 355
    },
    {
      "epoch": 0.009318233374631371,
      "grad_norm": 9.787101745605469,
      "learning_rate": 1.0885108610311437e-05,
      "loss": 1.9128,
      "step": 356
    },
    {
      "epoch": 0.00934440818748146,
      "grad_norm": 14.501033782958984,
      "learning_rate": 1.0881968071185554e-05,
      "loss": 1.8565,
      "step": 357
    },
    {
      "epoch": 0.009370583000331547,
      "grad_norm": 20.69178009033203,
      "learning_rate": 1.087882753205967e-05,
      "loss": 1.2296,
      "step": 358
    },
    {
      "epoch": 0.009396757813181635,
      "grad_norm": 15.595779418945312,
      "learning_rate": 1.0875686992933788e-05,
      "loss": 1.791,
      "step": 359
    },
    {
      "epoch": 0.009422932626031723,
      "grad_norm": 11.90365219116211,
      "learning_rate": 1.0872546453807904e-05,
      "loss": 1.906,
      "step": 360
    },
    {
      "epoch": 0.009449107438881813,
      "grad_norm": 16.038898468017578,
      "learning_rate": 1.0869405914682021e-05,
      "loss": 2.1006,
      "step": 361
    },
    {
      "epoch": 0.0094752822517319,
      "grad_norm": 15.617024421691895,
      "learning_rate": 1.0866265375556137e-05,
      "loss": 1.6754,
      "step": 362
    },
    {
      "epoch": 0.009501457064581989,
      "grad_norm": 16.19228744506836,
      "learning_rate": 1.0863124836430255e-05,
      "loss": 1.9961,
      "step": 363
    },
    {
      "epoch": 0.009527631877432077,
      "grad_norm": 12.502939224243164,
      "learning_rate": 1.0859984297304372e-05,
      "loss": 1.0281,
      "step": 364
    },
    {
      "epoch": 0.009553806690282165,
      "grad_norm": 15.915958404541016,
      "learning_rate": 1.0856843758178486e-05,
      "loss": 0.6828,
      "step": 365
    },
    {
      "epoch": 0.009579981503132252,
      "grad_norm": 16.778366088867188,
      "learning_rate": 1.0853703219052604e-05,
      "loss": 3.2747,
      "step": 366
    },
    {
      "epoch": 0.00960615631598234,
      "grad_norm": 13.121718406677246,
      "learning_rate": 1.085056267992672e-05,
      "loss": 1.331,
      "step": 367
    },
    {
      "epoch": 0.009632331128832428,
      "grad_norm": 14.476509094238281,
      "learning_rate": 1.0847422140800837e-05,
      "loss": 2.5347,
      "step": 368
    },
    {
      "epoch": 0.009658505941682516,
      "grad_norm": 10.666698455810547,
      "learning_rate": 1.0844281601674955e-05,
      "loss": 0.6903,
      "step": 369
    },
    {
      "epoch": 0.009684680754532606,
      "grad_norm": 11.608527183532715,
      "learning_rate": 1.084114106254907e-05,
      "loss": 1.458,
      "step": 370
    },
    {
      "epoch": 0.009710855567382694,
      "grad_norm": 18.27562713623047,
      "learning_rate": 1.0838000523423188e-05,
      "loss": 2.6017,
      "step": 371
    },
    {
      "epoch": 0.009737030380232782,
      "grad_norm": 9.951774597167969,
      "learning_rate": 1.0834859984297304e-05,
      "loss": 1.1349,
      "step": 372
    },
    {
      "epoch": 0.00976320519308287,
      "grad_norm": 12.113003730773926,
      "learning_rate": 1.0831719445171422e-05,
      "loss": 0.9292,
      "step": 373
    },
    {
      "epoch": 0.009789380005932958,
      "grad_norm": 13.29439926147461,
      "learning_rate": 1.0828578906045538e-05,
      "loss": 0.463,
      "step": 374
    },
    {
      "epoch": 0.009815554818783045,
      "grad_norm": 12.57712459564209,
      "learning_rate": 1.0825438366919655e-05,
      "loss": 1.6228,
      "step": 375
    },
    {
      "epoch": 0.009841729631633133,
      "grad_norm": 17.21816062927246,
      "learning_rate": 1.0822297827793771e-05,
      "loss": 1.159,
      "step": 376
    },
    {
      "epoch": 0.009867904444483221,
      "grad_norm": 13.877412796020508,
      "learning_rate": 1.0819157288667889e-05,
      "loss": 1.415,
      "step": 377
    },
    {
      "epoch": 0.00989407925733331,
      "grad_norm": 14.684090614318848,
      "learning_rate": 1.0816016749542006e-05,
      "loss": 1.9764,
      "step": 378
    },
    {
      "epoch": 0.009920254070183399,
      "grad_norm": 13.613813400268555,
      "learning_rate": 1.0812876210416122e-05,
      "loss": 1.2159,
      "step": 379
    },
    {
      "epoch": 0.009946428883033487,
      "grad_norm": 11.879084587097168,
      "learning_rate": 1.080973567129024e-05,
      "loss": 1.1953,
      "step": 380
    },
    {
      "epoch": 0.009972603695883575,
      "grad_norm": 15.752952575683594,
      "learning_rate": 1.0806595132164354e-05,
      "loss": 0.5855,
      "step": 381
    },
    {
      "epoch": 0.009998778508733663,
      "grad_norm": 20.031707763671875,
      "learning_rate": 1.0803454593038471e-05,
      "loss": 0.705,
      "step": 382
    },
    {
      "epoch": 0.01002495332158375,
      "grad_norm": 14.504175186157227,
      "learning_rate": 1.0800314053912589e-05,
      "loss": 1.343,
      "step": 383
    },
    {
      "epoch": 0.010051128134433839,
      "grad_norm": 17.33380699157715,
      "learning_rate": 1.0797173514786705e-05,
      "loss": 1.6279,
      "step": 384
    },
    {
      "epoch": 0.010077302947283926,
      "grad_norm": 14.024443626403809,
      "learning_rate": 1.0794032975660822e-05,
      "loss": 1.2053,
      "step": 385
    },
    {
      "epoch": 0.010103477760134014,
      "grad_norm": 16.3476505279541,
      "learning_rate": 1.0790892436534938e-05,
      "loss": 0.8981,
      "step": 386
    },
    {
      "epoch": 0.010129652572984104,
      "grad_norm": 13.890003204345703,
      "learning_rate": 1.0787751897409056e-05,
      "loss": 1.7365,
      "step": 387
    },
    {
      "epoch": 0.010155827385834192,
      "grad_norm": 15.620462417602539,
      "learning_rate": 1.0784611358283172e-05,
      "loss": 0.8853,
      "step": 388
    },
    {
      "epoch": 0.01018200219868428,
      "grad_norm": 18.737411499023438,
      "learning_rate": 1.0781470819157289e-05,
      "loss": 1.7459,
      "step": 389
    },
    {
      "epoch": 0.010208177011534368,
      "grad_norm": 15.257023811340332,
      "learning_rate": 1.0778330280031407e-05,
      "loss": 1.618,
      "step": 390
    },
    {
      "epoch": 0.010234351824384456,
      "grad_norm": 16.263566970825195,
      "learning_rate": 1.0775189740905523e-05,
      "loss": 1.8461,
      "step": 391
    },
    {
      "epoch": 0.010260526637234544,
      "grad_norm": 18.674882888793945,
      "learning_rate": 1.077204920177964e-05,
      "loss": 0.8709,
      "step": 392
    },
    {
      "epoch": 0.010286701450084632,
      "grad_norm": 12.205535888671875,
      "learning_rate": 1.0768908662653756e-05,
      "loss": 0.3403,
      "step": 393
    },
    {
      "epoch": 0.01031287626293472,
      "grad_norm": 32.3964958190918,
      "learning_rate": 1.0765768123527873e-05,
      "loss": 1.6504,
      "step": 394
    },
    {
      "epoch": 0.010339051075784807,
      "grad_norm": 17.43682098388672,
      "learning_rate": 1.076262758440199e-05,
      "loss": 0.5355,
      "step": 395
    },
    {
      "epoch": 0.010365225888634897,
      "grad_norm": 15.280424118041992,
      "learning_rate": 1.0759487045276105e-05,
      "loss": 1.1856,
      "step": 396
    },
    {
      "epoch": 0.010391400701484985,
      "grad_norm": 14.801470756530762,
      "learning_rate": 1.0756346506150223e-05,
      "loss": 1.2562,
      "step": 397
    },
    {
      "epoch": 0.010417575514335073,
      "grad_norm": 24.957561492919922,
      "learning_rate": 1.0753205967024339e-05,
      "loss": 0.8216,
      "step": 398
    },
    {
      "epoch": 0.01044375032718516,
      "grad_norm": 18.14193344116211,
      "learning_rate": 1.0750065427898456e-05,
      "loss": 1.7284,
      "step": 399
    },
    {
      "epoch": 0.010469925140035249,
      "grad_norm": 20.479413986206055,
      "learning_rate": 1.0746924888772572e-05,
      "loss": 1.6653,
      "step": 400
    },
    {
      "epoch": 0.010496099952885337,
      "grad_norm": 21.713754653930664,
      "learning_rate": 1.074378434964669e-05,
      "loss": 1.5809,
      "step": 401
    },
    {
      "epoch": 0.010522274765735425,
      "grad_norm": 26.573183059692383,
      "learning_rate": 1.0740643810520805e-05,
      "loss": 0.7763,
      "step": 402
    },
    {
      "epoch": 0.010548449578585513,
      "grad_norm": 21.182371139526367,
      "learning_rate": 1.0737503271394923e-05,
      "loss": 2.2911,
      "step": 403
    },
    {
      "epoch": 0.0105746243914356,
      "grad_norm": 15.674162864685059,
      "learning_rate": 1.073436273226904e-05,
      "loss": 1.6113,
      "step": 404
    },
    {
      "epoch": 0.01060079920428569,
      "grad_norm": 20.34441375732422,
      "learning_rate": 1.0731222193143156e-05,
      "loss": 1.9567,
      "step": 405
    },
    {
      "epoch": 0.010626974017135778,
      "grad_norm": 15.73863410949707,
      "learning_rate": 1.0728081654017274e-05,
      "loss": 1.2086,
      "step": 406
    },
    {
      "epoch": 0.010653148829985866,
      "grad_norm": 18.402544021606445,
      "learning_rate": 1.072494111489139e-05,
      "loss": 2.7246,
      "step": 407
    },
    {
      "epoch": 0.010679323642835954,
      "grad_norm": 18.604225158691406,
      "learning_rate": 1.0721800575765507e-05,
      "loss": 2.0139,
      "step": 408
    },
    {
      "epoch": 0.010705498455686042,
      "grad_norm": 11.0678129196167,
      "learning_rate": 1.0718660036639623e-05,
      "loss": 0.6523,
      "step": 409
    },
    {
      "epoch": 0.01073167326853613,
      "grad_norm": 17.70045280456543,
      "learning_rate": 1.071551949751374e-05,
      "loss": 1.9054,
      "step": 410
    },
    {
      "epoch": 0.010757848081386218,
      "grad_norm": 20.07218360900879,
      "learning_rate": 1.0712378958387857e-05,
      "loss": 0.5899,
      "step": 411
    },
    {
      "epoch": 0.010784022894236306,
      "grad_norm": 11.439613342285156,
      "learning_rate": 1.0709238419261972e-05,
      "loss": 1.6186,
      "step": 412
    },
    {
      "epoch": 0.010810197707086395,
      "grad_norm": 15.654707908630371,
      "learning_rate": 1.070609788013609e-05,
      "loss": 2.0962,
      "step": 413
    },
    {
      "epoch": 0.010836372519936483,
      "grad_norm": 13.228864669799805,
      "learning_rate": 1.0702957341010206e-05,
      "loss": 1.4326,
      "step": 414
    },
    {
      "epoch": 0.010862547332786571,
      "grad_norm": 18.476831436157227,
      "learning_rate": 1.0699816801884323e-05,
      "loss": 1.8706,
      "step": 415
    },
    {
      "epoch": 0.010888722145636659,
      "grad_norm": 17.671348571777344,
      "learning_rate": 1.0696676262758441e-05,
      "loss": 0.9268,
      "step": 416
    },
    {
      "epoch": 0.010914896958486747,
      "grad_norm": 14.055368423461914,
      "learning_rate": 1.0693535723632557e-05,
      "loss": 1.9308,
      "step": 417
    },
    {
      "epoch": 0.010941071771336835,
      "grad_norm": 21.15837860107422,
      "learning_rate": 1.0690395184506674e-05,
      "loss": 1.7632,
      "step": 418
    },
    {
      "epoch": 0.010967246584186923,
      "grad_norm": 13.882519721984863,
      "learning_rate": 1.068725464538079e-05,
      "loss": 2.4609,
      "step": 419
    },
    {
      "epoch": 0.01099342139703701,
      "grad_norm": 12.520979881286621,
      "learning_rate": 1.0684114106254908e-05,
      "loss": 1.5518,
      "step": 420
    },
    {
      "epoch": 0.011019596209887099,
      "grad_norm": 13.968025207519531,
      "learning_rate": 1.0680973567129024e-05,
      "loss": 0.2303,
      "step": 421
    },
    {
      "epoch": 0.011045771022737188,
      "grad_norm": 25.262121200561523,
      "learning_rate": 1.0677833028003141e-05,
      "loss": 1.2172,
      "step": 422
    },
    {
      "epoch": 0.011071945835587276,
      "grad_norm": 17.831470489501953,
      "learning_rate": 1.0674692488877257e-05,
      "loss": 1.6826,
      "step": 423
    },
    {
      "epoch": 0.011098120648437364,
      "grad_norm": 17.561426162719727,
      "learning_rate": 1.0671551949751375e-05,
      "loss": 1.532,
      "step": 424
    },
    {
      "epoch": 0.011124295461287452,
      "grad_norm": 11.379362106323242,
      "learning_rate": 1.0668411410625492e-05,
      "loss": 1.1711,
      "step": 425
    },
    {
      "epoch": 0.01115047027413754,
      "grad_norm": 14.574835777282715,
      "learning_rate": 1.0665270871499608e-05,
      "loss": 0.9812,
      "step": 426
    },
    {
      "epoch": 0.011176645086987628,
      "grad_norm": 26.660511016845703,
      "learning_rate": 1.0662130332373724e-05,
      "loss": 1.0162,
      "step": 427
    },
    {
      "epoch": 0.011202819899837716,
      "grad_norm": 14.806595802307129,
      "learning_rate": 1.065898979324784e-05,
      "loss": 1.7548,
      "step": 428
    },
    {
      "epoch": 0.011228994712687804,
      "grad_norm": 12.115906715393066,
      "learning_rate": 1.0655849254121957e-05,
      "loss": 0.5802,
      "step": 429
    },
    {
      "epoch": 0.011255169525537892,
      "grad_norm": 11.633270263671875,
      "learning_rate": 1.0652708714996075e-05,
      "loss": 0.983,
      "step": 430
    },
    {
      "epoch": 0.011281344338387981,
      "grad_norm": 15.43632698059082,
      "learning_rate": 1.064956817587019e-05,
      "loss": 1.6864,
      "step": 431
    },
    {
      "epoch": 0.01130751915123807,
      "grad_norm": 17.35012435913086,
      "learning_rate": 1.0646427636744308e-05,
      "loss": 1.5649,
      "step": 432
    },
    {
      "epoch": 0.011333693964088157,
      "grad_norm": 16.485750198364258,
      "learning_rate": 1.0643287097618424e-05,
      "loss": 2.7503,
      "step": 433
    },
    {
      "epoch": 0.011359868776938245,
      "grad_norm": 13.972626686096191,
      "learning_rate": 1.0640146558492542e-05,
      "loss": 2.4709,
      "step": 434
    },
    {
      "epoch": 0.011386043589788333,
      "grad_norm": 11.040950775146484,
      "learning_rate": 1.0637006019366658e-05,
      "loss": 0.5166,
      "step": 435
    },
    {
      "epoch": 0.011412218402638421,
      "grad_norm": 19.529491424560547,
      "learning_rate": 1.0633865480240775e-05,
      "loss": 3.101,
      "step": 436
    },
    {
      "epoch": 0.011438393215488509,
      "grad_norm": 7.284560203552246,
      "learning_rate": 1.0630724941114893e-05,
      "loss": 1.3231,
      "step": 437
    },
    {
      "epoch": 0.011464568028338597,
      "grad_norm": 11.900135040283203,
      "learning_rate": 1.0627584401989009e-05,
      "loss": 1.4284,
      "step": 438
    },
    {
      "epoch": 0.011490742841188685,
      "grad_norm": 20.86232566833496,
      "learning_rate": 1.0624443862863126e-05,
      "loss": 0.5947,
      "step": 439
    },
    {
      "epoch": 0.011516917654038774,
      "grad_norm": 15.35389232635498,
      "learning_rate": 1.0621303323737242e-05,
      "loss": 1.1575,
      "step": 440
    },
    {
      "epoch": 0.011543092466888862,
      "grad_norm": 19.859607696533203,
      "learning_rate": 1.061816278461136e-05,
      "loss": 0.8267,
      "step": 441
    },
    {
      "epoch": 0.01156926727973895,
      "grad_norm": 15.935205459594727,
      "learning_rate": 1.0615022245485475e-05,
      "loss": 1.0885,
      "step": 442
    },
    {
      "epoch": 0.011595442092589038,
      "grad_norm": 17.462512969970703,
      "learning_rate": 1.0611881706359591e-05,
      "loss": 0.2306,
      "step": 443
    },
    {
      "epoch": 0.011621616905439126,
      "grad_norm": 15.117812156677246,
      "learning_rate": 1.0608741167233709e-05,
      "loss": 1.4718,
      "step": 444
    },
    {
      "epoch": 0.011647791718289214,
      "grad_norm": 19.19476318359375,
      "learning_rate": 1.0605600628107825e-05,
      "loss": 1.4816,
      "step": 445
    },
    {
      "epoch": 0.011673966531139302,
      "grad_norm": 11.454585075378418,
      "learning_rate": 1.0602460088981942e-05,
      "loss": 0.6296,
      "step": 446
    },
    {
      "epoch": 0.01170014134398939,
      "grad_norm": 21.852258682250977,
      "learning_rate": 1.0599319549856058e-05,
      "loss": 2.4447,
      "step": 447
    },
    {
      "epoch": 0.01172631615683948,
      "grad_norm": 10.331344604492188,
      "learning_rate": 1.0596179010730176e-05,
      "loss": 1.3197,
      "step": 448
    },
    {
      "epoch": 0.011752490969689567,
      "grad_norm": 16.449445724487305,
      "learning_rate": 1.0593038471604291e-05,
      "loss": 1.1467,
      "step": 449
    },
    {
      "epoch": 0.011778665782539655,
      "grad_norm": 18.224082946777344,
      "learning_rate": 1.0589897932478409e-05,
      "loss": 1.2427,
      "step": 450
    },
    {
      "epoch": 0.011804840595389743,
      "grad_norm": 13.384048461914062,
      "learning_rate": 1.0586757393352527e-05,
      "loss": 0.7212,
      "step": 451
    },
    {
      "epoch": 0.011831015408239831,
      "grad_norm": 17.2220458984375,
      "learning_rate": 1.0583616854226642e-05,
      "loss": 1.2506,
      "step": 452
    },
    {
      "epoch": 0.011857190221089919,
      "grad_norm": 23.529722213745117,
      "learning_rate": 1.058047631510076e-05,
      "loss": 2.2798,
      "step": 453
    },
    {
      "epoch": 0.011883365033940007,
      "grad_norm": 13.772336959838867,
      "learning_rate": 1.0577335775974876e-05,
      "loss": 0.8208,
      "step": 454
    },
    {
      "epoch": 0.011909539846790095,
      "grad_norm": 17.47730255126953,
      "learning_rate": 1.0574195236848993e-05,
      "loss": 1.08,
      "step": 455
    },
    {
      "epoch": 0.011935714659640183,
      "grad_norm": 11.2437105178833,
      "learning_rate": 1.057105469772311e-05,
      "loss": 1.518,
      "step": 456
    },
    {
      "epoch": 0.011961889472490272,
      "grad_norm": 13.08504581451416,
      "learning_rate": 1.0567914158597227e-05,
      "loss": 1.1616,
      "step": 457
    },
    {
      "epoch": 0.01198806428534036,
      "grad_norm": 18.31845474243164,
      "learning_rate": 1.0564773619471343e-05,
      "loss": 1.4963,
      "step": 458
    },
    {
      "epoch": 0.012014239098190448,
      "grad_norm": 19.127975463867188,
      "learning_rate": 1.0561633080345459e-05,
      "loss": 1.6742,
      "step": 459
    },
    {
      "epoch": 0.012040413911040536,
      "grad_norm": 10.626084327697754,
      "learning_rate": 1.0558492541219576e-05,
      "loss": 1.5333,
      "step": 460
    },
    {
      "epoch": 0.012066588723890624,
      "grad_norm": 10.832667350769043,
      "learning_rate": 1.0555352002093692e-05,
      "loss": 1.2157,
      "step": 461
    },
    {
      "epoch": 0.012092763536740712,
      "grad_norm": 23.05351448059082,
      "learning_rate": 1.055221146296781e-05,
      "loss": 1.2669,
      "step": 462
    },
    {
      "epoch": 0.0121189383495908,
      "grad_norm": 18.36079216003418,
      "learning_rate": 1.0549070923841927e-05,
      "loss": 1.5412,
      "step": 463
    },
    {
      "epoch": 0.012145113162440888,
      "grad_norm": 22.77534294128418,
      "learning_rate": 1.0545930384716043e-05,
      "loss": 0.5649,
      "step": 464
    },
    {
      "epoch": 0.012171287975290976,
      "grad_norm": 16.045751571655273,
      "learning_rate": 1.054278984559016e-05,
      "loss": 1.1233,
      "step": 465
    },
    {
      "epoch": 0.012197462788141065,
      "grad_norm": 16.265403747558594,
      "learning_rate": 1.0539649306464276e-05,
      "loss": 1.679,
      "step": 466
    },
    {
      "epoch": 0.012223637600991153,
      "grad_norm": 20.71302604675293,
      "learning_rate": 1.0536508767338394e-05,
      "loss": 1.7828,
      "step": 467
    },
    {
      "epoch": 0.012249812413841241,
      "grad_norm": 17.872230529785156,
      "learning_rate": 1.053336822821251e-05,
      "loss": 1.9604,
      "step": 468
    },
    {
      "epoch": 0.01227598722669133,
      "grad_norm": 14.933849334716797,
      "learning_rate": 1.0530227689086627e-05,
      "loss": 1.4939,
      "step": 469
    },
    {
      "epoch": 0.012302162039541417,
      "grad_norm": 16.174427032470703,
      "learning_rate": 1.0527087149960743e-05,
      "loss": 0.4945,
      "step": 470
    },
    {
      "epoch": 0.012328336852391505,
      "grad_norm": 18.38136863708496,
      "learning_rate": 1.052394661083486e-05,
      "loss": 1.6362,
      "step": 471
    },
    {
      "epoch": 0.012354511665241593,
      "grad_norm": 18.399559020996094,
      "learning_rate": 1.0520806071708978e-05,
      "loss": 1.0418,
      "step": 472
    },
    {
      "epoch": 0.012380686478091681,
      "grad_norm": 16.468408584594727,
      "learning_rate": 1.0517665532583092e-05,
      "loss": 1.3063,
      "step": 473
    },
    {
      "epoch": 0.01240686129094177,
      "grad_norm": 19.39691162109375,
      "learning_rate": 1.051452499345721e-05,
      "loss": 0.5574,
      "step": 474
    },
    {
      "epoch": 0.012433036103791859,
      "grad_norm": 29.760330200195312,
      "learning_rate": 1.0511384454331326e-05,
      "loss": 0.7921,
      "step": 475
    },
    {
      "epoch": 0.012459210916641946,
      "grad_norm": 18.140844345092773,
      "learning_rate": 1.0508243915205443e-05,
      "loss": 0.3105,
      "step": 476
    },
    {
      "epoch": 0.012485385729492034,
      "grad_norm": 20.809045791625977,
      "learning_rate": 1.0505103376079561e-05,
      "loss": 1.2175,
      "step": 477
    },
    {
      "epoch": 0.012511560542342122,
      "grad_norm": 15.0239896774292,
      "learning_rate": 1.0501962836953677e-05,
      "loss": 1.9543,
      "step": 478
    },
    {
      "epoch": 0.01253773535519221,
      "grad_norm": 17.87742042541504,
      "learning_rate": 1.0498822297827794e-05,
      "loss": 0.9287,
      "step": 479
    },
    {
      "epoch": 0.012563910168042298,
      "grad_norm": 17.054691314697266,
      "learning_rate": 1.049568175870191e-05,
      "loss": 0.5677,
      "step": 480
    },
    {
      "epoch": 0.012590084980892386,
      "grad_norm": 18.954370498657227,
      "learning_rate": 1.0492541219576028e-05,
      "loss": 1.9907,
      "step": 481
    },
    {
      "epoch": 0.012616259793742474,
      "grad_norm": 21.343564987182617,
      "learning_rate": 1.0489400680450144e-05,
      "loss": 1.375,
      "step": 482
    },
    {
      "epoch": 0.012642434606592564,
      "grad_norm": 13.8994779586792,
      "learning_rate": 1.0486260141324261e-05,
      "loss": 1.7046,
      "step": 483
    },
    {
      "epoch": 0.012668609419442652,
      "grad_norm": 17.33802604675293,
      "learning_rate": 1.0483119602198379e-05,
      "loss": 2.1402,
      "step": 484
    },
    {
      "epoch": 0.01269478423229274,
      "grad_norm": 20.242996215820312,
      "learning_rate": 1.0479979063072495e-05,
      "loss": 0.793,
      "step": 485
    },
    {
      "epoch": 0.012720959045142827,
      "grad_norm": 12.956822395324707,
      "learning_rate": 1.0476838523946612e-05,
      "loss": 1.0627,
      "step": 486
    },
    {
      "epoch": 0.012747133857992915,
      "grad_norm": 24.263866424560547,
      "learning_rate": 1.0473697984820728e-05,
      "loss": 1.5583,
      "step": 487
    },
    {
      "epoch": 0.012773308670843003,
      "grad_norm": 14.9137544631958,
      "learning_rate": 1.0470557445694846e-05,
      "loss": 1.044,
      "step": 488
    },
    {
      "epoch": 0.012799483483693091,
      "grad_norm": 19.844078063964844,
      "learning_rate": 1.0467416906568961e-05,
      "loss": 2.2749,
      "step": 489
    },
    {
      "epoch": 0.012825658296543179,
      "grad_norm": 12.397175788879395,
      "learning_rate": 1.0464276367443077e-05,
      "loss": 1.0671,
      "step": 490
    },
    {
      "epoch": 0.012851833109393267,
      "grad_norm": 15.332509994506836,
      "learning_rate": 1.0461135828317195e-05,
      "loss": 1.1739,
      "step": 491
    },
    {
      "epoch": 0.012878007922243357,
      "grad_norm": 11.42781925201416,
      "learning_rate": 1.045799528919131e-05,
      "loss": 1.2521,
      "step": 492
    },
    {
      "epoch": 0.012904182735093445,
      "grad_norm": 15.851160049438477,
      "learning_rate": 1.0454854750065428e-05,
      "loss": 1.3552,
      "step": 493
    },
    {
      "epoch": 0.012930357547943533,
      "grad_norm": 27.704286575317383,
      "learning_rate": 1.0451714210939544e-05,
      "loss": 1.7059,
      "step": 494
    },
    {
      "epoch": 0.01295653236079362,
      "grad_norm": 13.650613784790039,
      "learning_rate": 1.0448573671813662e-05,
      "loss": 1.7345,
      "step": 495
    },
    {
      "epoch": 0.012982707173643708,
      "grad_norm": 13.209798812866211,
      "learning_rate": 1.0445433132687778e-05,
      "loss": 1.0491,
      "step": 496
    },
    {
      "epoch": 0.013008881986493796,
      "grad_norm": 17.11916160583496,
      "learning_rate": 1.0442292593561895e-05,
      "loss": 0.6053,
      "step": 497
    },
    {
      "epoch": 0.013035056799343884,
      "grad_norm": 13.11049747467041,
      "learning_rate": 1.0439152054436013e-05,
      "loss": 1.0892,
      "step": 498
    },
    {
      "epoch": 0.013061231612193972,
      "grad_norm": 9.885848045349121,
      "learning_rate": 1.0436011515310128e-05,
      "loss": 1.4003,
      "step": 499
    },
    {
      "epoch": 0.013087406425044062,
      "grad_norm": 13.354086875915527,
      "learning_rate": 1.0432870976184246e-05,
      "loss": 0.9472,
      "step": 500
    },
    {
      "epoch": 0.01311358123789415,
      "grad_norm": 18.980764389038086,
      "learning_rate": 1.0429730437058362e-05,
      "loss": 1.6532,
      "step": 501
    },
    {
      "epoch": 0.013139756050744238,
      "grad_norm": 14.018961906433105,
      "learning_rate": 1.042658989793248e-05,
      "loss": 1.2485,
      "step": 502
    },
    {
      "epoch": 0.013165930863594326,
      "grad_norm": 19.385862350463867,
      "learning_rate": 1.0423449358806595e-05,
      "loss": 1.8448,
      "step": 503
    },
    {
      "epoch": 0.013192105676444413,
      "grad_norm": 24.350013732910156,
      "learning_rate": 1.0420308819680711e-05,
      "loss": 1.4734,
      "step": 504
    },
    {
      "epoch": 0.013218280489294501,
      "grad_norm": 27.88928985595703,
      "learning_rate": 1.0417168280554829e-05,
      "loss": 0.9323,
      "step": 505
    },
    {
      "epoch": 0.01324445530214459,
      "grad_norm": 11.809305191040039,
      "learning_rate": 1.0414027741428945e-05,
      "loss": 0.5544,
      "step": 506
    },
    {
      "epoch": 0.013270630114994677,
      "grad_norm": 24.713443756103516,
      "learning_rate": 1.0410887202303062e-05,
      "loss": 1.6557,
      "step": 507
    },
    {
      "epoch": 0.013296804927844765,
      "grad_norm": 21.127344131469727,
      "learning_rate": 1.0407746663177178e-05,
      "loss": 1.8234,
      "step": 508
    },
    {
      "epoch": 0.013322979740694855,
      "grad_norm": 12.818073272705078,
      "learning_rate": 1.0404606124051296e-05,
      "loss": 0.3886,
      "step": 509
    },
    {
      "epoch": 0.013349154553544943,
      "grad_norm": 16.266637802124023,
      "learning_rate": 1.0401465584925413e-05,
      "loss": 0.4114,
      "step": 510
    },
    {
      "epoch": 0.01337532936639503,
      "grad_norm": 19.857887268066406,
      "learning_rate": 1.0398325045799529e-05,
      "loss": 1.439,
      "step": 511
    },
    {
      "epoch": 0.013401504179245119,
      "grad_norm": 16.09312629699707,
      "learning_rate": 1.0395184506673647e-05,
      "loss": 1.1217,
      "step": 512
    },
    {
      "epoch": 0.013427678992095207,
      "grad_norm": 17.024757385253906,
      "learning_rate": 1.0392043967547762e-05,
      "loss": 1.1014,
      "step": 513
    },
    {
      "epoch": 0.013453853804945294,
      "grad_norm": 13.367935180664062,
      "learning_rate": 1.038890342842188e-05,
      "loss": 1.2313,
      "step": 514
    },
    {
      "epoch": 0.013480028617795382,
      "grad_norm": 10.811565399169922,
      "learning_rate": 1.0385762889295996e-05,
      "loss": 0.6174,
      "step": 515
    },
    {
      "epoch": 0.01350620343064547,
      "grad_norm": 13.5171480178833,
      "learning_rate": 1.0382622350170113e-05,
      "loss": 1.4272,
      "step": 516
    },
    {
      "epoch": 0.013532378243495558,
      "grad_norm": 17.4696102142334,
      "learning_rate": 1.037948181104423e-05,
      "loss": 1.4821,
      "step": 517
    },
    {
      "epoch": 0.013558553056345648,
      "grad_norm": 15.194046020507812,
      "learning_rate": 1.0376341271918347e-05,
      "loss": 0.6873,
      "step": 518
    },
    {
      "epoch": 0.013584727869195736,
      "grad_norm": 19.164377212524414,
      "learning_rate": 1.0373200732792464e-05,
      "loss": 1.3236,
      "step": 519
    },
    {
      "epoch": 0.013610902682045824,
      "grad_norm": 17.039722442626953,
      "learning_rate": 1.0370060193666578e-05,
      "loss": 2.5316,
      "step": 520
    },
    {
      "epoch": 0.013637077494895912,
      "grad_norm": 10.069087982177734,
      "learning_rate": 1.0366919654540696e-05,
      "loss": 0.2891,
      "step": 521
    },
    {
      "epoch": 0.013663252307746,
      "grad_norm": 17.341087341308594,
      "learning_rate": 1.0363779115414812e-05,
      "loss": 1.7373,
      "step": 522
    },
    {
      "epoch": 0.013689427120596087,
      "grad_norm": 23.268796920776367,
      "learning_rate": 1.036063857628893e-05,
      "loss": 0.8477,
      "step": 523
    },
    {
      "epoch": 0.013715601933446175,
      "grad_norm": 12.995265007019043,
      "learning_rate": 1.0357498037163047e-05,
      "loss": 1.5938,
      "step": 524
    },
    {
      "epoch": 0.013741776746296263,
      "grad_norm": 24.226688385009766,
      "learning_rate": 1.0354357498037163e-05,
      "loss": 1.8073,
      "step": 525
    },
    {
      "epoch": 0.013767951559146351,
      "grad_norm": 19.130029678344727,
      "learning_rate": 1.035121695891128e-05,
      "loss": 0.3941,
      "step": 526
    },
    {
      "epoch": 0.013794126371996441,
      "grad_norm": 17.605464935302734,
      "learning_rate": 1.0348076419785396e-05,
      "loss": 0.6631,
      "step": 527
    },
    {
      "epoch": 0.013820301184846529,
      "grad_norm": 19.67823600769043,
      "learning_rate": 1.0344935880659514e-05,
      "loss": 0.5204,
      "step": 528
    },
    {
      "epoch": 0.013846475997696617,
      "grad_norm": 15.978683471679688,
      "learning_rate": 1.034179534153363e-05,
      "loss": 1.4297,
      "step": 529
    },
    {
      "epoch": 0.013872650810546705,
      "grad_norm": 17.287260055541992,
      "learning_rate": 1.0338654802407747e-05,
      "loss": 1.3039,
      "step": 530
    },
    {
      "epoch": 0.013898825623396793,
      "grad_norm": 24.034582138061523,
      "learning_rate": 1.0335514263281865e-05,
      "loss": 0.87,
      "step": 531
    },
    {
      "epoch": 0.01392500043624688,
      "grad_norm": 16.89599609375,
      "learning_rate": 1.033237372415598e-05,
      "loss": 1.83,
      "step": 532
    },
    {
      "epoch": 0.013951175249096968,
      "grad_norm": 20.946035385131836,
      "learning_rate": 1.0329233185030098e-05,
      "loss": 0.9731,
      "step": 533
    },
    {
      "epoch": 0.013977350061947056,
      "grad_norm": 19.191268920898438,
      "learning_rate": 1.0326092645904214e-05,
      "loss": 1.6011,
      "step": 534
    },
    {
      "epoch": 0.014003524874797146,
      "grad_norm": 18.254608154296875,
      "learning_rate": 1.032295210677833e-05,
      "loss": 0.9383,
      "step": 535
    },
    {
      "epoch": 0.014029699687647234,
      "grad_norm": 26.649686813354492,
      "learning_rate": 1.0319811567652447e-05,
      "loss": 0.6128,
      "step": 536
    },
    {
      "epoch": 0.014055874500497322,
      "grad_norm": 19.536026000976562,
      "learning_rate": 1.0316671028526563e-05,
      "loss": 1.4551,
      "step": 537
    },
    {
      "epoch": 0.01408204931334741,
      "grad_norm": 22.442358016967773,
      "learning_rate": 1.0313530489400681e-05,
      "loss": 1.3228,
      "step": 538
    },
    {
      "epoch": 0.014108224126197498,
      "grad_norm": 35.857269287109375,
      "learning_rate": 1.0310389950274797e-05,
      "loss": 1.1974,
      "step": 539
    },
    {
      "epoch": 0.014134398939047586,
      "grad_norm": 21.048797607421875,
      "learning_rate": 1.0307249411148914e-05,
      "loss": 1.11,
      "step": 540
    },
    {
      "epoch": 0.014160573751897674,
      "grad_norm": 12.779808044433594,
      "learning_rate": 1.030410887202303e-05,
      "loss": 0.595,
      "step": 541
    },
    {
      "epoch": 0.014186748564747761,
      "grad_norm": 21.295724868774414,
      "learning_rate": 1.0300968332897148e-05,
      "loss": 1.8999,
      "step": 542
    },
    {
      "epoch": 0.01421292337759785,
      "grad_norm": 15.289767265319824,
      "learning_rate": 1.0297827793771264e-05,
      "loss": 1.8437,
      "step": 543
    },
    {
      "epoch": 0.014239098190447939,
      "grad_norm": 11.33554744720459,
      "learning_rate": 1.0294687254645381e-05,
      "loss": 1.1999,
      "step": 544
    },
    {
      "epoch": 0.014265273003298027,
      "grad_norm": 18.923627853393555,
      "learning_rate": 1.0291546715519499e-05,
      "loss": 0.9595,
      "step": 545
    },
    {
      "epoch": 0.014291447816148115,
      "grad_norm": 12.86658763885498,
      "learning_rate": 1.0288406176393615e-05,
      "loss": 1.8985,
      "step": 546
    },
    {
      "epoch": 0.014317622628998203,
      "grad_norm": 27.39386558532715,
      "learning_rate": 1.0285265637267732e-05,
      "loss": 1.1023,
      "step": 547
    },
    {
      "epoch": 0.01434379744184829,
      "grad_norm": 8.369140625,
      "learning_rate": 1.0282125098141848e-05,
      "loss": 0.8538,
      "step": 548
    },
    {
      "epoch": 0.014369972254698379,
      "grad_norm": 16.76064109802246,
      "learning_rate": 1.0278984559015965e-05,
      "loss": 0.6368,
      "step": 549
    },
    {
      "epoch": 0.014396147067548467,
      "grad_norm": 14.80250358581543,
      "learning_rate": 1.0275844019890081e-05,
      "loss": 1.1061,
      "step": 550
    },
    {
      "epoch": 0.014422321880398555,
      "grad_norm": 21.996402740478516,
      "learning_rate": 1.0272703480764197e-05,
      "loss": 0.4493,
      "step": 551
    },
    {
      "epoch": 0.014448496693248642,
      "grad_norm": 16.09116554260254,
      "learning_rate": 1.0269562941638315e-05,
      "loss": 1.6801,
      "step": 552
    },
    {
      "epoch": 0.014474671506098732,
      "grad_norm": 14.361990928649902,
      "learning_rate": 1.026642240251243e-05,
      "loss": 1.2444,
      "step": 553
    },
    {
      "epoch": 0.01450084631894882,
      "grad_norm": 16.11039161682129,
      "learning_rate": 1.0263281863386548e-05,
      "loss": 2.086,
      "step": 554
    },
    {
      "epoch": 0.014527021131798908,
      "grad_norm": 17.37653160095215,
      "learning_rate": 1.0260141324260664e-05,
      "loss": 1.0474,
      "step": 555
    },
    {
      "epoch": 0.014553195944648996,
      "grad_norm": 15.034843444824219,
      "learning_rate": 1.0257000785134782e-05,
      "loss": 0.9432,
      "step": 556
    },
    {
      "epoch": 0.014579370757499084,
      "grad_norm": 13.30081844329834,
      "learning_rate": 1.0253860246008899e-05,
      "loss": 1.4939,
      "step": 557
    },
    {
      "epoch": 0.014605545570349172,
      "grad_norm": 15.75667953491211,
      "learning_rate": 1.0250719706883015e-05,
      "loss": 1.2449,
      "step": 558
    },
    {
      "epoch": 0.01463172038319926,
      "grad_norm": 17.654273986816406,
      "learning_rate": 1.0247579167757133e-05,
      "loss": 1.4912,
      "step": 559
    },
    {
      "epoch": 0.014657895196049348,
      "grad_norm": 11.865370750427246,
      "learning_rate": 1.0244438628631248e-05,
      "loss": 0.8986,
      "step": 560
    },
    {
      "epoch": 0.014684070008899437,
      "grad_norm": 15.675153732299805,
      "learning_rate": 1.0241298089505366e-05,
      "loss": 1.1994,
      "step": 561
    },
    {
      "epoch": 0.014710244821749525,
      "grad_norm": 28.483217239379883,
      "learning_rate": 1.0238157550379482e-05,
      "loss": 1.322,
      "step": 562
    },
    {
      "epoch": 0.014736419634599613,
      "grad_norm": 14.541632652282715,
      "learning_rate": 1.02350170112536e-05,
      "loss": 0.77,
      "step": 563
    },
    {
      "epoch": 0.014762594447449701,
      "grad_norm": 17.677297592163086,
      "learning_rate": 1.0231876472127715e-05,
      "loss": 1.1277,
      "step": 564
    },
    {
      "epoch": 0.014788769260299789,
      "grad_norm": 16.82846450805664,
      "learning_rate": 1.0228735933001833e-05,
      "loss": 1.3135,
      "step": 565
    },
    {
      "epoch": 0.014814944073149877,
      "grad_norm": 15.833956718444824,
      "learning_rate": 1.0225595393875949e-05,
      "loss": 0.5963,
      "step": 566
    },
    {
      "epoch": 0.014841118885999965,
      "grad_norm": 12.425332069396973,
      "learning_rate": 1.0222454854750064e-05,
      "loss": 0.6848,
      "step": 567
    },
    {
      "epoch": 0.014867293698850053,
      "grad_norm": 16.202608108520508,
      "learning_rate": 1.0219314315624182e-05,
      "loss": 1.6065,
      "step": 568
    },
    {
      "epoch": 0.01489346851170014,
      "grad_norm": 14.26634407043457,
      "learning_rate": 1.02161737764983e-05,
      "loss": 0.8677,
      "step": 569
    },
    {
      "epoch": 0.01491964332455023,
      "grad_norm": 17.37994956970215,
      "learning_rate": 1.0213033237372415e-05,
      "loss": 1.5347,
      "step": 570
    },
    {
      "epoch": 0.014945818137400318,
      "grad_norm": 24.423118591308594,
      "learning_rate": 1.0209892698246533e-05,
      "loss": 1.2907,
      "step": 571
    },
    {
      "epoch": 0.014971992950250406,
      "grad_norm": 16.64789581298828,
      "learning_rate": 1.0206752159120649e-05,
      "loss": 0.9604,
      "step": 572
    },
    {
      "epoch": 0.014998167763100494,
      "grad_norm": 11.346396446228027,
      "learning_rate": 1.0203611619994766e-05,
      "loss": 0.6298,
      "step": 573
    },
    {
      "epoch": 0.015024342575950582,
      "grad_norm": 15.451828002929688,
      "learning_rate": 1.0200471080868882e-05,
      "loss": 0.849,
      "step": 574
    },
    {
      "epoch": 0.01505051738880067,
      "grad_norm": 20.49965476989746,
      "learning_rate": 1.0197330541743e-05,
      "loss": 1.0977,
      "step": 575
    },
    {
      "epoch": 0.015076692201650758,
      "grad_norm": 25.713367462158203,
      "learning_rate": 1.0194190002617116e-05,
      "loss": 1.3771,
      "step": 576
    },
    {
      "epoch": 0.015102867014500846,
      "grad_norm": 13.735161781311035,
      "learning_rate": 1.0191049463491233e-05,
      "loss": 1.3282,
      "step": 577
    },
    {
      "epoch": 0.015129041827350934,
      "grad_norm": 21.298664093017578,
      "learning_rate": 1.018790892436535e-05,
      "loss": 1.3679,
      "step": 578
    },
    {
      "epoch": 0.015155216640201023,
      "grad_norm": 23.778156280517578,
      "learning_rate": 1.0184768385239467e-05,
      "loss": 0.7208,
      "step": 579
    },
    {
      "epoch": 0.015181391453051111,
      "grad_norm": 17.97779655456543,
      "learning_rate": 1.0181627846113584e-05,
      "loss": 0.7386,
      "step": 580
    },
    {
      "epoch": 0.015207566265901199,
      "grad_norm": 17.146623611450195,
      "learning_rate": 1.01784873069877e-05,
      "loss": 0.6495,
      "step": 581
    },
    {
      "epoch": 0.015233741078751287,
      "grad_norm": 10.271608352661133,
      "learning_rate": 1.0175346767861816e-05,
      "loss": 0.6259,
      "step": 582
    },
    {
      "epoch": 0.015259915891601375,
      "grad_norm": 16.0623722076416,
      "learning_rate": 1.0172206228735933e-05,
      "loss": 1.9244,
      "step": 583
    },
    {
      "epoch": 0.015286090704451463,
      "grad_norm": 21.74067497253418,
      "learning_rate": 1.016906568961005e-05,
      "loss": 0.677,
      "step": 584
    },
    {
      "epoch": 0.01531226551730155,
      "grad_norm": 23.636545181274414,
      "learning_rate": 1.0165925150484167e-05,
      "loss": 1.4505,
      "step": 585
    },
    {
      "epoch": 0.015338440330151639,
      "grad_norm": 17.47941017150879,
      "learning_rate": 1.0162784611358283e-05,
      "loss": 0.435,
      "step": 586
    },
    {
      "epoch": 0.015364615143001728,
      "grad_norm": 11.056567192077637,
      "learning_rate": 1.01596440722324e-05,
      "loss": 0.4203,
      "step": 587
    },
    {
      "epoch": 0.015390789955851816,
      "grad_norm": 20.983240127563477,
      "learning_rate": 1.0156503533106516e-05,
      "loss": 0.3669,
      "step": 588
    },
    {
      "epoch": 0.015416964768701904,
      "grad_norm": 24.821361541748047,
      "learning_rate": 1.0153362993980634e-05,
      "loss": 0.4563,
      "step": 589
    },
    {
      "epoch": 0.015443139581551992,
      "grad_norm": 16.500356674194336,
      "learning_rate": 1.015022245485475e-05,
      "loss": 1.7656,
      "step": 590
    },
    {
      "epoch": 0.01546931439440208,
      "grad_norm": 18.274242401123047,
      "learning_rate": 1.0147081915728867e-05,
      "loss": 2.6145,
      "step": 591
    },
    {
      "epoch": 0.015495489207252168,
      "grad_norm": 31.088054656982422,
      "learning_rate": 1.0143941376602985e-05,
      "loss": 1.531,
      "step": 592
    },
    {
      "epoch": 0.015521664020102256,
      "grad_norm": 15.16018295288086,
      "learning_rate": 1.01408008374771e-05,
      "loss": 1.6802,
      "step": 593
    },
    {
      "epoch": 0.015547838832952344,
      "grad_norm": 22.216768264770508,
      "learning_rate": 1.0137660298351218e-05,
      "loss": 1.2264,
      "step": 594
    },
    {
      "epoch": 0.015574013645802432,
      "grad_norm": 18.0421085357666,
      "learning_rate": 1.0134519759225334e-05,
      "loss": 1.1351,
      "step": 595
    },
    {
      "epoch": 0.015600188458652521,
      "grad_norm": 29.573402404785156,
      "learning_rate": 1.0131379220099452e-05,
      "loss": 1.6207,
      "step": 596
    },
    {
      "epoch": 0.01562636327150261,
      "grad_norm": 11.64367389678955,
      "learning_rate": 1.0128238680973567e-05,
      "loss": 0.329,
      "step": 597
    },
    {
      "epoch": 0.015652538084352696,
      "grad_norm": 24.179574966430664,
      "learning_rate": 1.0125098141847683e-05,
      "loss": 1.1547,
      "step": 598
    },
    {
      "epoch": 0.015678712897202785,
      "grad_norm": 17.136369705200195,
      "learning_rate": 1.01219576027218e-05,
      "loss": 0.9613,
      "step": 599
    },
    {
      "epoch": 0.015704887710052875,
      "grad_norm": 21.638912200927734,
      "learning_rate": 1.0118817063595917e-05,
      "loss": 1.6673,
      "step": 600
    },
    {
      "epoch": 0.01573106252290296,
      "grad_norm": 15.265533447265625,
      "learning_rate": 1.0115676524470034e-05,
      "loss": 0.9758,
      "step": 601
    },
    {
      "epoch": 0.01575723733575305,
      "grad_norm": 16.836572647094727,
      "learning_rate": 1.011253598534415e-05,
      "loss": 1.216,
      "step": 602
    },
    {
      "epoch": 0.015783412148603137,
      "grad_norm": 17.460739135742188,
      "learning_rate": 1.0109395446218268e-05,
      "loss": 1.6175,
      "step": 603
    },
    {
      "epoch": 0.015809586961453227,
      "grad_norm": 13.88507080078125,
      "learning_rate": 1.0106254907092385e-05,
      "loss": 0.4466,
      "step": 604
    },
    {
      "epoch": 0.015835761774303313,
      "grad_norm": 15.179227828979492,
      "learning_rate": 1.0103114367966501e-05,
      "loss": 0.7412,
      "step": 605
    },
    {
      "epoch": 0.015861936587153402,
      "grad_norm": 19.4517822265625,
      "learning_rate": 1.0099973828840619e-05,
      "loss": 1.2817,
      "step": 606
    },
    {
      "epoch": 0.01588811140000349,
      "grad_norm": 17.643497467041016,
      "learning_rate": 1.0096833289714734e-05,
      "loss": 0.9285,
      "step": 607
    },
    {
      "epoch": 0.015914286212853578,
      "grad_norm": 16.297937393188477,
      "learning_rate": 1.0093692750588852e-05,
      "loss": 1.655,
      "step": 608
    },
    {
      "epoch": 0.015940461025703668,
      "grad_norm": 19.885833740234375,
      "learning_rate": 1.0090552211462968e-05,
      "loss": 1.3252,
      "step": 609
    },
    {
      "epoch": 0.015966635838553754,
      "grad_norm": 17.58584976196289,
      "learning_rate": 1.0087411672337085e-05,
      "loss": 1.1313,
      "step": 610
    },
    {
      "epoch": 0.015992810651403844,
      "grad_norm": 17.28054428100586,
      "learning_rate": 1.0084271133211201e-05,
      "loss": 0.6783,
      "step": 611
    },
    {
      "epoch": 0.01601898546425393,
      "grad_norm": 17.80898666381836,
      "learning_rate": 1.0081130594085319e-05,
      "loss": 0.7197,
      "step": 612
    },
    {
      "epoch": 0.01604516027710402,
      "grad_norm": 12.051491737365723,
      "learning_rate": 1.0077990054959435e-05,
      "loss": 1.4658,
      "step": 613
    },
    {
      "epoch": 0.016071335089954106,
      "grad_norm": 16.611989974975586,
      "learning_rate": 1.007484951583355e-05,
      "loss": 1.5251,
      "step": 614
    },
    {
      "epoch": 0.016097509902804195,
      "grad_norm": 13.03529167175293,
      "learning_rate": 1.0071708976707668e-05,
      "loss": 1.0846,
      "step": 615
    },
    {
      "epoch": 0.01612368471565428,
      "grad_norm": 15.323126792907715,
      "learning_rate": 1.0068568437581786e-05,
      "loss": 1.5093,
      "step": 616
    },
    {
      "epoch": 0.01614985952850437,
      "grad_norm": 17.194801330566406,
      "learning_rate": 1.0065427898455902e-05,
      "loss": 1.1138,
      "step": 617
    },
    {
      "epoch": 0.01617603434135446,
      "grad_norm": 15.417850494384766,
      "learning_rate": 1.0062287359330019e-05,
      "loss": 1.9317,
      "step": 618
    },
    {
      "epoch": 0.016202209154204547,
      "grad_norm": 15.945985794067383,
      "learning_rate": 1.0059146820204135e-05,
      "loss": 1.4502,
      "step": 619
    },
    {
      "epoch": 0.016228383967054637,
      "grad_norm": 20.059925079345703,
      "learning_rate": 1.0056006281078252e-05,
      "loss": 1.8998,
      "step": 620
    },
    {
      "epoch": 0.016254558779904723,
      "grad_norm": 9.894926071166992,
      "learning_rate": 1.0052865741952368e-05,
      "loss": 1.1092,
      "step": 621
    },
    {
      "epoch": 0.016280733592754813,
      "grad_norm": 13.820646286010742,
      "learning_rate": 1.0049725202826486e-05,
      "loss": 0.5971,
      "step": 622
    },
    {
      "epoch": 0.0163069084056049,
      "grad_norm": 19.131881713867188,
      "learning_rate": 1.0046584663700602e-05,
      "loss": 1.7721,
      "step": 623
    },
    {
      "epoch": 0.01633308321845499,
      "grad_norm": 13.103532791137695,
      "learning_rate": 1.004344412457472e-05,
      "loss": 0.6004,
      "step": 624
    },
    {
      "epoch": 0.016359258031305075,
      "grad_norm": 24.562854766845703,
      "learning_rate": 1.0040303585448837e-05,
      "loss": 1.2052,
      "step": 625
    },
    {
      "epoch": 0.016385432844155164,
      "grad_norm": 14.394401550292969,
      "learning_rate": 1.0037163046322953e-05,
      "loss": 1.0025,
      "step": 626
    },
    {
      "epoch": 0.016411607657005254,
      "grad_norm": 12.460485458374023,
      "learning_rate": 1.003402250719707e-05,
      "loss": 0.9003,
      "step": 627
    },
    {
      "epoch": 0.01643778246985534,
      "grad_norm": 16.823667526245117,
      "learning_rate": 1.0030881968071184e-05,
      "loss": 1.5238,
      "step": 628
    },
    {
      "epoch": 0.01646395728270543,
      "grad_norm": 20.3724422454834,
      "learning_rate": 1.0027741428945302e-05,
      "loss": 0.9671,
      "step": 629
    },
    {
      "epoch": 0.016490132095555516,
      "grad_norm": 14.112608909606934,
      "learning_rate": 1.002460088981942e-05,
      "loss": 1.4033,
      "step": 630
    },
    {
      "epoch": 0.016516306908405606,
      "grad_norm": 14.81662368774414,
      "learning_rate": 1.0021460350693535e-05,
      "loss": 1.6368,
      "step": 631
    },
    {
      "epoch": 0.016542481721255692,
      "grad_norm": 14.163483619689941,
      "learning_rate": 1.0018319811567653e-05,
      "loss": 1.4091,
      "step": 632
    },
    {
      "epoch": 0.01656865653410578,
      "grad_norm": 18.039451599121094,
      "learning_rate": 1.0015179272441769e-05,
      "loss": 1.3988,
      "step": 633
    },
    {
      "epoch": 0.016594831346955868,
      "grad_norm": 15.886393547058105,
      "learning_rate": 1.0012038733315886e-05,
      "loss": 1.1611,
      "step": 634
    },
    {
      "epoch": 0.016621006159805957,
      "grad_norm": 15.184565544128418,
      "learning_rate": 1.0008898194190002e-05,
      "loss": 0.7846,
      "step": 635
    },
    {
      "epoch": 0.016647180972656047,
      "grad_norm": 23.3873291015625,
      "learning_rate": 1.000575765506412e-05,
      "loss": 1.078,
      "step": 636
    },
    {
      "epoch": 0.016673355785506133,
      "grad_norm": 26.920629501342773,
      "learning_rate": 1.0002617115938236e-05,
      "loss": 0.6904,
      "step": 637
    },
    {
      "epoch": 0.016699530598356223,
      "grad_norm": 19.735254287719727,
      "learning_rate": 9.999476576812353e-06,
      "loss": 1.4643,
      "step": 638
    },
    {
      "epoch": 0.01672570541120631,
      "grad_norm": 13.736712455749512,
      "learning_rate": 9.99633603768647e-06,
      "loss": 0.6774,
      "step": 639
    },
    {
      "epoch": 0.0167518802240564,
      "grad_norm": 17.016040802001953,
      "learning_rate": 9.993195498560587e-06,
      "loss": 1.862,
      "step": 640
    },
    {
      "epoch": 0.016778055036906485,
      "grad_norm": 19.27604103088379,
      "learning_rate": 9.990054959434704e-06,
      "loss": 1.6934,
      "step": 641
    },
    {
      "epoch": 0.016804229849756575,
      "grad_norm": 12.857343673706055,
      "learning_rate": 9.98691442030882e-06,
      "loss": 0.7248,
      "step": 642
    },
    {
      "epoch": 0.01683040466260666,
      "grad_norm": 14.547774314880371,
      "learning_rate": 9.983773881182936e-06,
      "loss": 1.4196,
      "step": 643
    },
    {
      "epoch": 0.01685657947545675,
      "grad_norm": 19.974966049194336,
      "learning_rate": 9.980633342057053e-06,
      "loss": 1.8311,
      "step": 644
    },
    {
      "epoch": 0.01688275428830684,
      "grad_norm": 27.84545135498047,
      "learning_rate": 9.97749280293117e-06,
      "loss": 1.4235,
      "step": 645
    },
    {
      "epoch": 0.016908929101156926,
      "grad_norm": 17.007457733154297,
      "learning_rate": 9.974352263805287e-06,
      "loss": 1.1627,
      "step": 646
    },
    {
      "epoch": 0.016935103914007016,
      "grad_norm": 13.931382179260254,
      "learning_rate": 9.971211724679403e-06,
      "loss": 1.36,
      "step": 647
    },
    {
      "epoch": 0.016961278726857102,
      "grad_norm": 13.613231658935547,
      "learning_rate": 9.96807118555352e-06,
      "loss": 0.9076,
      "step": 648
    },
    {
      "epoch": 0.01698745353970719,
      "grad_norm": 17.082406997680664,
      "learning_rate": 9.964930646427636e-06,
      "loss": 0.6391,
      "step": 649
    },
    {
      "epoch": 0.017013628352557278,
      "grad_norm": 16.20777130126953,
      "learning_rate": 9.961790107301754e-06,
      "loss": 1.1833,
      "step": 650
    },
    {
      "epoch": 0.017039803165407368,
      "grad_norm": 15.951851844787598,
      "learning_rate": 9.958649568175871e-06,
      "loss": 1.6044,
      "step": 651
    },
    {
      "epoch": 0.017065977978257454,
      "grad_norm": 21.459369659423828,
      "learning_rate": 9.955509029049987e-06,
      "loss": 0.7955,
      "step": 652
    },
    {
      "epoch": 0.017092152791107543,
      "grad_norm": 20.098995208740234,
      "learning_rate": 9.952368489924105e-06,
      "loss": 0.8043,
      "step": 653
    },
    {
      "epoch": 0.017118327603957633,
      "grad_norm": 17.153419494628906,
      "learning_rate": 9.94922795079822e-06,
      "loss": 1.6519,
      "step": 654
    },
    {
      "epoch": 0.01714450241680772,
      "grad_norm": 23.202945709228516,
      "learning_rate": 9.946087411672338e-06,
      "loss": 1.0297,
      "step": 655
    },
    {
      "epoch": 0.01717067722965781,
      "grad_norm": 20.658382415771484,
      "learning_rate": 9.942946872546454e-06,
      "loss": 2.0945,
      "step": 656
    },
    {
      "epoch": 0.017196852042507895,
      "grad_norm": 15.702583312988281,
      "learning_rate": 9.939806333420571e-06,
      "loss": 1.7336,
      "step": 657
    },
    {
      "epoch": 0.017223026855357985,
      "grad_norm": 16.572620391845703,
      "learning_rate": 9.936665794294687e-06,
      "loss": 1.4731,
      "step": 658
    },
    {
      "epoch": 0.01724920166820807,
      "grad_norm": 25.60801887512207,
      "learning_rate": 9.933525255168803e-06,
      "loss": 0.713,
      "step": 659
    },
    {
      "epoch": 0.01727537648105816,
      "grad_norm": 27.76213264465332,
      "learning_rate": 9.93038471604292e-06,
      "loss": 1.2635,
      "step": 660
    },
    {
      "epoch": 0.01730155129390825,
      "grad_norm": 19.563274383544922,
      "learning_rate": 9.927244176917037e-06,
      "loss": 0.928,
      "step": 661
    },
    {
      "epoch": 0.017327726106758336,
      "grad_norm": 12.06052017211914,
      "learning_rate": 9.924103637791154e-06,
      "loss": 0.6016,
      "step": 662
    },
    {
      "epoch": 0.017353900919608426,
      "grad_norm": 23.698055267333984,
      "learning_rate": 9.920963098665272e-06,
      "loss": 1.2381,
      "step": 663
    },
    {
      "epoch": 0.017380075732458512,
      "grad_norm": 22.761512756347656,
      "learning_rate": 9.917822559539388e-06,
      "loss": 1.2595,
      "step": 664
    },
    {
      "epoch": 0.017406250545308602,
      "grad_norm": 26.94725799560547,
      "learning_rate": 9.914682020413505e-06,
      "loss": 0.7076,
      "step": 665
    },
    {
      "epoch": 0.017432425358158688,
      "grad_norm": 15.709563255310059,
      "learning_rate": 9.911541481287621e-06,
      "loss": 0.7692,
      "step": 666
    },
    {
      "epoch": 0.017458600171008778,
      "grad_norm": 17.319183349609375,
      "learning_rate": 9.908400942161739e-06,
      "loss": 1.0559,
      "step": 667
    },
    {
      "epoch": 0.017484774983858864,
      "grad_norm": 11.224342346191406,
      "learning_rate": 9.905260403035854e-06,
      "loss": 0.8501,
      "step": 668
    },
    {
      "epoch": 0.017510949796708954,
      "grad_norm": 31.263355255126953,
      "learning_rate": 9.902119863909972e-06,
      "loss": 1.5874,
      "step": 669
    },
    {
      "epoch": 0.017537124609559043,
      "grad_norm": 17.1675968170166,
      "learning_rate": 9.898979324784088e-06,
      "loss": 2.0209,
      "step": 670
    },
    {
      "epoch": 0.01756329942240913,
      "grad_norm": 27.857654571533203,
      "learning_rate": 9.895838785658205e-06,
      "loss": 1.3345,
      "step": 671
    },
    {
      "epoch": 0.01758947423525922,
      "grad_norm": 15.675129890441895,
      "learning_rate": 9.892698246532323e-06,
      "loss": 0.9799,
      "step": 672
    },
    {
      "epoch": 0.017615649048109305,
      "grad_norm": 17.52001953125,
      "learning_rate": 9.889557707406439e-06,
      "loss": 0.5756,
      "step": 673
    },
    {
      "epoch": 0.017641823860959395,
      "grad_norm": 17.825580596923828,
      "learning_rate": 9.886417168280555e-06,
      "loss": 1.1695,
      "step": 674
    },
    {
      "epoch": 0.01766799867380948,
      "grad_norm": 34.43243408203125,
      "learning_rate": 9.88327662915467e-06,
      "loss": 1.4855,
      "step": 675
    },
    {
      "epoch": 0.01769417348665957,
      "grad_norm": 13.802438735961914,
      "learning_rate": 9.880136090028788e-06,
      "loss": 1.2805,
      "step": 676
    },
    {
      "epoch": 0.017720348299509657,
      "grad_norm": 14.27303695678711,
      "learning_rate": 9.876995550902906e-06,
      "loss": 1.2247,
      "step": 677
    },
    {
      "epoch": 0.017746523112359747,
      "grad_norm": 16.53683853149414,
      "learning_rate": 9.873855011777021e-06,
      "loss": 1.0945,
      "step": 678
    },
    {
      "epoch": 0.017772697925209836,
      "grad_norm": 17.676671981811523,
      "learning_rate": 9.870714472651139e-06,
      "loss": 0.6955,
      "step": 679
    },
    {
      "epoch": 0.017798872738059923,
      "grad_norm": 19.15084457397461,
      "learning_rate": 9.867573933525255e-06,
      "loss": 1.3074,
      "step": 680
    },
    {
      "epoch": 0.017825047550910012,
      "grad_norm": 15.243623733520508,
      "learning_rate": 9.864433394399372e-06,
      "loss": 0.8546,
      "step": 681
    },
    {
      "epoch": 0.0178512223637601,
      "grad_norm": 17.337862014770508,
      "learning_rate": 9.861292855273488e-06,
      "loss": 1.3106,
      "step": 682
    },
    {
      "epoch": 0.017877397176610188,
      "grad_norm": 16.083049774169922,
      "learning_rate": 9.858152316147606e-06,
      "loss": 1.1712,
      "step": 683
    },
    {
      "epoch": 0.017903571989460274,
      "grad_norm": 16.112470626831055,
      "learning_rate": 9.855011777021723e-06,
      "loss": 1.0885,
      "step": 684
    },
    {
      "epoch": 0.017929746802310364,
      "grad_norm": 17.15744972229004,
      "learning_rate": 9.85187123789584e-06,
      "loss": 0.5098,
      "step": 685
    },
    {
      "epoch": 0.01795592161516045,
      "grad_norm": 14.356409072875977,
      "learning_rate": 9.848730698769957e-06,
      "loss": 0.5409,
      "step": 686
    },
    {
      "epoch": 0.01798209642801054,
      "grad_norm": 18.209165573120117,
      "learning_rate": 9.845590159644073e-06,
      "loss": 0.639,
      "step": 687
    },
    {
      "epoch": 0.01800827124086063,
      "grad_norm": 13.603002548217773,
      "learning_rate": 9.84244962051819e-06,
      "loss": 0.3668,
      "step": 688
    },
    {
      "epoch": 0.018034446053710716,
      "grad_norm": 21.50389289855957,
      "learning_rate": 9.839309081392306e-06,
      "loss": 1.9122,
      "step": 689
    },
    {
      "epoch": 0.018060620866560805,
      "grad_norm": 12.84823989868164,
      "learning_rate": 9.836168542266422e-06,
      "loss": 0.9989,
      "step": 690
    },
    {
      "epoch": 0.01808679567941089,
      "grad_norm": 10.444880485534668,
      "learning_rate": 9.83302800314054e-06,
      "loss": 0.4122,
      "step": 691
    },
    {
      "epoch": 0.01811297049226098,
      "grad_norm": 12.18832778930664,
      "learning_rate": 9.829887464014655e-06,
      "loss": 0.2205,
      "step": 692
    },
    {
      "epoch": 0.018139145305111067,
      "grad_norm": 13.163751602172852,
      "learning_rate": 9.826746924888773e-06,
      "loss": 0.4694,
      "step": 693
    },
    {
      "epoch": 0.018165320117961157,
      "grad_norm": 16.712440490722656,
      "learning_rate": 9.823606385762889e-06,
      "loss": 0.8977,
      "step": 694
    },
    {
      "epoch": 0.018191494930811243,
      "grad_norm": 14.04924201965332,
      "learning_rate": 9.820465846637006e-06,
      "loss": 0.449,
      "step": 695
    },
    {
      "epoch": 0.018217669743661333,
      "grad_norm": 9.60124397277832,
      "learning_rate": 9.817325307511122e-06,
      "loss": 0.7003,
      "step": 696
    },
    {
      "epoch": 0.018243844556511422,
      "grad_norm": 12.51932430267334,
      "learning_rate": 9.81418476838524e-06,
      "loss": 0.7259,
      "step": 697
    },
    {
      "epoch": 0.01827001936936151,
      "grad_norm": 11.624013900756836,
      "learning_rate": 9.811044229259357e-06,
      "loss": 0.7229,
      "step": 698
    },
    {
      "epoch": 0.018296194182211598,
      "grad_norm": 13.647758483886719,
      "learning_rate": 9.807903690133473e-06,
      "loss": 0.3062,
      "step": 699
    },
    {
      "epoch": 0.018322368995061684,
      "grad_norm": 13.513497352600098,
      "learning_rate": 9.80476315100759e-06,
      "loss": 0.8838,
      "step": 700
    },
    {
      "epoch": 0.018348543807911774,
      "grad_norm": 32.562904357910156,
      "learning_rate": 9.801622611881707e-06,
      "loss": 1.1553,
      "step": 701
    },
    {
      "epoch": 0.01837471862076186,
      "grad_norm": 20.50946044921875,
      "learning_rate": 9.798482072755824e-06,
      "loss": 0.9915,
      "step": 702
    },
    {
      "epoch": 0.01840089343361195,
      "grad_norm": 24.44700813293457,
      "learning_rate": 9.79534153362994e-06,
      "loss": 0.9622,
      "step": 703
    },
    {
      "epoch": 0.018427068246462036,
      "grad_norm": 18.90838050842285,
      "learning_rate": 9.792200994504057e-06,
      "loss": 1.5541,
      "step": 704
    },
    {
      "epoch": 0.018453243059312126,
      "grad_norm": 23.44221305847168,
      "learning_rate": 9.789060455378173e-06,
      "loss": 0.9468,
      "step": 705
    },
    {
      "epoch": 0.018479417872162215,
      "grad_norm": 16.447603225708008,
      "learning_rate": 9.78591991625229e-06,
      "loss": 0.5329,
      "step": 706
    },
    {
      "epoch": 0.0185055926850123,
      "grad_norm": 12.879626274108887,
      "learning_rate": 9.782779377126407e-06,
      "loss": 0.8984,
      "step": 707
    },
    {
      "epoch": 0.01853176749786239,
      "grad_norm": 18.267839431762695,
      "learning_rate": 9.779638838000523e-06,
      "loss": 0.4716,
      "step": 708
    },
    {
      "epoch": 0.018557942310712477,
      "grad_norm": 21.336830139160156,
      "learning_rate": 9.77649829887464e-06,
      "loss": 0.9637,
      "step": 709
    },
    {
      "epoch": 0.018584117123562567,
      "grad_norm": 16.403493881225586,
      "learning_rate": 9.773357759748758e-06,
      "loss": 0.9196,
      "step": 710
    },
    {
      "epoch": 0.018610291936412653,
      "grad_norm": 27.69179344177246,
      "learning_rate": 9.770217220622874e-06,
      "loss": 1.7087,
      "step": 711
    },
    {
      "epoch": 0.018636466749262743,
      "grad_norm": 14.248048782348633,
      "learning_rate": 9.767076681496991e-06,
      "loss": 1.428,
      "step": 712
    },
    {
      "epoch": 0.018662641562112833,
      "grad_norm": 21.588621139526367,
      "learning_rate": 9.763936142371107e-06,
      "loss": 1.364,
      "step": 713
    },
    {
      "epoch": 0.01868881637496292,
      "grad_norm": 15.811211585998535,
      "learning_rate": 9.760795603245225e-06,
      "loss": 0.9928,
      "step": 714
    },
    {
      "epoch": 0.01871499118781301,
      "grad_norm": 15.685888290405273,
      "learning_rate": 9.75765506411934e-06,
      "loss": 1.4014,
      "step": 715
    },
    {
      "epoch": 0.018741166000663095,
      "grad_norm": 13.539826393127441,
      "learning_rate": 9.754514524993458e-06,
      "loss": 0.8617,
      "step": 716
    },
    {
      "epoch": 0.018767340813513184,
      "grad_norm": 26.868104934692383,
      "learning_rate": 9.751373985867574e-06,
      "loss": 0.871,
      "step": 717
    },
    {
      "epoch": 0.01879351562636327,
      "grad_norm": 23.70384407043457,
      "learning_rate": 9.748233446741691e-06,
      "loss": 0.8109,
      "step": 718
    },
    {
      "epoch": 0.01881969043921336,
      "grad_norm": 22.60154914855957,
      "learning_rate": 9.745092907615809e-06,
      "loss": 0.9675,
      "step": 719
    },
    {
      "epoch": 0.018845865252063446,
      "grad_norm": 17.463958740234375,
      "learning_rate": 9.741952368489925e-06,
      "loss": 1.4124,
      "step": 720
    },
    {
      "epoch": 0.018872040064913536,
      "grad_norm": 19.23381805419922,
      "learning_rate": 9.73881182936404e-06,
      "loss": 1.0755,
      "step": 721
    },
    {
      "epoch": 0.018898214877763626,
      "grad_norm": 20.375213623046875,
      "learning_rate": 9.735671290238157e-06,
      "loss": 0.3726,
      "step": 722
    },
    {
      "epoch": 0.018924389690613712,
      "grad_norm": 18.133026123046875,
      "learning_rate": 9.732530751112274e-06,
      "loss": 1.4864,
      "step": 723
    },
    {
      "epoch": 0.0189505645034638,
      "grad_norm": 12.9170560836792,
      "learning_rate": 9.729390211986392e-06,
      "loss": 0.4277,
      "step": 724
    },
    {
      "epoch": 0.018976739316313888,
      "grad_norm": 16.993864059448242,
      "learning_rate": 9.726249672860507e-06,
      "loss": 0.5723,
      "step": 725
    },
    {
      "epoch": 0.019002914129163977,
      "grad_norm": 16.910964965820312,
      "learning_rate": 9.723109133734625e-06,
      "loss": 1.581,
      "step": 726
    },
    {
      "epoch": 0.019029088942014064,
      "grad_norm": 20.793357849121094,
      "learning_rate": 9.719968594608741e-06,
      "loss": 1.3553,
      "step": 727
    },
    {
      "epoch": 0.019055263754864153,
      "grad_norm": 17.70785903930664,
      "learning_rate": 9.716828055482858e-06,
      "loss": 0.6134,
      "step": 728
    },
    {
      "epoch": 0.01908143856771424,
      "grad_norm": 24.655664443969727,
      "learning_rate": 9.713687516356974e-06,
      "loss": 1.293,
      "step": 729
    },
    {
      "epoch": 0.01910761338056433,
      "grad_norm": 18.088884353637695,
      "learning_rate": 9.710546977231092e-06,
      "loss": 0.7405,
      "step": 730
    },
    {
      "epoch": 0.01913378819341442,
      "grad_norm": 18.831151962280273,
      "learning_rate": 9.70740643810521e-06,
      "loss": 0.8394,
      "step": 731
    },
    {
      "epoch": 0.019159963006264505,
      "grad_norm": 21.053407669067383,
      "learning_rate": 9.704265898979325e-06,
      "loss": 0.561,
      "step": 732
    },
    {
      "epoch": 0.019186137819114595,
      "grad_norm": 19.18998146057129,
      "learning_rate": 9.701125359853443e-06,
      "loss": 1.0119,
      "step": 733
    },
    {
      "epoch": 0.01921231263196468,
      "grad_norm": 17.89796257019043,
      "learning_rate": 9.697984820727559e-06,
      "loss": 1.3368,
      "step": 734
    },
    {
      "epoch": 0.01923848744481477,
      "grad_norm": 18.24168586730957,
      "learning_rate": 9.694844281601676e-06,
      "loss": 1.0352,
      "step": 735
    },
    {
      "epoch": 0.019264662257664857,
      "grad_norm": 24.430374145507812,
      "learning_rate": 9.691703742475792e-06,
      "loss": 0.543,
      "step": 736
    },
    {
      "epoch": 0.019290837070514946,
      "grad_norm": 14.245957374572754,
      "learning_rate": 9.688563203349908e-06,
      "loss": 1.0272,
      "step": 737
    },
    {
      "epoch": 0.019317011883365032,
      "grad_norm": 19.776704788208008,
      "learning_rate": 9.685422664224025e-06,
      "loss": 1.6423,
      "step": 738
    },
    {
      "epoch": 0.019343186696215122,
      "grad_norm": 14.0235595703125,
      "learning_rate": 9.682282125098141e-06,
      "loss": 0.9076,
      "step": 739
    },
    {
      "epoch": 0.01936936150906521,
      "grad_norm": 35.515357971191406,
      "learning_rate": 9.679141585972259e-06,
      "loss": 1.1531,
      "step": 740
    },
    {
      "epoch": 0.019395536321915298,
      "grad_norm": 20.482864379882812,
      "learning_rate": 9.676001046846375e-06,
      "loss": 1.2819,
      "step": 741
    },
    {
      "epoch": 0.019421711134765388,
      "grad_norm": 11.562335968017578,
      "learning_rate": 9.672860507720492e-06,
      "loss": 0.7234,
      "step": 742
    },
    {
      "epoch": 0.019447885947615474,
      "grad_norm": 22.146974563598633,
      "learning_rate": 9.669719968594608e-06,
      "loss": 1.5042,
      "step": 743
    },
    {
      "epoch": 0.019474060760465563,
      "grad_norm": 13.914840698242188,
      "learning_rate": 9.666579429468726e-06,
      "loss": 0.3689,
      "step": 744
    },
    {
      "epoch": 0.01950023557331565,
      "grad_norm": 19.512216567993164,
      "learning_rate": 9.663438890342843e-06,
      "loss": 0.262,
      "step": 745
    },
    {
      "epoch": 0.01952641038616574,
      "grad_norm": 17.122352600097656,
      "learning_rate": 9.660298351216959e-06,
      "loss": 1.5711,
      "step": 746
    },
    {
      "epoch": 0.019552585199015825,
      "grad_norm": 16.549701690673828,
      "learning_rate": 9.657157812091077e-06,
      "loss": 1.4184,
      "step": 747
    },
    {
      "epoch": 0.019578760011865915,
      "grad_norm": 41.34917449951172,
      "learning_rate": 9.654017272965193e-06,
      "loss": 0.8543,
      "step": 748
    },
    {
      "epoch": 0.019604934824716005,
      "grad_norm": 39.634857177734375,
      "learning_rate": 9.65087673383931e-06,
      "loss": 2.4896,
      "step": 749
    },
    {
      "epoch": 0.01963110963756609,
      "grad_norm": 23.902969360351562,
      "learning_rate": 9.647736194713426e-06,
      "loss": 0.8284,
      "step": 750
    },
    {
      "epoch": 0.01965728445041618,
      "grad_norm": 17.309837341308594,
      "learning_rate": 9.644595655587544e-06,
      "loss": 0.6987,
      "step": 751
    },
    {
      "epoch": 0.019683459263266267,
      "grad_norm": 21.494098663330078,
      "learning_rate": 9.64145511646166e-06,
      "loss": 1.3827,
      "step": 752
    },
    {
      "epoch": 0.019709634076116356,
      "grad_norm": 27.717483520507812,
      "learning_rate": 9.638314577335775e-06,
      "loss": 1.4204,
      "step": 753
    },
    {
      "epoch": 0.019735808888966443,
      "grad_norm": 24.596920013427734,
      "learning_rate": 9.635174038209893e-06,
      "loss": 0.9197,
      "step": 754
    },
    {
      "epoch": 0.019761983701816532,
      "grad_norm": 9.76498794555664,
      "learning_rate": 9.632033499084009e-06,
      "loss": 0.7534,
      "step": 755
    },
    {
      "epoch": 0.01978815851466662,
      "grad_norm": 12.94786262512207,
      "learning_rate": 9.628892959958126e-06,
      "loss": 1.1284,
      "step": 756
    },
    {
      "epoch": 0.019814333327516708,
      "grad_norm": 26.44408416748047,
      "learning_rate": 9.625752420832244e-06,
      "loss": 1.7044,
      "step": 757
    },
    {
      "epoch": 0.019840508140366798,
      "grad_norm": 17.869482040405273,
      "learning_rate": 9.62261188170636e-06,
      "loss": 0.7075,
      "step": 758
    },
    {
      "epoch": 0.019866682953216884,
      "grad_norm": 20.225215911865234,
      "learning_rate": 9.619471342580477e-06,
      "loss": 1.1812,
      "step": 759
    },
    {
      "epoch": 0.019892857766066974,
      "grad_norm": 16.958093643188477,
      "learning_rate": 9.616330803454593e-06,
      "loss": 1.4305,
      "step": 760
    },
    {
      "epoch": 0.01991903257891706,
      "grad_norm": 24.149658203125,
      "learning_rate": 9.61319026432871e-06,
      "loss": 0.6364,
      "step": 761
    },
    {
      "epoch": 0.01994520739176715,
      "grad_norm": 30.773408889770508,
      "learning_rate": 9.610049725202826e-06,
      "loss": 1.7494,
      "step": 762
    },
    {
      "epoch": 0.019971382204617236,
      "grad_norm": 49.424156188964844,
      "learning_rate": 9.606909186076944e-06,
      "loss": 0.5795,
      "step": 763
    },
    {
      "epoch": 0.019997557017467325,
      "grad_norm": 20.740760803222656,
      "learning_rate": 9.60376864695106e-06,
      "loss": 1.1851,
      "step": 764
    },
    {
      "epoch": 0.02002373183031741,
      "grad_norm": 20.17764663696289,
      "learning_rate": 9.600628107825177e-06,
      "loss": 1.5211,
      "step": 765
    },
    {
      "epoch": 0.0200499066431675,
      "grad_norm": 15.325651168823242,
      "learning_rate": 9.597487568699295e-06,
      "loss": 0.8559,
      "step": 766
    },
    {
      "epoch": 0.02007608145601759,
      "grad_norm": 18.244585037231445,
      "learning_rate": 9.594347029573409e-06,
      "loss": 1.2824,
      "step": 767
    },
    {
      "epoch": 0.020102256268867677,
      "grad_norm": 26.134756088256836,
      "learning_rate": 9.591206490447527e-06,
      "loss": 0.8992,
      "step": 768
    },
    {
      "epoch": 0.020128431081717767,
      "grad_norm": 20.381423950195312,
      "learning_rate": 9.588065951321643e-06,
      "loss": 0.8284,
      "step": 769
    },
    {
      "epoch": 0.020154605894567853,
      "grad_norm": 19.88959312438965,
      "learning_rate": 9.58492541219576e-06,
      "loss": 1.0607,
      "step": 770
    },
    {
      "epoch": 0.020180780707417943,
      "grad_norm": 16.797544479370117,
      "learning_rate": 9.581784873069878e-06,
      "loss": 1.3892,
      "step": 771
    },
    {
      "epoch": 0.02020695552026803,
      "grad_norm": 19.421751022338867,
      "learning_rate": 9.578644333943994e-06,
      "loss": 0.8593,
      "step": 772
    },
    {
      "epoch": 0.02023313033311812,
      "grad_norm": 24.74592399597168,
      "learning_rate": 9.575503794818111e-06,
      "loss": 0.7753,
      "step": 773
    },
    {
      "epoch": 0.020259305145968208,
      "grad_norm": 15.516802787780762,
      "learning_rate": 9.572363255692227e-06,
      "loss": 1.8316,
      "step": 774
    },
    {
      "epoch": 0.020285479958818294,
      "grad_norm": 17.479339599609375,
      "learning_rate": 9.569222716566344e-06,
      "loss": 0.8337,
      "step": 775
    },
    {
      "epoch": 0.020311654771668384,
      "grad_norm": 17.79015350341797,
      "learning_rate": 9.56608217744046e-06,
      "loss": 1.3226,
      "step": 776
    },
    {
      "epoch": 0.02033782958451847,
      "grad_norm": 13.167654991149902,
      "learning_rate": 9.562941638314578e-06,
      "loss": 0.2478,
      "step": 777
    },
    {
      "epoch": 0.02036400439736856,
      "grad_norm": 23.760839462280273,
      "learning_rate": 9.559801099188695e-06,
      "loss": 0.6621,
      "step": 778
    },
    {
      "epoch": 0.020390179210218646,
      "grad_norm": 14.234769821166992,
      "learning_rate": 9.556660560062811e-06,
      "loss": 0.7955,
      "step": 779
    },
    {
      "epoch": 0.020416354023068736,
      "grad_norm": 20.016338348388672,
      "learning_rate": 9.553520020936929e-06,
      "loss": 0.4906,
      "step": 780
    },
    {
      "epoch": 0.020442528835918822,
      "grad_norm": 15.903776168823242,
      "learning_rate": 9.550379481811045e-06,
      "loss": 0.8145,
      "step": 781
    },
    {
      "epoch": 0.02046870364876891,
      "grad_norm": 10.543067932128906,
      "learning_rate": 9.547238942685162e-06,
      "loss": 1.0888,
      "step": 782
    },
    {
      "epoch": 0.020494878461619,
      "grad_norm": 16.61180877685547,
      "learning_rate": 9.544098403559278e-06,
      "loss": 0.898,
      "step": 783
    },
    {
      "epoch": 0.020521053274469087,
      "grad_norm": 20.497962951660156,
      "learning_rate": 9.540957864433394e-06,
      "loss": 0.3235,
      "step": 784
    },
    {
      "epoch": 0.020547228087319177,
      "grad_norm": 19.104768753051758,
      "learning_rate": 9.537817325307512e-06,
      "loss": 0.98,
      "step": 785
    },
    {
      "epoch": 0.020573402900169263,
      "grad_norm": 36.84475326538086,
      "learning_rate": 9.534676786181627e-06,
      "loss": 1.5279,
      "step": 786
    },
    {
      "epoch": 0.020599577713019353,
      "grad_norm": 18.15375518798828,
      "learning_rate": 9.531536247055745e-06,
      "loss": 0.7684,
      "step": 787
    },
    {
      "epoch": 0.02062575252586944,
      "grad_norm": 17.69504165649414,
      "learning_rate": 9.52839570792986e-06,
      "loss": 0.8457,
      "step": 788
    },
    {
      "epoch": 0.02065192733871953,
      "grad_norm": 17.214595794677734,
      "learning_rate": 9.525255168803978e-06,
      "loss": 1.2459,
      "step": 789
    },
    {
      "epoch": 0.020678102151569615,
      "grad_norm": 20.988004684448242,
      "learning_rate": 9.522114629678094e-06,
      "loss": 0.6531,
      "step": 790
    },
    {
      "epoch": 0.020704276964419704,
      "grad_norm": 18.54210090637207,
      "learning_rate": 9.518974090552212e-06,
      "loss": 1.1261,
      "step": 791
    },
    {
      "epoch": 0.020730451777269794,
      "grad_norm": 18.410629272460938,
      "learning_rate": 9.51583355142633e-06,
      "loss": 1.41,
      "step": 792
    },
    {
      "epoch": 0.02075662659011988,
      "grad_norm": 20.796512603759766,
      "learning_rate": 9.512693012300445e-06,
      "loss": 0.9583,
      "step": 793
    },
    {
      "epoch": 0.02078280140296997,
      "grad_norm": 18.472131729125977,
      "learning_rate": 9.509552473174563e-06,
      "loss": 1.3308,
      "step": 794
    },
    {
      "epoch": 0.020808976215820056,
      "grad_norm": 16.66689682006836,
      "learning_rate": 9.506411934048679e-06,
      "loss": 0.5378,
      "step": 795
    },
    {
      "epoch": 0.020835151028670146,
      "grad_norm": 16.13934898376465,
      "learning_rate": 9.503271394922796e-06,
      "loss": 0.9605,
      "step": 796
    },
    {
      "epoch": 0.020861325841520232,
      "grad_norm": 14.453042984008789,
      "learning_rate": 9.500130855796912e-06,
      "loss": 1.1476,
      "step": 797
    },
    {
      "epoch": 0.02088750065437032,
      "grad_norm": 17.827566146850586,
      "learning_rate": 9.496990316671028e-06,
      "loss": 0.7694,
      "step": 798
    },
    {
      "epoch": 0.020913675467220408,
      "grad_norm": 20.049230575561523,
      "learning_rate": 9.493849777545145e-06,
      "loss": 0.9847,
      "step": 799
    },
    {
      "epoch": 0.020939850280070497,
      "grad_norm": 18.180843353271484,
      "learning_rate": 9.490709238419261e-06,
      "loss": 0.7593,
      "step": 800
    },
    {
      "epoch": 0.020966025092920587,
      "grad_norm": 22.90544319152832,
      "learning_rate": 9.487568699293379e-06,
      "loss": 1.9158,
      "step": 801
    },
    {
      "epoch": 0.020992199905770673,
      "grad_norm": 15.583829879760742,
      "learning_rate": 9.484428160167495e-06,
      "loss": 0.7942,
      "step": 802
    },
    {
      "epoch": 0.021018374718620763,
      "grad_norm": 15.503565788269043,
      "learning_rate": 9.481287621041612e-06,
      "loss": 1.1382,
      "step": 803
    },
    {
      "epoch": 0.02104454953147085,
      "grad_norm": 16.46419334411621,
      "learning_rate": 9.47814708191573e-06,
      "loss": 0.7715,
      "step": 804
    },
    {
      "epoch": 0.02107072434432094,
      "grad_norm": 14.946967124938965,
      "learning_rate": 9.475006542789846e-06,
      "loss": 1.55,
      "step": 805
    },
    {
      "epoch": 0.021096899157171025,
      "grad_norm": 19.88494873046875,
      "learning_rate": 9.471866003663963e-06,
      "loss": 1.4033,
      "step": 806
    },
    {
      "epoch": 0.021123073970021115,
      "grad_norm": 13.883177757263184,
      "learning_rate": 9.468725464538079e-06,
      "loss": 0.7306,
      "step": 807
    },
    {
      "epoch": 0.0211492487828712,
      "grad_norm": 21.33315658569336,
      "learning_rate": 9.465584925412197e-06,
      "loss": 0.9368,
      "step": 808
    },
    {
      "epoch": 0.02117542359572129,
      "grad_norm": 22.590017318725586,
      "learning_rate": 9.462444386286312e-06,
      "loss": 1.7683,
      "step": 809
    },
    {
      "epoch": 0.02120159840857138,
      "grad_norm": 21.756120681762695,
      "learning_rate": 9.45930384716043e-06,
      "loss": 1.4662,
      "step": 810
    },
    {
      "epoch": 0.021227773221421466,
      "grad_norm": 23.95503044128418,
      "learning_rate": 9.456163308034546e-06,
      "loss": 1.5254,
      "step": 811
    },
    {
      "epoch": 0.021253948034271556,
      "grad_norm": 13.111517906188965,
      "learning_rate": 9.453022768908663e-06,
      "loss": 0.826,
      "step": 812
    },
    {
      "epoch": 0.021280122847121642,
      "grad_norm": 41.495079040527344,
      "learning_rate": 9.449882229782781e-06,
      "loss": 0.5876,
      "step": 813
    },
    {
      "epoch": 0.021306297659971732,
      "grad_norm": 22.80171775817871,
      "learning_rate": 9.446741690656895e-06,
      "loss": 0.6666,
      "step": 814
    },
    {
      "epoch": 0.021332472472821818,
      "grad_norm": 14.649494171142578,
      "learning_rate": 9.443601151531013e-06,
      "loss": 0.8617,
      "step": 815
    },
    {
      "epoch": 0.021358647285671908,
      "grad_norm": 19.83968734741211,
      "learning_rate": 9.440460612405129e-06,
      "loss": 0.7347,
      "step": 816
    },
    {
      "epoch": 0.021384822098521994,
      "grad_norm": 18.460817337036133,
      "learning_rate": 9.437320073279246e-06,
      "loss": 1.3103,
      "step": 817
    },
    {
      "epoch": 0.021410996911372084,
      "grad_norm": 13.688819885253906,
      "learning_rate": 9.434179534153364e-06,
      "loss": 0.5554,
      "step": 818
    },
    {
      "epoch": 0.021437171724222173,
      "grad_norm": 21.763050079345703,
      "learning_rate": 9.43103899502748e-06,
      "loss": 0.9539,
      "step": 819
    },
    {
      "epoch": 0.02146334653707226,
      "grad_norm": 16.133323669433594,
      "learning_rate": 9.427898455901597e-06,
      "loss": 1.0567,
      "step": 820
    },
    {
      "epoch": 0.02148952134992235,
      "grad_norm": 20.971946716308594,
      "learning_rate": 9.424757916775713e-06,
      "loss": 1.0149,
      "step": 821
    },
    {
      "epoch": 0.021515696162772435,
      "grad_norm": 13.58545207977295,
      "learning_rate": 9.42161737764983e-06,
      "loss": 0.9597,
      "step": 822
    },
    {
      "epoch": 0.021541870975622525,
      "grad_norm": 11.846320152282715,
      "learning_rate": 9.418476838523946e-06,
      "loss": 0.7976,
      "step": 823
    },
    {
      "epoch": 0.02156804578847261,
      "grad_norm": 13.286391258239746,
      "learning_rate": 9.415336299398064e-06,
      "loss": 0.8484,
      "step": 824
    },
    {
      "epoch": 0.0215942206013227,
      "grad_norm": 20.09535026550293,
      "learning_rate": 9.412195760272181e-06,
      "loss": 0.9009,
      "step": 825
    },
    {
      "epoch": 0.02162039541417279,
      "grad_norm": 30.039936065673828,
      "learning_rate": 9.409055221146297e-06,
      "loss": 1.0417,
      "step": 826
    },
    {
      "epoch": 0.021646570227022877,
      "grad_norm": 18.605485916137695,
      "learning_rate": 9.405914682020415e-06,
      "loss": 1.3671,
      "step": 827
    },
    {
      "epoch": 0.021672745039872966,
      "grad_norm": 16.464641571044922,
      "learning_rate": 9.40277414289453e-06,
      "loss": 1.4686,
      "step": 828
    },
    {
      "epoch": 0.021698919852723052,
      "grad_norm": 14.652993202209473,
      "learning_rate": 9.399633603768647e-06,
      "loss": 0.6242,
      "step": 829
    },
    {
      "epoch": 0.021725094665573142,
      "grad_norm": 11.027660369873047,
      "learning_rate": 9.396493064642764e-06,
      "loss": 0.5346,
      "step": 830
    },
    {
      "epoch": 0.02175126947842323,
      "grad_norm": 21.316246032714844,
      "learning_rate": 9.39335252551688e-06,
      "loss": 1.2364,
      "step": 831
    },
    {
      "epoch": 0.021777444291273318,
      "grad_norm": 12.937393188476562,
      "learning_rate": 9.390211986390998e-06,
      "loss": 0.8563,
      "step": 832
    },
    {
      "epoch": 0.021803619104123404,
      "grad_norm": 12.538131713867188,
      "learning_rate": 9.387071447265113e-06,
      "loss": 0.611,
      "step": 833
    },
    {
      "epoch": 0.021829793916973494,
      "grad_norm": 26.206907272338867,
      "learning_rate": 9.383930908139231e-06,
      "loss": 0.6199,
      "step": 834
    },
    {
      "epoch": 0.021855968729823583,
      "grad_norm": 15.744893074035645,
      "learning_rate": 9.380790369013347e-06,
      "loss": 0.8119,
      "step": 835
    },
    {
      "epoch": 0.02188214354267367,
      "grad_norm": 18.580564498901367,
      "learning_rate": 9.377649829887464e-06,
      "loss": 1.0415,
      "step": 836
    },
    {
      "epoch": 0.02190831835552376,
      "grad_norm": 23.435142517089844,
      "learning_rate": 9.37450929076158e-06,
      "loss": 0.9622,
      "step": 837
    },
    {
      "epoch": 0.021934493168373845,
      "grad_norm": 12.608865737915039,
      "learning_rate": 9.371368751635698e-06,
      "loss": 0.8076,
      "step": 838
    },
    {
      "epoch": 0.021960667981223935,
      "grad_norm": 22.230405807495117,
      "learning_rate": 9.368228212509815e-06,
      "loss": 0.6271,
      "step": 839
    },
    {
      "epoch": 0.02198684279407402,
      "grad_norm": 17.52166748046875,
      "learning_rate": 9.365087673383931e-06,
      "loss": 0.6983,
      "step": 840
    },
    {
      "epoch": 0.02201301760692411,
      "grad_norm": 14.322993278503418,
      "learning_rate": 9.361947134258049e-06,
      "loss": 0.41,
      "step": 841
    },
    {
      "epoch": 0.022039192419774197,
      "grad_norm": 19.112977981567383,
      "learning_rate": 9.358806595132165e-06,
      "loss": 1.4177,
      "step": 842
    },
    {
      "epoch": 0.022065367232624287,
      "grad_norm": 16.49955940246582,
      "learning_rate": 9.355666056006282e-06,
      "loss": 0.7804,
      "step": 843
    },
    {
      "epoch": 0.022091542045474376,
      "grad_norm": 14.767380714416504,
      "learning_rate": 9.352525516880398e-06,
      "loss": 0.4157,
      "step": 844
    },
    {
      "epoch": 0.022117716858324463,
      "grad_norm": 17.366716384887695,
      "learning_rate": 9.349384977754514e-06,
      "loss": 0.7004,
      "step": 845
    },
    {
      "epoch": 0.022143891671174552,
      "grad_norm": 17.415742874145508,
      "learning_rate": 9.346244438628631e-06,
      "loss": 0.4496,
      "step": 846
    },
    {
      "epoch": 0.02217006648402464,
      "grad_norm": 24.452388763427734,
      "learning_rate": 9.343103899502747e-06,
      "loss": 1.1579,
      "step": 847
    },
    {
      "epoch": 0.022196241296874728,
      "grad_norm": 14.396705627441406,
      "learning_rate": 9.339963360376865e-06,
      "loss": 1.0059,
      "step": 848
    },
    {
      "epoch": 0.022222416109724814,
      "grad_norm": 16.048259735107422,
      "learning_rate": 9.33682282125098e-06,
      "loss": 0.5732,
      "step": 849
    },
    {
      "epoch": 0.022248590922574904,
      "grad_norm": 13.568750381469727,
      "learning_rate": 9.333682282125098e-06,
      "loss": 0.6994,
      "step": 850
    },
    {
      "epoch": 0.02227476573542499,
      "grad_norm": 17.99097442626953,
      "learning_rate": 9.330541742999216e-06,
      "loss": 1.6178,
      "step": 851
    },
    {
      "epoch": 0.02230094054827508,
      "grad_norm": 16.24090003967285,
      "learning_rate": 9.327401203873332e-06,
      "loss": 0.8511,
      "step": 852
    },
    {
      "epoch": 0.02232711536112517,
      "grad_norm": 19.298377990722656,
      "learning_rate": 9.32426066474745e-06,
      "loss": 0.8962,
      "step": 853
    },
    {
      "epoch": 0.022353290173975256,
      "grad_norm": 18.328216552734375,
      "learning_rate": 9.321120125621565e-06,
      "loss": 1.577,
      "step": 854
    },
    {
      "epoch": 0.022379464986825345,
      "grad_norm": 16.088275909423828,
      "learning_rate": 9.317979586495683e-06,
      "loss": 0.5868,
      "step": 855
    },
    {
      "epoch": 0.02240563979967543,
      "grad_norm": 17.16234016418457,
      "learning_rate": 9.314839047369799e-06,
      "loss": 0.5742,
      "step": 856
    },
    {
      "epoch": 0.02243181461252552,
      "grad_norm": 18.589988708496094,
      "learning_rate": 9.311698508243916e-06,
      "loss": 0.8341,
      "step": 857
    },
    {
      "epoch": 0.022457989425375607,
      "grad_norm": 19.9235897064209,
      "learning_rate": 9.308557969118032e-06,
      "loss": 0.9547,
      "step": 858
    },
    {
      "epoch": 0.022484164238225697,
      "grad_norm": 21.933839797973633,
      "learning_rate": 9.30541742999215e-06,
      "loss": 0.2659,
      "step": 859
    },
    {
      "epoch": 0.022510339051075783,
      "grad_norm": 20.53569984436035,
      "learning_rate": 9.302276890866265e-06,
      "loss": 0.798,
      "step": 860
    },
    {
      "epoch": 0.022536513863925873,
      "grad_norm": 10.95396614074707,
      "learning_rate": 9.299136351740381e-06,
      "loss": 0.3219,
      "step": 861
    },
    {
      "epoch": 0.022562688676775963,
      "grad_norm": 11.00860595703125,
      "learning_rate": 9.295995812614499e-06,
      "loss": 0.3292,
      "step": 862
    },
    {
      "epoch": 0.02258886348962605,
      "grad_norm": 14.921107292175293,
      "learning_rate": 9.292855273488615e-06,
      "loss": 1.1664,
      "step": 863
    },
    {
      "epoch": 0.02261503830247614,
      "grad_norm": 24.649066925048828,
      "learning_rate": 9.289714734362732e-06,
      "loss": 1.2226,
      "step": 864
    },
    {
      "epoch": 0.022641213115326225,
      "grad_norm": 22.066865921020508,
      "learning_rate": 9.28657419523685e-06,
      "loss": 1.5929,
      "step": 865
    },
    {
      "epoch": 0.022667387928176314,
      "grad_norm": 23.01406478881836,
      "learning_rate": 9.283433656110966e-06,
      "loss": 1.0643,
      "step": 866
    },
    {
      "epoch": 0.0226935627410264,
      "grad_norm": 18.122507095336914,
      "learning_rate": 9.280293116985083e-06,
      "loss": 1.2052,
      "step": 867
    },
    {
      "epoch": 0.02271973755387649,
      "grad_norm": 25.248008728027344,
      "learning_rate": 9.277152577859199e-06,
      "loss": 1.1317,
      "step": 868
    },
    {
      "epoch": 0.022745912366726576,
      "grad_norm": 19.595775604248047,
      "learning_rate": 9.274012038733317e-06,
      "loss": 0.7689,
      "step": 869
    },
    {
      "epoch": 0.022772087179576666,
      "grad_norm": 14.219436645507812,
      "learning_rate": 9.270871499607432e-06,
      "loss": 0.3689,
      "step": 870
    },
    {
      "epoch": 0.022798261992426756,
      "grad_norm": 11.443872451782227,
      "learning_rate": 9.26773096048155e-06,
      "loss": 0.275,
      "step": 871
    },
    {
      "epoch": 0.022824436805276842,
      "grad_norm": 20.41217041015625,
      "learning_rate": 9.264590421355668e-06,
      "loss": 1.3332,
      "step": 872
    },
    {
      "epoch": 0.02285061161812693,
      "grad_norm": 13.329365730285645,
      "learning_rate": 9.261449882229783e-06,
      "loss": 0.7467,
      "step": 873
    },
    {
      "epoch": 0.022876786430977018,
      "grad_norm": 13.33541488647461,
      "learning_rate": 9.258309343103901e-06,
      "loss": 0.8206,
      "step": 874
    },
    {
      "epoch": 0.022902961243827107,
      "grad_norm": 29.824663162231445,
      "learning_rate": 9.255168803978017e-06,
      "loss": 1.269,
      "step": 875
    },
    {
      "epoch": 0.022929136056677193,
      "grad_norm": 25.781095504760742,
      "learning_rate": 9.252028264852133e-06,
      "loss": 1.1239,
      "step": 876
    },
    {
      "epoch": 0.022955310869527283,
      "grad_norm": 36.34675216674805,
      "learning_rate": 9.24888772572625e-06,
      "loss": 1.8357,
      "step": 877
    },
    {
      "epoch": 0.02298148568237737,
      "grad_norm": 14.29274845123291,
      "learning_rate": 9.245747186600366e-06,
      "loss": 0.4518,
      "step": 878
    },
    {
      "epoch": 0.02300766049522746,
      "grad_norm": 13.736344337463379,
      "learning_rate": 9.242606647474484e-06,
      "loss": 0.331,
      "step": 879
    },
    {
      "epoch": 0.02303383530807755,
      "grad_norm": 16.67820167541504,
      "learning_rate": 9.2394661083486e-06,
      "loss": 0.9536,
      "step": 880
    },
    {
      "epoch": 0.023060010120927635,
      "grad_norm": 16.06148910522461,
      "learning_rate": 9.236325569222717e-06,
      "loss": 0.4296,
      "step": 881
    },
    {
      "epoch": 0.023086184933777724,
      "grad_norm": 17.447650909423828,
      "learning_rate": 9.233185030096833e-06,
      "loss": 1.0124,
      "step": 882
    },
    {
      "epoch": 0.02311235974662781,
      "grad_norm": 18.11922264099121,
      "learning_rate": 9.23004449097095e-06,
      "loss": 0.7357,
      "step": 883
    },
    {
      "epoch": 0.0231385345594779,
      "grad_norm": 15.388917922973633,
      "learning_rate": 9.226903951845066e-06,
      "loss": 0.872,
      "step": 884
    },
    {
      "epoch": 0.023164709372327986,
      "grad_norm": 28.77885627746582,
      "learning_rate": 9.223763412719184e-06,
      "loss": 0.9286,
      "step": 885
    },
    {
      "epoch": 0.023190884185178076,
      "grad_norm": 21.84748077392578,
      "learning_rate": 9.220622873593301e-06,
      "loss": 0.8844,
      "step": 886
    },
    {
      "epoch": 0.023217058998028166,
      "grad_norm": 12.316901206970215,
      "learning_rate": 9.217482334467417e-06,
      "loss": 0.4265,
      "step": 887
    },
    {
      "epoch": 0.023243233810878252,
      "grad_norm": 20.639610290527344,
      "learning_rate": 9.214341795341535e-06,
      "loss": 0.5737,
      "step": 888
    },
    {
      "epoch": 0.02326940862372834,
      "grad_norm": 20.400196075439453,
      "learning_rate": 9.21120125621565e-06,
      "loss": 1.8349,
      "step": 889
    },
    {
      "epoch": 0.023295583436578428,
      "grad_norm": 17.621192932128906,
      "learning_rate": 9.208060717089768e-06,
      "loss": 1.3701,
      "step": 890
    },
    {
      "epoch": 0.023321758249428517,
      "grad_norm": 14.860630989074707,
      "learning_rate": 9.204920177963884e-06,
      "loss": 0.5056,
      "step": 891
    },
    {
      "epoch": 0.023347933062278604,
      "grad_norm": 12.993891716003418,
      "learning_rate": 9.201779638838e-06,
      "loss": 0.4475,
      "step": 892
    },
    {
      "epoch": 0.023374107875128693,
      "grad_norm": 13.648996353149414,
      "learning_rate": 9.198639099712118e-06,
      "loss": 0.8555,
      "step": 893
    },
    {
      "epoch": 0.02340028268797878,
      "grad_norm": 23.163522720336914,
      "learning_rate": 9.195498560586233e-06,
      "loss": 1.0766,
      "step": 894
    },
    {
      "epoch": 0.02342645750082887,
      "grad_norm": 37.671993255615234,
      "learning_rate": 9.192358021460351e-06,
      "loss": 0.6127,
      "step": 895
    },
    {
      "epoch": 0.02345263231367896,
      "grad_norm": 16.284955978393555,
      "learning_rate": 9.189217482334467e-06,
      "loss": 0.6062,
      "step": 896
    },
    {
      "epoch": 0.023478807126529045,
      "grad_norm": 10.86940860748291,
      "learning_rate": 9.186076943208584e-06,
      "loss": 0.853,
      "step": 897
    },
    {
      "epoch": 0.023504981939379135,
      "grad_norm": 20.21002960205078,
      "learning_rate": 9.182936404082702e-06,
      "loss": 0.9093,
      "step": 898
    },
    {
      "epoch": 0.02353115675222922,
      "grad_norm": 14.43177318572998,
      "learning_rate": 9.179795864956818e-06,
      "loss": 0.4658,
      "step": 899
    },
    {
      "epoch": 0.02355733156507931,
      "grad_norm": 15.009284019470215,
      "learning_rate": 9.176655325830935e-06,
      "loss": 0.9047,
      "step": 900
    },
    {
      "epoch": 0.023583506377929397,
      "grad_norm": 13.192595481872559,
      "learning_rate": 9.173514786705051e-06,
      "loss": 0.2976,
      "step": 901
    },
    {
      "epoch": 0.023609681190779486,
      "grad_norm": 18.487560272216797,
      "learning_rate": 9.170374247579169e-06,
      "loss": 1.3448,
      "step": 902
    },
    {
      "epoch": 0.023635856003629573,
      "grad_norm": 15.864055633544922,
      "learning_rate": 9.167233708453285e-06,
      "loss": 0.8958,
      "step": 903
    },
    {
      "epoch": 0.023662030816479662,
      "grad_norm": 23.53626251220703,
      "learning_rate": 9.164093169327402e-06,
      "loss": 1.4936,
      "step": 904
    },
    {
      "epoch": 0.023688205629329752,
      "grad_norm": 32.990623474121094,
      "learning_rate": 9.160952630201518e-06,
      "loss": 1.1259,
      "step": 905
    },
    {
      "epoch": 0.023714380442179838,
      "grad_norm": 37.983612060546875,
      "learning_rate": 9.157812091075634e-06,
      "loss": 0.983,
      "step": 906
    },
    {
      "epoch": 0.023740555255029928,
      "grad_norm": 10.072787284851074,
      "learning_rate": 9.154671551949751e-06,
      "loss": 0.1772,
      "step": 907
    },
    {
      "epoch": 0.023766730067880014,
      "grad_norm": 15.133919715881348,
      "learning_rate": 9.151531012823867e-06,
      "loss": 0.9437,
      "step": 908
    },
    {
      "epoch": 0.023792904880730104,
      "grad_norm": 16.344951629638672,
      "learning_rate": 9.148390473697985e-06,
      "loss": 1.4256,
      "step": 909
    },
    {
      "epoch": 0.02381907969358019,
      "grad_norm": 14.026981353759766,
      "learning_rate": 9.1452499345721e-06,
      "loss": 0.6265,
      "step": 910
    },
    {
      "epoch": 0.02384525450643028,
      "grad_norm": 17.207155227661133,
      "learning_rate": 9.142109395446218e-06,
      "loss": 0.6298,
      "step": 911
    },
    {
      "epoch": 0.023871429319280366,
      "grad_norm": 16.117677688598633,
      "learning_rate": 9.138968856320336e-06,
      "loss": 1.2458,
      "step": 912
    },
    {
      "epoch": 0.023897604132130455,
      "grad_norm": 20.59002113342285,
      "learning_rate": 9.135828317194452e-06,
      "loss": 0.5596,
      "step": 913
    },
    {
      "epoch": 0.023923778944980545,
      "grad_norm": 19.817651748657227,
      "learning_rate": 9.13268777806857e-06,
      "loss": 1.412,
      "step": 914
    },
    {
      "epoch": 0.02394995375783063,
      "grad_norm": 25.94467544555664,
      "learning_rate": 9.129547238942685e-06,
      "loss": 1.0077,
      "step": 915
    },
    {
      "epoch": 0.02397612857068072,
      "grad_norm": 13.771411895751953,
      "learning_rate": 9.126406699816803e-06,
      "loss": 1.6331,
      "step": 916
    },
    {
      "epoch": 0.024002303383530807,
      "grad_norm": 19.249732971191406,
      "learning_rate": 9.123266160690918e-06,
      "loss": 0.5763,
      "step": 917
    },
    {
      "epoch": 0.024028478196380897,
      "grad_norm": 17.815757751464844,
      "learning_rate": 9.120125621565036e-06,
      "loss": 1.3727,
      "step": 918
    },
    {
      "epoch": 0.024054653009230983,
      "grad_norm": 24.883371353149414,
      "learning_rate": 9.116985082439154e-06,
      "loss": 0.8962,
      "step": 919
    },
    {
      "epoch": 0.024080827822081072,
      "grad_norm": 19.292198181152344,
      "learning_rate": 9.11384454331327e-06,
      "loss": 0.9108,
      "step": 920
    },
    {
      "epoch": 0.02410700263493116,
      "grad_norm": 13.042547225952148,
      "learning_rate": 9.110704004187387e-06,
      "loss": 0.7221,
      "step": 921
    },
    {
      "epoch": 0.02413317744778125,
      "grad_norm": 18.20087242126465,
      "learning_rate": 9.107563465061501e-06,
      "loss": 0.607,
      "step": 922
    },
    {
      "epoch": 0.024159352260631338,
      "grad_norm": 28.631694793701172,
      "learning_rate": 9.104422925935619e-06,
      "loss": 1.1953,
      "step": 923
    },
    {
      "epoch": 0.024185527073481424,
      "grad_norm": 19.396739959716797,
      "learning_rate": 9.101282386809736e-06,
      "loss": 0.8718,
      "step": 924
    },
    {
      "epoch": 0.024211701886331514,
      "grad_norm": 13.718029975891113,
      "learning_rate": 9.098141847683852e-06,
      "loss": 0.9772,
      "step": 925
    },
    {
      "epoch": 0.0242378766991816,
      "grad_norm": 18.981956481933594,
      "learning_rate": 9.09500130855797e-06,
      "loss": 0.6103,
      "step": 926
    },
    {
      "epoch": 0.02426405151203169,
      "grad_norm": 14.229768753051758,
      "learning_rate": 9.091860769432086e-06,
      "loss": 0.688,
      "step": 927
    },
    {
      "epoch": 0.024290226324881776,
      "grad_norm": 17.579540252685547,
      "learning_rate": 9.088720230306203e-06,
      "loss": 0.7758,
      "step": 928
    },
    {
      "epoch": 0.024316401137731865,
      "grad_norm": 24.20450782775879,
      "learning_rate": 9.085579691180319e-06,
      "loss": 1.332,
      "step": 929
    },
    {
      "epoch": 0.02434257595058195,
      "grad_norm": 19.184921264648438,
      "learning_rate": 9.082439152054436e-06,
      "loss": 1.3894,
      "step": 930
    },
    {
      "epoch": 0.02436875076343204,
      "grad_norm": 13.487165451049805,
      "learning_rate": 9.079298612928552e-06,
      "loss": 0.569,
      "step": 931
    },
    {
      "epoch": 0.02439492557628213,
      "grad_norm": 17.618600845336914,
      "learning_rate": 9.07615807380267e-06,
      "loss": 1.0034,
      "step": 932
    },
    {
      "epoch": 0.024421100389132217,
      "grad_norm": 20.07601547241211,
      "learning_rate": 9.073017534676787e-06,
      "loss": 1.1376,
      "step": 933
    },
    {
      "epoch": 0.024447275201982307,
      "grad_norm": 14.599526405334473,
      "learning_rate": 9.069876995550903e-06,
      "loss": 0.5347,
      "step": 934
    },
    {
      "epoch": 0.024473450014832393,
      "grad_norm": 12.583362579345703,
      "learning_rate": 9.066736456425021e-06,
      "loss": 0.6666,
      "step": 935
    },
    {
      "epoch": 0.024499624827682483,
      "grad_norm": 16.10581398010254,
      "learning_rate": 9.063595917299137e-06,
      "loss": 0.811,
      "step": 936
    },
    {
      "epoch": 0.02452579964053257,
      "grad_norm": 17.573604583740234,
      "learning_rate": 9.060455378173253e-06,
      "loss": 1.055,
      "step": 937
    },
    {
      "epoch": 0.02455197445338266,
      "grad_norm": 20.570842742919922,
      "learning_rate": 9.05731483904737e-06,
      "loss": 0.5977,
      "step": 938
    },
    {
      "epoch": 0.024578149266232745,
      "grad_norm": 15.86668586730957,
      "learning_rate": 9.054174299921486e-06,
      "loss": 0.6941,
      "step": 939
    },
    {
      "epoch": 0.024604324079082834,
      "grad_norm": 15.788714408874512,
      "learning_rate": 9.051033760795604e-06,
      "loss": 1.3602,
      "step": 940
    },
    {
      "epoch": 0.024630498891932924,
      "grad_norm": 30.584949493408203,
      "learning_rate": 9.04789322166972e-06,
      "loss": 1.1841,
      "step": 941
    },
    {
      "epoch": 0.02465667370478301,
      "grad_norm": 29.247314453125,
      "learning_rate": 9.044752682543837e-06,
      "loss": 1.1479,
      "step": 942
    },
    {
      "epoch": 0.0246828485176331,
      "grad_norm": 38.163150787353516,
      "learning_rate": 9.041612143417953e-06,
      "loss": 0.8425,
      "step": 943
    },
    {
      "epoch": 0.024709023330483186,
      "grad_norm": 18.887632369995117,
      "learning_rate": 9.03847160429207e-06,
      "loss": 0.9595,
      "step": 944
    },
    {
      "epoch": 0.024735198143333276,
      "grad_norm": 21.60202407836914,
      "learning_rate": 9.035331065166188e-06,
      "loss": 1.1404,
      "step": 945
    },
    {
      "epoch": 0.024761372956183362,
      "grad_norm": 13.927895545959473,
      "learning_rate": 9.032190526040304e-06,
      "loss": 0.5938,
      "step": 946
    },
    {
      "epoch": 0.02478754776903345,
      "grad_norm": 12.215208053588867,
      "learning_rate": 9.029049986914421e-06,
      "loss": 0.6235,
      "step": 947
    },
    {
      "epoch": 0.02481372258188354,
      "grad_norm": 27.692947387695312,
      "learning_rate": 9.025909447788537e-06,
      "loss": 0.9193,
      "step": 948
    },
    {
      "epoch": 0.024839897394733627,
      "grad_norm": 18.661052703857422,
      "learning_rate": 9.022768908662655e-06,
      "loss": 0.8735,
      "step": 949
    },
    {
      "epoch": 0.024866072207583717,
      "grad_norm": 18.25432777404785,
      "learning_rate": 9.01962836953677e-06,
      "loss": 0.4867,
      "step": 950
    },
    {
      "epoch": 0.024892247020433803,
      "grad_norm": 22.347694396972656,
      "learning_rate": 9.016487830410888e-06,
      "loss": 0.8666,
      "step": 951
    },
    {
      "epoch": 0.024918421833283893,
      "grad_norm": 20.735393524169922,
      "learning_rate": 9.013347291285004e-06,
      "loss": 0.8297,
      "step": 952
    },
    {
      "epoch": 0.02494459664613398,
      "grad_norm": 17.30999755859375,
      "learning_rate": 9.01020675215912e-06,
      "loss": 0.6298,
      "step": 953
    },
    {
      "epoch": 0.02497077145898407,
      "grad_norm": 15.732367515563965,
      "learning_rate": 9.007066213033237e-06,
      "loss": 1.0287,
      "step": 954
    },
    {
      "epoch": 0.024996946271834155,
      "grad_norm": 17.219409942626953,
      "learning_rate": 9.003925673907353e-06,
      "loss": 0.9774,
      "step": 955
    },
    {
      "epoch": 0.025023121084684245,
      "grad_norm": 20.012283325195312,
      "learning_rate": 9.00078513478147e-06,
      "loss": 0.6432,
      "step": 956
    },
    {
      "epoch": 0.025049295897534334,
      "grad_norm": 19.707353591918945,
      "learning_rate": 8.997644595655587e-06,
      "loss": 1.464,
      "step": 957
    },
    {
      "epoch": 0.02507547071038442,
      "grad_norm": 13.880343437194824,
      "learning_rate": 8.994504056529704e-06,
      "loss": 0.2431,
      "step": 958
    },
    {
      "epoch": 0.02510164552323451,
      "grad_norm": 15.792356491088867,
      "learning_rate": 8.991363517403822e-06,
      "loss": 1.0366,
      "step": 959
    },
    {
      "epoch": 0.025127820336084596,
      "grad_norm": 17.151491165161133,
      "learning_rate": 8.988222978277938e-06,
      "loss": 1.4181,
      "step": 960
    },
    {
      "epoch": 0.025153995148934686,
      "grad_norm": 24.469707489013672,
      "learning_rate": 8.985082439152055e-06,
      "loss": 0.5972,
      "step": 961
    },
    {
      "epoch": 0.025180169961784772,
      "grad_norm": 13.280518531799316,
      "learning_rate": 8.981941900026171e-06,
      "loss": 0.6855,
      "step": 962
    },
    {
      "epoch": 0.025206344774634862,
      "grad_norm": 11.197561264038086,
      "learning_rate": 8.978801360900289e-06,
      "loss": 0.5592,
      "step": 963
    },
    {
      "epoch": 0.025232519587484948,
      "grad_norm": 20.724884033203125,
      "learning_rate": 8.975660821774404e-06,
      "loss": 1.3622,
      "step": 964
    },
    {
      "epoch": 0.025258694400335038,
      "grad_norm": 18.04754638671875,
      "learning_rate": 8.972520282648522e-06,
      "loss": 1.1807,
      "step": 965
    },
    {
      "epoch": 0.025284869213185127,
      "grad_norm": 21.614206314086914,
      "learning_rate": 8.96937974352264e-06,
      "loss": 0.8417,
      "step": 966
    },
    {
      "epoch": 0.025311044026035213,
      "grad_norm": 17.01732063293457,
      "learning_rate": 8.966239204396755e-06,
      "loss": 0.9531,
      "step": 967
    },
    {
      "epoch": 0.025337218838885303,
      "grad_norm": 31.740556716918945,
      "learning_rate": 8.963098665270871e-06,
      "loss": 0.9707,
      "step": 968
    },
    {
      "epoch": 0.02536339365173539,
      "grad_norm": 24.956823348999023,
      "learning_rate": 8.959958126144987e-06,
      "loss": 0.8804,
      "step": 969
    },
    {
      "epoch": 0.02538956846458548,
      "grad_norm": 12.155966758728027,
      "learning_rate": 8.956817587019105e-06,
      "loss": 0.8008,
      "step": 970
    },
    {
      "epoch": 0.025415743277435565,
      "grad_norm": 9.722857475280762,
      "learning_rate": 8.953677047893222e-06,
      "loss": 0.362,
      "step": 971
    },
    {
      "epoch": 0.025441918090285655,
      "grad_norm": 18.809152603149414,
      "learning_rate": 8.950536508767338e-06,
      "loss": 1.1316,
      "step": 972
    },
    {
      "epoch": 0.02546809290313574,
      "grad_norm": 21.56940269470215,
      "learning_rate": 8.947395969641456e-06,
      "loss": 0.9384,
      "step": 973
    },
    {
      "epoch": 0.02549426771598583,
      "grad_norm": 23.18474578857422,
      "learning_rate": 8.944255430515572e-06,
      "loss": 0.6256,
      "step": 974
    },
    {
      "epoch": 0.02552044252883592,
      "grad_norm": 15.431642532348633,
      "learning_rate": 8.941114891389689e-06,
      "loss": 0.8208,
      "step": 975
    },
    {
      "epoch": 0.025546617341686007,
      "grad_norm": 14.796995162963867,
      "learning_rate": 8.937974352263805e-06,
      "loss": 0.7459,
      "step": 976
    },
    {
      "epoch": 0.025572792154536096,
      "grad_norm": 9.691946029663086,
      "learning_rate": 8.934833813137923e-06,
      "loss": 0.2852,
      "step": 977
    },
    {
      "epoch": 0.025598966967386182,
      "grad_norm": 37.812442779541016,
      "learning_rate": 8.931693274012038e-06,
      "loss": 1.1989,
      "step": 978
    },
    {
      "epoch": 0.025625141780236272,
      "grad_norm": 22.795846939086914,
      "learning_rate": 8.928552734886156e-06,
      "loss": 0.7045,
      "step": 979
    },
    {
      "epoch": 0.025651316593086358,
      "grad_norm": 15.780804634094238,
      "learning_rate": 8.925412195760273e-06,
      "loss": 1.3536,
      "step": 980
    },
    {
      "epoch": 0.025677491405936448,
      "grad_norm": 17.481834411621094,
      "learning_rate": 8.92227165663439e-06,
      "loss": 0.9872,
      "step": 981
    },
    {
      "epoch": 0.025703666218786534,
      "grad_norm": 20.693843841552734,
      "learning_rate": 8.919131117508507e-06,
      "loss": 0.7777,
      "step": 982
    },
    {
      "epoch": 0.025729841031636624,
      "grad_norm": 10.231376647949219,
      "learning_rate": 8.915990578382623e-06,
      "loss": 0.2145,
      "step": 983
    },
    {
      "epoch": 0.025756015844486713,
      "grad_norm": 18.13493537902832,
      "learning_rate": 8.912850039256739e-06,
      "loss": 0.7166,
      "step": 984
    },
    {
      "epoch": 0.0257821906573368,
      "grad_norm": 10.336857795715332,
      "learning_rate": 8.909709500130856e-06,
      "loss": 0.4098,
      "step": 985
    },
    {
      "epoch": 0.02580836547018689,
      "grad_norm": 16.242277145385742,
      "learning_rate": 8.906568961004972e-06,
      "loss": 0.7698,
      "step": 986
    },
    {
      "epoch": 0.025834540283036975,
      "grad_norm": 20.488733291625977,
      "learning_rate": 8.90342842187909e-06,
      "loss": 0.8884,
      "step": 987
    },
    {
      "epoch": 0.025860715095887065,
      "grad_norm": 21.018901824951172,
      "learning_rate": 8.900287882753205e-06,
      "loss": 0.9054,
      "step": 988
    },
    {
      "epoch": 0.02588688990873715,
      "grad_norm": 16.938274383544922,
      "learning_rate": 8.897147343627323e-06,
      "loss": 1.0424,
      "step": 989
    },
    {
      "epoch": 0.02591306472158724,
      "grad_norm": 24.921157836914062,
      "learning_rate": 8.894006804501439e-06,
      "loss": 0.8191,
      "step": 990
    },
    {
      "epoch": 0.025939239534437327,
      "grad_norm": 23.72759246826172,
      "learning_rate": 8.890866265375556e-06,
      "loss": 0.7427,
      "step": 991
    },
    {
      "epoch": 0.025965414347287417,
      "grad_norm": 13.402997016906738,
      "learning_rate": 8.887725726249674e-06,
      "loss": 0.6203,
      "step": 992
    },
    {
      "epoch": 0.025991589160137506,
      "grad_norm": 13.776045799255371,
      "learning_rate": 8.88458518712379e-06,
      "loss": 0.9125,
      "step": 993
    },
    {
      "epoch": 0.026017763972987593,
      "grad_norm": 17.09200668334961,
      "learning_rate": 8.881444647997907e-06,
      "loss": 1.339,
      "step": 994
    },
    {
      "epoch": 0.026043938785837682,
      "grad_norm": 19.210105895996094,
      "learning_rate": 8.878304108872023e-06,
      "loss": 0.6543,
      "step": 995
    },
    {
      "epoch": 0.02607011359868777,
      "grad_norm": 12.220773696899414,
      "learning_rate": 8.87516356974614e-06,
      "loss": 0.5568,
      "step": 996
    },
    {
      "epoch": 0.026096288411537858,
      "grad_norm": 17.78321075439453,
      "learning_rate": 8.872023030620257e-06,
      "loss": 0.7973,
      "step": 997
    },
    {
      "epoch": 0.026122463224387944,
      "grad_norm": 25.987051010131836,
      "learning_rate": 8.868882491494374e-06,
      "loss": 0.753,
      "step": 998
    },
    {
      "epoch": 0.026148638037238034,
      "grad_norm": 19.03817367553711,
      "learning_rate": 8.86574195236849e-06,
      "loss": 0.7087,
      "step": 999
    },
    {
      "epoch": 0.026174812850088124,
      "grad_norm": 12.386798858642578,
      "learning_rate": 8.862601413242606e-06,
      "loss": 0.7176,
      "step": 1000
    },
    {
      "epoch": 0.02620098766293821,
      "grad_norm": 31.661161422729492,
      "learning_rate": 8.859460874116723e-06,
      "loss": 0.9982,
      "step": 1001
    },
    {
      "epoch": 0.0262271624757883,
      "grad_norm": 20.21319007873535,
      "learning_rate": 8.85632033499084e-06,
      "loss": 0.8212,
      "step": 1002
    },
    {
      "epoch": 0.026253337288638386,
      "grad_norm": 19.39395523071289,
      "learning_rate": 8.853179795864957e-06,
      "loss": 0.9278,
      "step": 1003
    },
    {
      "epoch": 0.026279512101488475,
      "grad_norm": 17.41468620300293,
      "learning_rate": 8.850039256739073e-06,
      "loss": 0.4466,
      "step": 1004
    },
    {
      "epoch": 0.02630568691433856,
      "grad_norm": 16.003231048583984,
      "learning_rate": 8.84689871761319e-06,
      "loss": 0.8415,
      "step": 1005
    },
    {
      "epoch": 0.02633186172718865,
      "grad_norm": 26.415393829345703,
      "learning_rate": 8.843758178487308e-06,
      "loss": 1.0569,
      "step": 1006
    },
    {
      "epoch": 0.026358036540038737,
      "grad_norm": 23.302183151245117,
      "learning_rate": 8.840617639361424e-06,
      "loss": 0.9145,
      "step": 1007
    },
    {
      "epoch": 0.026384211352888827,
      "grad_norm": 21.894014358520508,
      "learning_rate": 8.837477100235541e-06,
      "loss": 0.5281,
      "step": 1008
    },
    {
      "epoch": 0.026410386165738917,
      "grad_norm": 21.145177841186523,
      "learning_rate": 8.834336561109657e-06,
      "loss": 1.2992,
      "step": 1009
    },
    {
      "epoch": 0.026436560978589003,
      "grad_norm": 14.931975364685059,
      "learning_rate": 8.831196021983775e-06,
      "loss": 1.1661,
      "step": 1010
    },
    {
      "epoch": 0.026462735791439092,
      "grad_norm": 21.843217849731445,
      "learning_rate": 8.82805548285789e-06,
      "loss": 0.8543,
      "step": 1011
    },
    {
      "epoch": 0.02648891060428918,
      "grad_norm": 17.63266372680664,
      "learning_rate": 8.824914943732008e-06,
      "loss": 1.1092,
      "step": 1012
    },
    {
      "epoch": 0.02651508541713927,
      "grad_norm": 26.341777801513672,
      "learning_rate": 8.821774404606126e-06,
      "loss": 0.6214,
      "step": 1013
    },
    {
      "epoch": 0.026541260229989354,
      "grad_norm": 26.317171096801758,
      "learning_rate": 8.818633865480241e-06,
      "loss": 0.9403,
      "step": 1014
    },
    {
      "epoch": 0.026567435042839444,
      "grad_norm": 17.038082122802734,
      "learning_rate": 8.815493326354357e-06,
      "loss": 0.4633,
      "step": 1015
    },
    {
      "epoch": 0.02659360985568953,
      "grad_norm": 25.592071533203125,
      "learning_rate": 8.812352787228473e-06,
      "loss": 0.7419,
      "step": 1016
    },
    {
      "epoch": 0.02661978466853962,
      "grad_norm": 26.74091339111328,
      "learning_rate": 8.80921224810259e-06,
      "loss": 0.7687,
      "step": 1017
    },
    {
      "epoch": 0.02664595948138971,
      "grad_norm": 20.84470558166504,
      "learning_rate": 8.806071708976708e-06,
      "loss": 1.6443,
      "step": 1018
    },
    {
      "epoch": 0.026672134294239796,
      "grad_norm": 13.161638259887695,
      "learning_rate": 8.802931169850824e-06,
      "loss": 0.5368,
      "step": 1019
    },
    {
      "epoch": 0.026698309107089886,
      "grad_norm": 16.118995666503906,
      "learning_rate": 8.799790630724942e-06,
      "loss": 1.2032,
      "step": 1020
    },
    {
      "epoch": 0.02672448391993997,
      "grad_norm": 13.004922866821289,
      "learning_rate": 8.796650091599058e-06,
      "loss": 0.798,
      "step": 1021
    },
    {
      "epoch": 0.02675065873279006,
      "grad_norm": 22.58649253845215,
      "learning_rate": 8.793509552473175e-06,
      "loss": 0.8055,
      "step": 1022
    },
    {
      "epoch": 0.026776833545640148,
      "grad_norm": 21.10123634338379,
      "learning_rate": 8.790369013347291e-06,
      "loss": 1.2528,
      "step": 1023
    },
    {
      "epoch": 0.026803008358490237,
      "grad_norm": 20.990276336669922,
      "learning_rate": 8.787228474221409e-06,
      "loss": 0.7672,
      "step": 1024
    },
    {
      "epoch": 0.026829183171340323,
      "grad_norm": 13.316649436950684,
      "learning_rate": 8.784087935095524e-06,
      "loss": 0.8661,
      "step": 1025
    },
    {
      "epoch": 0.026855357984190413,
      "grad_norm": 24.921159744262695,
      "learning_rate": 8.780947395969642e-06,
      "loss": 1.433,
      "step": 1026
    },
    {
      "epoch": 0.026881532797040503,
      "grad_norm": 23.345951080322266,
      "learning_rate": 8.77780685684376e-06,
      "loss": 0.5345,
      "step": 1027
    },
    {
      "epoch": 0.02690770760989059,
      "grad_norm": 22.206424713134766,
      "learning_rate": 8.774666317717875e-06,
      "loss": 1.4068,
      "step": 1028
    },
    {
      "epoch": 0.02693388242274068,
      "grad_norm": 13.34490966796875,
      "learning_rate": 8.771525778591993e-06,
      "loss": 0.4092,
      "step": 1029
    },
    {
      "epoch": 0.026960057235590765,
      "grad_norm": 17.30352210998535,
      "learning_rate": 8.768385239466107e-06,
      "loss": 0.6091,
      "step": 1030
    },
    {
      "epoch": 0.026986232048440854,
      "grad_norm": 10.58128833770752,
      "learning_rate": 8.765244700340225e-06,
      "loss": 0.202,
      "step": 1031
    },
    {
      "epoch": 0.02701240686129094,
      "grad_norm": 14.250791549682617,
      "learning_rate": 8.762104161214342e-06,
      "loss": 0.4598,
      "step": 1032
    },
    {
      "epoch": 0.02703858167414103,
      "grad_norm": 15.873455047607422,
      "learning_rate": 8.758963622088458e-06,
      "loss": 0.7285,
      "step": 1033
    },
    {
      "epoch": 0.027064756486991116,
      "grad_norm": 16.286922454833984,
      "learning_rate": 8.755823082962576e-06,
      "loss": 0.8312,
      "step": 1034
    },
    {
      "epoch": 0.027090931299841206,
      "grad_norm": 18.8758544921875,
      "learning_rate": 8.752682543836691e-06,
      "loss": 0.9187,
      "step": 1035
    },
    {
      "epoch": 0.027117106112691296,
      "grad_norm": 23.0502986907959,
      "learning_rate": 8.749542004710809e-06,
      "loss": 0.8209,
      "step": 1036
    },
    {
      "epoch": 0.027143280925541382,
      "grad_norm": 12.960359573364258,
      "learning_rate": 8.746401465584925e-06,
      "loss": 0.6758,
      "step": 1037
    },
    {
      "epoch": 0.02716945573839147,
      "grad_norm": 11.770807266235352,
      "learning_rate": 8.743260926459042e-06,
      "loss": 0.4284,
      "step": 1038
    },
    {
      "epoch": 0.027195630551241558,
      "grad_norm": 17.059663772583008,
      "learning_rate": 8.74012038733316e-06,
      "loss": 1.0634,
      "step": 1039
    },
    {
      "epoch": 0.027221805364091647,
      "grad_norm": 19.367183685302734,
      "learning_rate": 8.736979848207276e-06,
      "loss": 0.7698,
      "step": 1040
    },
    {
      "epoch": 0.027247980176941734,
      "grad_norm": 22.40457534790039,
      "learning_rate": 8.733839309081393e-06,
      "loss": 1.3049,
      "step": 1041
    },
    {
      "epoch": 0.027274154989791823,
      "grad_norm": 16.627735137939453,
      "learning_rate": 8.73069876995551e-06,
      "loss": 0.5922,
      "step": 1042
    },
    {
      "epoch": 0.02730032980264191,
      "grad_norm": 14.184876441955566,
      "learning_rate": 8.727558230829627e-06,
      "loss": 0.8558,
      "step": 1043
    },
    {
      "epoch": 0.027326504615492,
      "grad_norm": 18.4725341796875,
      "learning_rate": 8.724417691703743e-06,
      "loss": 0.6219,
      "step": 1044
    },
    {
      "epoch": 0.02735267942834209,
      "grad_norm": 19.5185489654541,
      "learning_rate": 8.72127715257786e-06,
      "loss": 1.2628,
      "step": 1045
    },
    {
      "epoch": 0.027378854241192175,
      "grad_norm": 22.9426212310791,
      "learning_rate": 8.718136613451976e-06,
      "loss": 1.3125,
      "step": 1046
    },
    {
      "epoch": 0.027405029054042265,
      "grad_norm": 13.754690170288086,
      "learning_rate": 8.714996074326092e-06,
      "loss": 0.5608,
      "step": 1047
    },
    {
      "epoch": 0.02743120386689235,
      "grad_norm": 22.93401336669922,
      "learning_rate": 8.71185553520021e-06,
      "loss": 1.2349,
      "step": 1048
    },
    {
      "epoch": 0.02745737867974244,
      "grad_norm": 21.262117385864258,
      "learning_rate": 8.708714996074325e-06,
      "loss": 1.1149,
      "step": 1049
    },
    {
      "epoch": 0.027483553492592527,
      "grad_norm": 17.424516677856445,
      "learning_rate": 8.705574456948443e-06,
      "loss": 0.5403,
      "step": 1050
    },
    {
      "epoch": 0.027509728305442616,
      "grad_norm": 21.398582458496094,
      "learning_rate": 8.702433917822559e-06,
      "loss": 0.6427,
      "step": 1051
    },
    {
      "epoch": 0.027535903118292702,
      "grad_norm": 12.129341125488281,
      "learning_rate": 8.699293378696676e-06,
      "loss": 0.2071,
      "step": 1052
    },
    {
      "epoch": 0.027562077931142792,
      "grad_norm": 23.26810646057129,
      "learning_rate": 8.696152839570794e-06,
      "loss": 0.9383,
      "step": 1053
    },
    {
      "epoch": 0.027588252743992882,
      "grad_norm": 14.787328720092773,
      "learning_rate": 8.69301230044491e-06,
      "loss": 0.5591,
      "step": 1054
    },
    {
      "epoch": 0.027614427556842968,
      "grad_norm": 16.443042755126953,
      "learning_rate": 8.689871761319027e-06,
      "loss": 0.8913,
      "step": 1055
    },
    {
      "epoch": 0.027640602369693058,
      "grad_norm": 22.455860137939453,
      "learning_rate": 8.686731222193143e-06,
      "loss": 0.8008,
      "step": 1056
    },
    {
      "epoch": 0.027666777182543144,
      "grad_norm": 12.400203704833984,
      "learning_rate": 8.68359068306726e-06,
      "loss": 0.5985,
      "step": 1057
    },
    {
      "epoch": 0.027692951995393233,
      "grad_norm": 23.49595069885254,
      "learning_rate": 8.680450143941377e-06,
      "loss": 0.8391,
      "step": 1058
    },
    {
      "epoch": 0.02771912680824332,
      "grad_norm": 26.503957748413086,
      "learning_rate": 8.677309604815494e-06,
      "loss": 0.9271,
      "step": 1059
    },
    {
      "epoch": 0.02774530162109341,
      "grad_norm": 12.038450241088867,
      "learning_rate": 8.674169065689612e-06,
      "loss": 0.7766,
      "step": 1060
    },
    {
      "epoch": 0.0277714764339435,
      "grad_norm": 17.184579849243164,
      "learning_rate": 8.671028526563726e-06,
      "loss": 0.8093,
      "step": 1061
    },
    {
      "epoch": 0.027797651246793585,
      "grad_norm": 20.5596923828125,
      "learning_rate": 8.667887987437843e-06,
      "loss": 0.7478,
      "step": 1062
    },
    {
      "epoch": 0.027823826059643675,
      "grad_norm": 16.137678146362305,
      "learning_rate": 8.66474744831196e-06,
      "loss": 0.5387,
      "step": 1063
    },
    {
      "epoch": 0.02785000087249376,
      "grad_norm": 23.701396942138672,
      "learning_rate": 8.661606909186077e-06,
      "loss": 0.5342,
      "step": 1064
    },
    {
      "epoch": 0.02787617568534385,
      "grad_norm": 23.95350456237793,
      "learning_rate": 8.658466370060194e-06,
      "loss": 0.4558,
      "step": 1065
    },
    {
      "epoch": 0.027902350498193937,
      "grad_norm": 24.890338897705078,
      "learning_rate": 8.65532583093431e-06,
      "loss": 1.1289,
      "step": 1066
    },
    {
      "epoch": 0.027928525311044027,
      "grad_norm": 23.081668853759766,
      "learning_rate": 8.652185291808428e-06,
      "loss": 0.7197,
      "step": 1067
    },
    {
      "epoch": 0.027954700123894113,
      "grad_norm": 21.151029586791992,
      "learning_rate": 8.649044752682544e-06,
      "loss": 1.4057,
      "step": 1068
    },
    {
      "epoch": 0.027980874936744202,
      "grad_norm": 16.886066436767578,
      "learning_rate": 8.645904213556661e-06,
      "loss": 1.0316,
      "step": 1069
    },
    {
      "epoch": 0.028007049749594292,
      "grad_norm": 16.506628036499023,
      "learning_rate": 8.642763674430777e-06,
      "loss": 0.5831,
      "step": 1070
    },
    {
      "epoch": 0.028033224562444378,
      "grad_norm": 14.477432250976562,
      "learning_rate": 8.639623135304895e-06,
      "loss": 0.7674,
      "step": 1071
    },
    {
      "epoch": 0.028059399375294468,
      "grad_norm": 18.04631805419922,
      "learning_rate": 8.63648259617901e-06,
      "loss": 0.5762,
      "step": 1072
    },
    {
      "epoch": 0.028085574188144554,
      "grad_norm": 18.638492584228516,
      "learning_rate": 8.633342057053128e-06,
      "loss": 0.798,
      "step": 1073
    },
    {
      "epoch": 0.028111749000994644,
      "grad_norm": 15.144615173339844,
      "learning_rate": 8.630201517927246e-06,
      "loss": 0.5755,
      "step": 1074
    },
    {
      "epoch": 0.02813792381384473,
      "grad_norm": 20.565866470336914,
      "learning_rate": 8.627060978801361e-06,
      "loss": 0.8479,
      "step": 1075
    },
    {
      "epoch": 0.02816409862669482,
      "grad_norm": 14.001458168029785,
      "learning_rate": 8.623920439675479e-06,
      "loss": 0.6734,
      "step": 1076
    },
    {
      "epoch": 0.028190273439544906,
      "grad_norm": 14.062703132629395,
      "learning_rate": 8.620779900549593e-06,
      "loss": 0.7343,
      "step": 1077
    },
    {
      "epoch": 0.028216448252394995,
      "grad_norm": 24.722597122192383,
      "learning_rate": 8.61763936142371e-06,
      "loss": 0.556,
      "step": 1078
    },
    {
      "epoch": 0.028242623065245085,
      "grad_norm": 17.72638702392578,
      "learning_rate": 8.614498822297828e-06,
      "loss": 0.9352,
      "step": 1079
    },
    {
      "epoch": 0.02826879787809517,
      "grad_norm": 19.15571403503418,
      "learning_rate": 8.611358283171944e-06,
      "loss": 0.5792,
      "step": 1080
    },
    {
      "epoch": 0.02829497269094526,
      "grad_norm": 17.305070877075195,
      "learning_rate": 8.608217744046062e-06,
      "loss": 0.5794,
      "step": 1081
    },
    {
      "epoch": 0.028321147503795347,
      "grad_norm": 12.248754501342773,
      "learning_rate": 8.605077204920178e-06,
      "loss": 0.8311,
      "step": 1082
    },
    {
      "epoch": 0.028347322316645437,
      "grad_norm": 10.051974296569824,
      "learning_rate": 8.601936665794295e-06,
      "loss": 0.3714,
      "step": 1083
    },
    {
      "epoch": 0.028373497129495523,
      "grad_norm": 16.07948112487793,
      "learning_rate": 8.598796126668411e-06,
      "loss": 0.5383,
      "step": 1084
    },
    {
      "epoch": 0.028399671942345613,
      "grad_norm": 15.120184898376465,
      "learning_rate": 8.595655587542528e-06,
      "loss": 0.5718,
      "step": 1085
    },
    {
      "epoch": 0.0284258467551957,
      "grad_norm": 12.277276039123535,
      "learning_rate": 8.592515048416646e-06,
      "loss": 0.5078,
      "step": 1086
    },
    {
      "epoch": 0.02845202156804579,
      "grad_norm": 14.57281494140625,
      "learning_rate": 8.589374509290762e-06,
      "loss": 0.4459,
      "step": 1087
    },
    {
      "epoch": 0.028478196380895878,
      "grad_norm": 12.37181282043457,
      "learning_rate": 8.58623397016488e-06,
      "loss": 0.3174,
      "step": 1088
    },
    {
      "epoch": 0.028504371193745964,
      "grad_norm": 44.744449615478516,
      "learning_rate": 8.583093431038995e-06,
      "loss": 0.9136,
      "step": 1089
    },
    {
      "epoch": 0.028530546006596054,
      "grad_norm": 18.840185165405273,
      "learning_rate": 8.579952891913113e-06,
      "loss": 0.9319,
      "step": 1090
    },
    {
      "epoch": 0.02855672081944614,
      "grad_norm": 13.691912651062012,
      "learning_rate": 8.576812352787229e-06,
      "loss": 0.5228,
      "step": 1091
    },
    {
      "epoch": 0.02858289563229623,
      "grad_norm": 22.378864288330078,
      "learning_rate": 8.573671813661345e-06,
      "loss": 1.1765,
      "step": 1092
    },
    {
      "epoch": 0.028609070445146316,
      "grad_norm": 11.149384498596191,
      "learning_rate": 8.570531274535462e-06,
      "loss": 0.5181,
      "step": 1093
    },
    {
      "epoch": 0.028635245257996406,
      "grad_norm": 17.15279769897461,
      "learning_rate": 8.567390735409578e-06,
      "loss": 0.2792,
      "step": 1094
    },
    {
      "epoch": 0.028661420070846492,
      "grad_norm": 19.948707580566406,
      "learning_rate": 8.564250196283696e-06,
      "loss": 0.6726,
      "step": 1095
    },
    {
      "epoch": 0.02868759488369658,
      "grad_norm": 12.256278991699219,
      "learning_rate": 8.561109657157811e-06,
      "loss": 0.4863,
      "step": 1096
    },
    {
      "epoch": 0.02871376969654667,
      "grad_norm": 31.710067749023438,
      "learning_rate": 8.557969118031929e-06,
      "loss": 0.5402,
      "step": 1097
    },
    {
      "epoch": 0.028739944509396757,
      "grad_norm": 20.61297035217285,
      "learning_rate": 8.554828578906045e-06,
      "loss": 0.7936,
      "step": 1098
    },
    {
      "epoch": 0.028766119322246847,
      "grad_norm": 17.21793556213379,
      "learning_rate": 8.551688039780162e-06,
      "loss": 0.6017,
      "step": 1099
    },
    {
      "epoch": 0.028792294135096933,
      "grad_norm": 11.220561027526855,
      "learning_rate": 8.54854750065428e-06,
      "loss": 0.6992,
      "step": 1100
    },
    {
      "epoch": 0.028818468947947023,
      "grad_norm": 21.484561920166016,
      "learning_rate": 8.545406961528396e-06,
      "loss": 0.983,
      "step": 1101
    },
    {
      "epoch": 0.02884464376079711,
      "grad_norm": 16.25380516052246,
      "learning_rate": 8.542266422402513e-06,
      "loss": 1.1884,
      "step": 1102
    },
    {
      "epoch": 0.0288708185736472,
      "grad_norm": 25.784748077392578,
      "learning_rate": 8.53912588327663e-06,
      "loss": 1.3922,
      "step": 1103
    },
    {
      "epoch": 0.028896993386497285,
      "grad_norm": 18.808609008789062,
      "learning_rate": 8.535985344150747e-06,
      "loss": 0.5598,
      "step": 1104
    },
    {
      "epoch": 0.028923168199347375,
      "grad_norm": 21.099699020385742,
      "learning_rate": 8.532844805024863e-06,
      "loss": 0.8535,
      "step": 1105
    },
    {
      "epoch": 0.028949343012197464,
      "grad_norm": 13.567341804504395,
      "learning_rate": 8.52970426589898e-06,
      "loss": 0.4399,
      "step": 1106
    },
    {
      "epoch": 0.02897551782504755,
      "grad_norm": 14.692951202392578,
      "learning_rate": 8.526563726773098e-06,
      "loss": 0.5651,
      "step": 1107
    },
    {
      "epoch": 0.02900169263789764,
      "grad_norm": 20.135211944580078,
      "learning_rate": 8.523423187647212e-06,
      "loss": 0.886,
      "step": 1108
    },
    {
      "epoch": 0.029027867450747726,
      "grad_norm": 15.79466438293457,
      "learning_rate": 8.52028264852133e-06,
      "loss": 0.6142,
      "step": 1109
    },
    {
      "epoch": 0.029054042263597816,
      "grad_norm": 22.539276123046875,
      "learning_rate": 8.517142109395445e-06,
      "loss": 0.5389,
      "step": 1110
    },
    {
      "epoch": 0.029080217076447902,
      "grad_norm": 17.808666229248047,
      "learning_rate": 8.514001570269563e-06,
      "loss": 0.5496,
      "step": 1111
    },
    {
      "epoch": 0.02910639188929799,
      "grad_norm": 21.386821746826172,
      "learning_rate": 8.51086103114368e-06,
      "loss": 0.5362,
      "step": 1112
    },
    {
      "epoch": 0.02913256670214808,
      "grad_norm": 19.760534286499023,
      "learning_rate": 8.507720492017796e-06,
      "loss": 1.3748,
      "step": 1113
    },
    {
      "epoch": 0.029158741514998168,
      "grad_norm": 18.82125473022461,
      "learning_rate": 8.504579952891914e-06,
      "loss": 1.1346,
      "step": 1114
    },
    {
      "epoch": 0.029184916327848257,
      "grad_norm": 15.346317291259766,
      "learning_rate": 8.50143941376603e-06,
      "loss": 0.7954,
      "step": 1115
    },
    {
      "epoch": 0.029211091140698343,
      "grad_norm": 14.889585494995117,
      "learning_rate": 8.498298874640147e-06,
      "loss": 0.6926,
      "step": 1116
    },
    {
      "epoch": 0.029237265953548433,
      "grad_norm": 13.870555877685547,
      "learning_rate": 8.495158335514263e-06,
      "loss": 0.5275,
      "step": 1117
    },
    {
      "epoch": 0.02926344076639852,
      "grad_norm": 15.321096420288086,
      "learning_rate": 8.49201779638838e-06,
      "loss": 0.784,
      "step": 1118
    },
    {
      "epoch": 0.02928961557924861,
      "grad_norm": 16.731369018554688,
      "learning_rate": 8.488877257262496e-06,
      "loss": 1.0845,
      "step": 1119
    },
    {
      "epoch": 0.029315790392098695,
      "grad_norm": 21.339454650878906,
      "learning_rate": 8.485736718136614e-06,
      "loss": 0.8838,
      "step": 1120
    },
    {
      "epoch": 0.029341965204948785,
      "grad_norm": 18.006811141967773,
      "learning_rate": 8.482596179010732e-06,
      "loss": 0.539,
      "step": 1121
    },
    {
      "epoch": 0.029368140017798874,
      "grad_norm": 21.32979393005371,
      "learning_rate": 8.479455639884847e-06,
      "loss": 0.919,
      "step": 1122
    },
    {
      "epoch": 0.02939431483064896,
      "grad_norm": 21.35277557373047,
      "learning_rate": 8.476315100758963e-06,
      "loss": 0.5432,
      "step": 1123
    },
    {
      "epoch": 0.02942048964349905,
      "grad_norm": 18.464923858642578,
      "learning_rate": 8.47317456163308e-06,
      "loss": 0.737,
      "step": 1124
    },
    {
      "epoch": 0.029446664456349136,
      "grad_norm": 13.816466331481934,
      "learning_rate": 8.470034022507197e-06,
      "loss": 0.493,
      "step": 1125
    },
    {
      "epoch": 0.029472839269199226,
      "grad_norm": 20.707138061523438,
      "learning_rate": 8.466893483381314e-06,
      "loss": 0.6488,
      "step": 1126
    },
    {
      "epoch": 0.029499014082049312,
      "grad_norm": 9.800002098083496,
      "learning_rate": 8.46375294425543e-06,
      "loss": 0.1409,
      "step": 1127
    },
    {
      "epoch": 0.029525188894899402,
      "grad_norm": 12.881309509277344,
      "learning_rate": 8.460612405129548e-06,
      "loss": 0.8813,
      "step": 1128
    },
    {
      "epoch": 0.029551363707749488,
      "grad_norm": 16.126968383789062,
      "learning_rate": 8.457471866003664e-06,
      "loss": 0.7646,
      "step": 1129
    },
    {
      "epoch": 0.029577538520599578,
      "grad_norm": 27.870956420898438,
      "learning_rate": 8.454331326877781e-06,
      "loss": 0.6939,
      "step": 1130
    },
    {
      "epoch": 0.029603713333449667,
      "grad_norm": 10.35516357421875,
      "learning_rate": 8.451190787751897e-06,
      "loss": 0.4844,
      "step": 1131
    },
    {
      "epoch": 0.029629888146299754,
      "grad_norm": 19.119333267211914,
      "learning_rate": 8.448050248626015e-06,
      "loss": 0.4759,
      "step": 1132
    },
    {
      "epoch": 0.029656062959149843,
      "grad_norm": 13.546180725097656,
      "learning_rate": 8.444909709500132e-06,
      "loss": 0.2931,
      "step": 1133
    },
    {
      "epoch": 0.02968223777199993,
      "grad_norm": 13.884352684020996,
      "learning_rate": 8.441769170374248e-06,
      "loss": 0.5421,
      "step": 1134
    },
    {
      "epoch": 0.02970841258485002,
      "grad_norm": 20.500089645385742,
      "learning_rate": 8.438628631248365e-06,
      "loss": 0.9405,
      "step": 1135
    },
    {
      "epoch": 0.029734587397700105,
      "grad_norm": 21.481983184814453,
      "learning_rate": 8.435488092122481e-06,
      "loss": 0.6657,
      "step": 1136
    },
    {
      "epoch": 0.029760762210550195,
      "grad_norm": 12.30611801147461,
      "learning_rate": 8.432347552996599e-06,
      "loss": 0.4874,
      "step": 1137
    },
    {
      "epoch": 0.02978693702340028,
      "grad_norm": 23.186002731323242,
      "learning_rate": 8.429207013870715e-06,
      "loss": 1.1045,
      "step": 1138
    },
    {
      "epoch": 0.02981311183625037,
      "grad_norm": 19.82857322692871,
      "learning_rate": 8.42606647474483e-06,
      "loss": 0.4769,
      "step": 1139
    },
    {
      "epoch": 0.02983928664910046,
      "grad_norm": 8.644344329833984,
      "learning_rate": 8.422925935618948e-06,
      "loss": 0.2814,
      "step": 1140
    },
    {
      "epoch": 0.029865461461950547,
      "grad_norm": 15.507308959960938,
      "learning_rate": 8.419785396493064e-06,
      "loss": 0.3227,
      "step": 1141
    },
    {
      "epoch": 0.029891636274800636,
      "grad_norm": 19.12523651123047,
      "learning_rate": 8.416644857367182e-06,
      "loss": 0.796,
      "step": 1142
    },
    {
      "epoch": 0.029917811087650723,
      "grad_norm": 17.373798370361328,
      "learning_rate": 8.413504318241297e-06,
      "loss": 0.5396,
      "step": 1143
    },
    {
      "epoch": 0.029943985900500812,
      "grad_norm": 21.202472686767578,
      "learning_rate": 8.410363779115415e-06,
      "loss": 0.7738,
      "step": 1144
    },
    {
      "epoch": 0.0299701607133509,
      "grad_norm": 20.175094604492188,
      "learning_rate": 8.407223239989531e-06,
      "loss": 0.6215,
      "step": 1145
    },
    {
      "epoch": 0.029996335526200988,
      "grad_norm": 18.285850524902344,
      "learning_rate": 8.404082700863648e-06,
      "loss": 0.4302,
      "step": 1146
    },
    {
      "epoch": 0.030022510339051074,
      "grad_norm": 12.60654354095459,
      "learning_rate": 8.400942161737766e-06,
      "loss": 0.2594,
      "step": 1147
    },
    {
      "epoch": 0.030048685151901164,
      "grad_norm": 16.586149215698242,
      "learning_rate": 8.397801622611882e-06,
      "loss": 0.599,
      "step": 1148
    },
    {
      "epoch": 0.030074859964751254,
      "grad_norm": 23.544668197631836,
      "learning_rate": 8.394661083486e-06,
      "loss": 0.5283,
      "step": 1149
    },
    {
      "epoch": 0.03010103477760134,
      "grad_norm": 23.43476676940918,
      "learning_rate": 8.391520544360115e-06,
      "loss": 0.8529,
      "step": 1150
    },
    {
      "epoch": 0.03012720959045143,
      "grad_norm": 18.024456024169922,
      "learning_rate": 8.388380005234233e-06,
      "loss": 0.8219,
      "step": 1151
    },
    {
      "epoch": 0.030153384403301516,
      "grad_norm": 23.987863540649414,
      "learning_rate": 8.385239466108349e-06,
      "loss": 0.799,
      "step": 1152
    },
    {
      "epoch": 0.030179559216151605,
      "grad_norm": 17.911882400512695,
      "learning_rate": 8.382098926982466e-06,
      "loss": 0.8065,
      "step": 1153
    },
    {
      "epoch": 0.03020573402900169,
      "grad_norm": 17.784088134765625,
      "learning_rate": 8.378958387856582e-06,
      "loss": 0.379,
      "step": 1154
    },
    {
      "epoch": 0.03023190884185178,
      "grad_norm": 18.194034576416016,
      "learning_rate": 8.375817848730698e-06,
      "loss": 0.6176,
      "step": 1155
    },
    {
      "epoch": 0.030258083654701867,
      "grad_norm": 21.927547454833984,
      "learning_rate": 8.372677309604815e-06,
      "loss": 0.7139,
      "step": 1156
    },
    {
      "epoch": 0.030284258467551957,
      "grad_norm": 12.75497817993164,
      "learning_rate": 8.369536770478931e-06,
      "loss": 0.2147,
      "step": 1157
    },
    {
      "epoch": 0.030310433280402047,
      "grad_norm": 16.065547943115234,
      "learning_rate": 8.366396231353049e-06,
      "loss": 0.9247,
      "step": 1158
    },
    {
      "epoch": 0.030336608093252133,
      "grad_norm": 17.36803436279297,
      "learning_rate": 8.363255692227166e-06,
      "loss": 0.8788,
      "step": 1159
    },
    {
      "epoch": 0.030362782906102222,
      "grad_norm": 17.176185607910156,
      "learning_rate": 8.360115153101282e-06,
      "loss": 0.5058,
      "step": 1160
    },
    {
      "epoch": 0.03038895771895231,
      "grad_norm": 17.31911277770996,
      "learning_rate": 8.3569746139754e-06,
      "loss": 0.5055,
      "step": 1161
    },
    {
      "epoch": 0.030415132531802398,
      "grad_norm": 16.709016799926758,
      "learning_rate": 8.353834074849516e-06,
      "loss": 0.4994,
      "step": 1162
    },
    {
      "epoch": 0.030441307344652484,
      "grad_norm": 11.271753311157227,
      "learning_rate": 8.350693535723633e-06,
      "loss": 0.2866,
      "step": 1163
    },
    {
      "epoch": 0.030467482157502574,
      "grad_norm": 18.358062744140625,
      "learning_rate": 8.347552996597749e-06,
      "loss": 0.4359,
      "step": 1164
    },
    {
      "epoch": 0.03049365697035266,
      "grad_norm": 17.194562911987305,
      "learning_rate": 8.344412457471867e-06,
      "loss": 0.3288,
      "step": 1165
    },
    {
      "epoch": 0.03051983178320275,
      "grad_norm": 17.716796875,
      "learning_rate": 8.341271918345983e-06,
      "loss": 0.5438,
      "step": 1166
    },
    {
      "epoch": 0.03054600659605284,
      "grad_norm": 15.6380033493042,
      "learning_rate": 8.3381313792201e-06,
      "loss": 0.3328,
      "step": 1167
    },
    {
      "epoch": 0.030572181408902926,
      "grad_norm": 12.974352836608887,
      "learning_rate": 8.334990840094218e-06,
      "loss": 0.4339,
      "step": 1168
    },
    {
      "epoch": 0.030598356221753015,
      "grad_norm": 20.544687271118164,
      "learning_rate": 8.331850300968332e-06,
      "loss": 0.5496,
      "step": 1169
    },
    {
      "epoch": 0.0306245310346031,
      "grad_norm": 20.235158920288086,
      "learning_rate": 8.32870976184245e-06,
      "loss": 0.4429,
      "step": 1170
    },
    {
      "epoch": 0.03065070584745319,
      "grad_norm": 20.41921043395996,
      "learning_rate": 8.325569222716565e-06,
      "loss": 0.5181,
      "step": 1171
    },
    {
      "epoch": 0.030676880660303277,
      "grad_norm": 18.73533821105957,
      "learning_rate": 8.322428683590683e-06,
      "loss": 0.8959,
      "step": 1172
    },
    {
      "epoch": 0.030703055473153367,
      "grad_norm": 20.244863510131836,
      "learning_rate": 8.3192881444648e-06,
      "loss": 0.9707,
      "step": 1173
    },
    {
      "epoch": 0.030729230286003457,
      "grad_norm": 39.13740158081055,
      "learning_rate": 8.316147605338916e-06,
      "loss": 0.9684,
      "step": 1174
    },
    {
      "epoch": 0.030755405098853543,
      "grad_norm": 23.43578338623047,
      "learning_rate": 8.313007066213034e-06,
      "loss": 0.3976,
      "step": 1175
    },
    {
      "epoch": 0.030781579911703633,
      "grad_norm": 16.28723907470703,
      "learning_rate": 8.30986652708715e-06,
      "loss": 0.4634,
      "step": 1176
    },
    {
      "epoch": 0.03080775472455372,
      "grad_norm": 28.243955612182617,
      "learning_rate": 8.306725987961267e-06,
      "loss": 0.642,
      "step": 1177
    },
    {
      "epoch": 0.03083392953740381,
      "grad_norm": 15.996028900146484,
      "learning_rate": 8.303585448835383e-06,
      "loss": 0.8636,
      "step": 1178
    },
    {
      "epoch": 0.030860104350253895,
      "grad_norm": 17.182666778564453,
      "learning_rate": 8.3004449097095e-06,
      "loss": 0.903,
      "step": 1179
    },
    {
      "epoch": 0.030886279163103984,
      "grad_norm": 20.0072078704834,
      "learning_rate": 8.297304370583618e-06,
      "loss": 0.7165,
      "step": 1180
    },
    {
      "epoch": 0.03091245397595407,
      "grad_norm": 36.47212219238281,
      "learning_rate": 8.294163831457734e-06,
      "loss": 0.7429,
      "step": 1181
    },
    {
      "epoch": 0.03093862878880416,
      "grad_norm": 31.582866668701172,
      "learning_rate": 8.291023292331852e-06,
      "loss": 0.5796,
      "step": 1182
    },
    {
      "epoch": 0.03096480360165425,
      "grad_norm": 28.1096134185791,
      "learning_rate": 8.287882753205967e-06,
      "loss": 0.7944,
      "step": 1183
    },
    {
      "epoch": 0.030990978414504336,
      "grad_norm": 13.248345375061035,
      "learning_rate": 8.284742214080085e-06,
      "loss": 0.4711,
      "step": 1184
    },
    {
      "epoch": 0.031017153227354426,
      "grad_norm": 24.967899322509766,
      "learning_rate": 8.2816016749542e-06,
      "loss": 0.6011,
      "step": 1185
    },
    {
      "epoch": 0.031043328040204512,
      "grad_norm": 15.102028846740723,
      "learning_rate": 8.278461135828317e-06,
      "loss": 0.9385,
      "step": 1186
    },
    {
      "epoch": 0.0310695028530546,
      "grad_norm": 33.186256408691406,
      "learning_rate": 8.275320596702434e-06,
      "loss": 0.5939,
      "step": 1187
    },
    {
      "epoch": 0.031095677665904688,
      "grad_norm": 27.179737091064453,
      "learning_rate": 8.27218005757655e-06,
      "loss": 0.6041,
      "step": 1188
    },
    {
      "epoch": 0.031121852478754777,
      "grad_norm": 18.57811737060547,
      "learning_rate": 8.269039518450668e-06,
      "loss": 0.8245,
      "step": 1189
    },
    {
      "epoch": 0.031148027291604864,
      "grad_norm": 22.103654861450195,
      "learning_rate": 8.265898979324783e-06,
      "loss": 0.8785,
      "step": 1190
    },
    {
      "epoch": 0.031174202104454953,
      "grad_norm": 17.225234985351562,
      "learning_rate": 8.262758440198901e-06,
      "loss": 0.5336,
      "step": 1191
    },
    {
      "epoch": 0.031200376917305043,
      "grad_norm": 17.45276641845703,
      "learning_rate": 8.259617901073017e-06,
      "loss": 0.5438,
      "step": 1192
    },
    {
      "epoch": 0.03122655173015513,
      "grad_norm": 31.36673355102539,
      "learning_rate": 8.256477361947134e-06,
      "loss": 0.6897,
      "step": 1193
    },
    {
      "epoch": 0.03125272654300522,
      "grad_norm": 13.989961624145508,
      "learning_rate": 8.253336822821252e-06,
      "loss": 0.5771,
      "step": 1194
    },
    {
      "epoch": 0.03127890135585531,
      "grad_norm": 16.098468780517578,
      "learning_rate": 8.250196283695368e-06,
      "loss": 0.5694,
      "step": 1195
    },
    {
      "epoch": 0.03130507616870539,
      "grad_norm": 19.139297485351562,
      "learning_rate": 8.247055744569485e-06,
      "loss": 0.5178,
      "step": 1196
    },
    {
      "epoch": 0.03133125098155548,
      "grad_norm": 19.031688690185547,
      "learning_rate": 8.243915205443601e-06,
      "loss": 0.9542,
      "step": 1197
    },
    {
      "epoch": 0.03135742579440557,
      "grad_norm": 19.0145206451416,
      "learning_rate": 8.240774666317719e-06,
      "loss": 0.6227,
      "step": 1198
    },
    {
      "epoch": 0.03138360060725566,
      "grad_norm": 11.67270278930664,
      "learning_rate": 8.237634127191835e-06,
      "loss": 0.481,
      "step": 1199
    },
    {
      "epoch": 0.03140977542010575,
      "grad_norm": 23.55051612854004,
      "learning_rate": 8.23449358806595e-06,
      "loss": 0.5623,
      "step": 1200
    },
    {
      "epoch": 0.03143595023295583,
      "grad_norm": 14.93101978302002,
      "learning_rate": 8.231353048940068e-06,
      "loss": 0.4639,
      "step": 1201
    },
    {
      "epoch": 0.03146212504580592,
      "grad_norm": 17.78436279296875,
      "learning_rate": 8.228212509814184e-06,
      "loss": 0.6971,
      "step": 1202
    },
    {
      "epoch": 0.03148829985865601,
      "grad_norm": 11.325952529907227,
      "learning_rate": 8.225071970688302e-06,
      "loss": 0.4083,
      "step": 1203
    },
    {
      "epoch": 0.0315144746715061,
      "grad_norm": 26.195804595947266,
      "learning_rate": 8.221931431562417e-06,
      "loss": 0.5792,
      "step": 1204
    },
    {
      "epoch": 0.031540649484356184,
      "grad_norm": 22.585079193115234,
      "learning_rate": 8.218790892436535e-06,
      "loss": 0.5056,
      "step": 1205
    },
    {
      "epoch": 0.031566824297206274,
      "grad_norm": 14.808723449707031,
      "learning_rate": 8.215650353310652e-06,
      "loss": 0.6428,
      "step": 1206
    },
    {
      "epoch": 0.03159299911005636,
      "grad_norm": 11.634310722351074,
      "learning_rate": 8.212509814184768e-06,
      "loss": 0.4184,
      "step": 1207
    },
    {
      "epoch": 0.03161917392290645,
      "grad_norm": 13.626801490783691,
      "learning_rate": 8.209369275058886e-06,
      "loss": 0.4627,
      "step": 1208
    },
    {
      "epoch": 0.03164534873575654,
      "grad_norm": 36.463348388671875,
      "learning_rate": 8.206228735933002e-06,
      "loss": 1.0859,
      "step": 1209
    },
    {
      "epoch": 0.031671523548606625,
      "grad_norm": 23.412649154663086,
      "learning_rate": 8.20308819680712e-06,
      "loss": 0.8266,
      "step": 1210
    },
    {
      "epoch": 0.031697698361456715,
      "grad_norm": 14.014134407043457,
      "learning_rate": 8.199947657681235e-06,
      "loss": 0.5628,
      "step": 1211
    },
    {
      "epoch": 0.031723873174306805,
      "grad_norm": 12.214584350585938,
      "learning_rate": 8.196807118555353e-06,
      "loss": 0.6562,
      "step": 1212
    },
    {
      "epoch": 0.031750047987156894,
      "grad_norm": 22.531726837158203,
      "learning_rate": 8.193666579429469e-06,
      "loss": 0.8606,
      "step": 1213
    },
    {
      "epoch": 0.03177622280000698,
      "grad_norm": 15.791736602783203,
      "learning_rate": 8.190526040303586e-06,
      "loss": 0.4822,
      "step": 1214
    },
    {
      "epoch": 0.03180239761285707,
      "grad_norm": 17.225994110107422,
      "learning_rate": 8.187385501177704e-06,
      "loss": 0.4605,
      "step": 1215
    },
    {
      "epoch": 0.031828572425707156,
      "grad_norm": 20.983505249023438,
      "learning_rate": 8.184244962051818e-06,
      "loss": 1.2678,
      "step": 1216
    },
    {
      "epoch": 0.031854747238557246,
      "grad_norm": 19.484222412109375,
      "learning_rate": 8.181104422925935e-06,
      "loss": 0.4738,
      "step": 1217
    },
    {
      "epoch": 0.031880922051407336,
      "grad_norm": 16.228822708129883,
      "learning_rate": 8.177963883800053e-06,
      "loss": 0.3947,
      "step": 1218
    },
    {
      "epoch": 0.03190709686425742,
      "grad_norm": 12.191750526428223,
      "learning_rate": 8.174823344674169e-06,
      "loss": 0.1781,
      "step": 1219
    },
    {
      "epoch": 0.03193327167710751,
      "grad_norm": 19.48598289489746,
      "learning_rate": 8.171682805548286e-06,
      "loss": 0.5076,
      "step": 1220
    },
    {
      "epoch": 0.0319594464899576,
      "grad_norm": 25.772581100463867,
      "learning_rate": 8.168542266422402e-06,
      "loss": 0.7241,
      "step": 1221
    },
    {
      "epoch": 0.03198562130280769,
      "grad_norm": 16.355422973632812,
      "learning_rate": 8.16540172729652e-06,
      "loss": 0.8785,
      "step": 1222
    },
    {
      "epoch": 0.03201179611565777,
      "grad_norm": 18.22478485107422,
      "learning_rate": 8.162261188170636e-06,
      "loss": 0.8397,
      "step": 1223
    },
    {
      "epoch": 0.03203797092850786,
      "grad_norm": 24.15901756286621,
      "learning_rate": 8.159120649044753e-06,
      "loss": 0.9735,
      "step": 1224
    },
    {
      "epoch": 0.03206414574135795,
      "grad_norm": 21.81362533569336,
      "learning_rate": 8.155980109918869e-06,
      "loss": 0.495,
      "step": 1225
    },
    {
      "epoch": 0.03209032055420804,
      "grad_norm": 14.11471939086914,
      "learning_rate": 8.152839570792987e-06,
      "loss": 0.9597,
      "step": 1226
    },
    {
      "epoch": 0.03211649536705813,
      "grad_norm": 17.33785629272461,
      "learning_rate": 8.149699031667104e-06,
      "loss": 0.6335,
      "step": 1227
    },
    {
      "epoch": 0.03214267017990821,
      "grad_norm": 14.59998893737793,
      "learning_rate": 8.14655849254122e-06,
      "loss": 0.5031,
      "step": 1228
    },
    {
      "epoch": 0.0321688449927583,
      "grad_norm": 22.87152099609375,
      "learning_rate": 8.143417953415338e-06,
      "loss": 0.9242,
      "step": 1229
    },
    {
      "epoch": 0.03219501980560839,
      "grad_norm": 15.980381965637207,
      "learning_rate": 8.140277414289453e-06,
      "loss": 0.3673,
      "step": 1230
    },
    {
      "epoch": 0.03222119461845848,
      "grad_norm": 15.866798400878906,
      "learning_rate": 8.13713687516357e-06,
      "loss": 0.6735,
      "step": 1231
    },
    {
      "epoch": 0.03224736943130856,
      "grad_norm": 15.759687423706055,
      "learning_rate": 8.133996336037687e-06,
      "loss": 0.6618,
      "step": 1232
    },
    {
      "epoch": 0.03227354424415865,
      "grad_norm": 27.058881759643555,
      "learning_rate": 8.130855796911803e-06,
      "loss": 0.9016,
      "step": 1233
    },
    {
      "epoch": 0.03229971905700874,
      "grad_norm": 15.65758228302002,
      "learning_rate": 8.12771525778592e-06,
      "loss": 0.2707,
      "step": 1234
    },
    {
      "epoch": 0.03232589386985883,
      "grad_norm": 20.938926696777344,
      "learning_rate": 8.124574718660036e-06,
      "loss": 0.6057,
      "step": 1235
    },
    {
      "epoch": 0.03235206868270892,
      "grad_norm": 27.787803649902344,
      "learning_rate": 8.121434179534154e-06,
      "loss": 0.5888,
      "step": 1236
    },
    {
      "epoch": 0.032378243495559005,
      "grad_norm": 19.271480560302734,
      "learning_rate": 8.11829364040827e-06,
      "loss": 0.8449,
      "step": 1237
    },
    {
      "epoch": 0.032404418308409094,
      "grad_norm": 20.993968963623047,
      "learning_rate": 8.115153101282387e-06,
      "loss": 0.674,
      "step": 1238
    },
    {
      "epoch": 0.032430593121259184,
      "grad_norm": 18.995145797729492,
      "learning_rate": 8.112012562156503e-06,
      "loss": 0.5012,
      "step": 1239
    },
    {
      "epoch": 0.032456767934109274,
      "grad_norm": 16.363677978515625,
      "learning_rate": 8.10887202303062e-06,
      "loss": 0.6249,
      "step": 1240
    },
    {
      "epoch": 0.032482942746959356,
      "grad_norm": 19.010217666625977,
      "learning_rate": 8.105731483904738e-06,
      "loss": 0.6131,
      "step": 1241
    },
    {
      "epoch": 0.032509117559809446,
      "grad_norm": 16.351497650146484,
      "learning_rate": 8.102590944778854e-06,
      "loss": 0.9537,
      "step": 1242
    },
    {
      "epoch": 0.032535292372659536,
      "grad_norm": 17.567142486572266,
      "learning_rate": 8.099450405652971e-06,
      "loss": 0.4697,
      "step": 1243
    },
    {
      "epoch": 0.032561467185509625,
      "grad_norm": 12.464038848876953,
      "learning_rate": 8.096309866527087e-06,
      "loss": 0.4733,
      "step": 1244
    },
    {
      "epoch": 0.032587641998359715,
      "grad_norm": 18.40229606628418,
      "learning_rate": 8.093169327401205e-06,
      "loss": 0.6218,
      "step": 1245
    },
    {
      "epoch": 0.0326138168112098,
      "grad_norm": 24.34620475769043,
      "learning_rate": 8.09002878827532e-06,
      "loss": 0.5618,
      "step": 1246
    },
    {
      "epoch": 0.03263999162405989,
      "grad_norm": 14.909196853637695,
      "learning_rate": 8.086888249149437e-06,
      "loss": 0.1699,
      "step": 1247
    },
    {
      "epoch": 0.03266616643690998,
      "grad_norm": 14.369684219360352,
      "learning_rate": 8.083747710023554e-06,
      "loss": 0.6847,
      "step": 1248
    },
    {
      "epoch": 0.03269234124976007,
      "grad_norm": 22.71152687072754,
      "learning_rate": 8.08060717089767e-06,
      "loss": 0.6789,
      "step": 1249
    },
    {
      "epoch": 0.03271851606261015,
      "grad_norm": 15.938042640686035,
      "learning_rate": 8.077466631771788e-06,
      "loss": 0.6214,
      "step": 1250
    },
    {
      "epoch": 0.03274469087546024,
      "grad_norm": 13.259037971496582,
      "learning_rate": 8.074326092645903e-06,
      "loss": 0.6184,
      "step": 1251
    },
    {
      "epoch": 0.03277086568831033,
      "grad_norm": 14.734495162963867,
      "learning_rate": 8.071185553520021e-06,
      "loss": 0.8519,
      "step": 1252
    },
    {
      "epoch": 0.03279704050116042,
      "grad_norm": 26.710166931152344,
      "learning_rate": 8.068045014394139e-06,
      "loss": 1.023,
      "step": 1253
    },
    {
      "epoch": 0.03282321531401051,
      "grad_norm": 15.79175853729248,
      "learning_rate": 8.064904475268254e-06,
      "loss": 0.4074,
      "step": 1254
    },
    {
      "epoch": 0.03284939012686059,
      "grad_norm": 16.635238647460938,
      "learning_rate": 8.061763936142372e-06,
      "loss": 0.4321,
      "step": 1255
    },
    {
      "epoch": 0.03287556493971068,
      "grad_norm": 19.640138626098633,
      "learning_rate": 8.058623397016488e-06,
      "loss": 0.4544,
      "step": 1256
    },
    {
      "epoch": 0.03290173975256077,
      "grad_norm": 18.679733276367188,
      "learning_rate": 8.055482857890605e-06,
      "loss": 0.9701,
      "step": 1257
    },
    {
      "epoch": 0.03292791456541086,
      "grad_norm": 11.439140319824219,
      "learning_rate": 8.052342318764721e-06,
      "loss": 0.4093,
      "step": 1258
    },
    {
      "epoch": 0.03295408937826094,
      "grad_norm": 21.076034545898438,
      "learning_rate": 8.049201779638839e-06,
      "loss": 0.5143,
      "step": 1259
    },
    {
      "epoch": 0.03298026419111103,
      "grad_norm": 15.715840339660645,
      "learning_rate": 8.046061240512955e-06,
      "loss": 0.2997,
      "step": 1260
    },
    {
      "epoch": 0.03300643900396112,
      "grad_norm": 17.428903579711914,
      "learning_rate": 8.042920701387072e-06,
      "loss": 0.1823,
      "step": 1261
    },
    {
      "epoch": 0.03303261381681121,
      "grad_norm": 14.643452644348145,
      "learning_rate": 8.039780162261188e-06,
      "loss": 0.5714,
      "step": 1262
    },
    {
      "epoch": 0.0330587886296613,
      "grad_norm": 16.547447204589844,
      "learning_rate": 8.036639623135304e-06,
      "loss": 0.7596,
      "step": 1263
    },
    {
      "epoch": 0.033084963442511384,
      "grad_norm": 16.34348487854004,
      "learning_rate": 8.033499084009421e-06,
      "loss": 0.6656,
      "step": 1264
    },
    {
      "epoch": 0.03311113825536147,
      "grad_norm": 16.112348556518555,
      "learning_rate": 8.030358544883539e-06,
      "loss": 0.5953,
      "step": 1265
    },
    {
      "epoch": 0.03313731306821156,
      "grad_norm": 24.08806800842285,
      "learning_rate": 8.027218005757655e-06,
      "loss": 0.8227,
      "step": 1266
    },
    {
      "epoch": 0.03316348788106165,
      "grad_norm": 15.168018341064453,
      "learning_rate": 8.024077466631772e-06,
      "loss": 0.7166,
      "step": 1267
    },
    {
      "epoch": 0.033189662693911735,
      "grad_norm": 16.91170883178711,
      "learning_rate": 8.020936927505888e-06,
      "loss": 0.6971,
      "step": 1268
    },
    {
      "epoch": 0.033215837506761825,
      "grad_norm": 16.468585968017578,
      "learning_rate": 8.017796388380006e-06,
      "loss": 0.4167,
      "step": 1269
    },
    {
      "epoch": 0.033242012319611915,
      "grad_norm": 16.293603897094727,
      "learning_rate": 8.014655849254122e-06,
      "loss": 0.6306,
      "step": 1270
    },
    {
      "epoch": 0.033268187132462004,
      "grad_norm": 13.426785469055176,
      "learning_rate": 8.01151531012824e-06,
      "loss": 0.5061,
      "step": 1271
    },
    {
      "epoch": 0.033294361945312094,
      "grad_norm": 11.03808307647705,
      "learning_rate": 8.008374771002355e-06,
      "loss": 0.3767,
      "step": 1272
    },
    {
      "epoch": 0.03332053675816218,
      "grad_norm": 19.42428207397461,
      "learning_rate": 8.005234231876473e-06,
      "loss": 0.6803,
      "step": 1273
    },
    {
      "epoch": 0.033346711571012266,
      "grad_norm": 23.124048233032227,
      "learning_rate": 8.00209369275059e-06,
      "loss": 0.8296,
      "step": 1274
    },
    {
      "epoch": 0.033372886383862356,
      "grad_norm": 15.580513954162598,
      "learning_rate": 7.998953153624706e-06,
      "loss": 0.3647,
      "step": 1275
    },
    {
      "epoch": 0.033399061196712446,
      "grad_norm": 13.890544891357422,
      "learning_rate": 7.995812614498824e-06,
      "loss": 0.7299,
      "step": 1276
    },
    {
      "epoch": 0.03342523600956253,
      "grad_norm": 17.880224227905273,
      "learning_rate": 7.99267207537294e-06,
      "loss": 0.547,
      "step": 1277
    },
    {
      "epoch": 0.03345141082241262,
      "grad_norm": 20.92080307006836,
      "learning_rate": 7.989531536247055e-06,
      "loss": 1.0129,
      "step": 1278
    },
    {
      "epoch": 0.03347758563526271,
      "grad_norm": 15.24743938446045,
      "learning_rate": 7.986390997121173e-06,
      "loss": 0.6501,
      "step": 1279
    },
    {
      "epoch": 0.0335037604481128,
      "grad_norm": 22.461286544799805,
      "learning_rate": 7.983250457995289e-06,
      "loss": 0.4712,
      "step": 1280
    },
    {
      "epoch": 0.03352993526096289,
      "grad_norm": 24.02013397216797,
      "learning_rate": 7.980109918869406e-06,
      "loss": 0.7796,
      "step": 1281
    },
    {
      "epoch": 0.03355611007381297,
      "grad_norm": 16.126285552978516,
      "learning_rate": 7.976969379743522e-06,
      "loss": 0.705,
      "step": 1282
    },
    {
      "epoch": 0.03358228488666306,
      "grad_norm": 14.313868522644043,
      "learning_rate": 7.97382884061764e-06,
      "loss": 0.3217,
      "step": 1283
    },
    {
      "epoch": 0.03360845969951315,
      "grad_norm": 24.181190490722656,
      "learning_rate": 7.970688301491756e-06,
      "loss": 1.0965,
      "step": 1284
    },
    {
      "epoch": 0.03363463451236324,
      "grad_norm": 16.900949478149414,
      "learning_rate": 7.967547762365873e-06,
      "loss": 0.8334,
      "step": 1285
    },
    {
      "epoch": 0.03366080932521332,
      "grad_norm": 11.52686882019043,
      "learning_rate": 7.964407223239989e-06,
      "loss": 0.5667,
      "step": 1286
    },
    {
      "epoch": 0.03368698413806341,
      "grad_norm": 18.446598052978516,
      "learning_rate": 7.961266684114107e-06,
      "loss": 0.5453,
      "step": 1287
    },
    {
      "epoch": 0.0337131589509135,
      "grad_norm": 16.6224308013916,
      "learning_rate": 7.958126144988224e-06,
      "loss": 0.4166,
      "step": 1288
    },
    {
      "epoch": 0.03373933376376359,
      "grad_norm": 19.50838851928711,
      "learning_rate": 7.95498560586234e-06,
      "loss": 0.2389,
      "step": 1289
    },
    {
      "epoch": 0.03376550857661368,
      "grad_norm": 21.648534774780273,
      "learning_rate": 7.951845066736457e-06,
      "loss": 0.6897,
      "step": 1290
    },
    {
      "epoch": 0.03379168338946376,
      "grad_norm": 30.21115493774414,
      "learning_rate": 7.948704527610573e-06,
      "loss": 0.6767,
      "step": 1291
    },
    {
      "epoch": 0.03381785820231385,
      "grad_norm": 21.699193954467773,
      "learning_rate": 7.945563988484691e-06,
      "loss": 0.8396,
      "step": 1292
    },
    {
      "epoch": 0.03384403301516394,
      "grad_norm": 13.824298858642578,
      "learning_rate": 7.942423449358807e-06,
      "loss": 0.7253,
      "step": 1293
    },
    {
      "epoch": 0.03387020782801403,
      "grad_norm": 8.179559707641602,
      "learning_rate": 7.939282910232923e-06,
      "loss": 0.2993,
      "step": 1294
    },
    {
      "epoch": 0.033896382640864114,
      "grad_norm": 20.188051223754883,
      "learning_rate": 7.93614237110704e-06,
      "loss": 0.5306,
      "step": 1295
    },
    {
      "epoch": 0.033922557453714204,
      "grad_norm": 13.978476524353027,
      "learning_rate": 7.933001831981156e-06,
      "loss": 0.6395,
      "step": 1296
    },
    {
      "epoch": 0.033948732266564294,
      "grad_norm": 17.256013870239258,
      "learning_rate": 7.929861292855274e-06,
      "loss": 0.5027,
      "step": 1297
    },
    {
      "epoch": 0.03397490707941438,
      "grad_norm": 20.41617202758789,
      "learning_rate": 7.92672075372939e-06,
      "loss": 0.6221,
      "step": 1298
    },
    {
      "epoch": 0.03400108189226447,
      "grad_norm": 12.171501159667969,
      "learning_rate": 7.923580214603507e-06,
      "loss": 0.4859,
      "step": 1299
    },
    {
      "epoch": 0.034027256705114556,
      "grad_norm": 17.15804672241211,
      "learning_rate": 7.920439675477625e-06,
      "loss": 0.6402,
      "step": 1300
    },
    {
      "epoch": 0.034053431517964645,
      "grad_norm": 18.020566940307617,
      "learning_rate": 7.91729913635174e-06,
      "loss": 0.5861,
      "step": 1301
    },
    {
      "epoch": 0.034079606330814735,
      "grad_norm": 9.038763046264648,
      "learning_rate": 7.914158597225858e-06,
      "loss": 0.3585,
      "step": 1302
    },
    {
      "epoch": 0.034105781143664825,
      "grad_norm": 21.747066497802734,
      "learning_rate": 7.911018058099974e-06,
      "loss": 0.6861,
      "step": 1303
    },
    {
      "epoch": 0.03413195595651491,
      "grad_norm": 18.91901206970215,
      "learning_rate": 7.907877518974091e-06,
      "loss": 0.3947,
      "step": 1304
    },
    {
      "epoch": 0.034158130769365,
      "grad_norm": 32.5510368347168,
      "learning_rate": 7.904736979848207e-06,
      "loss": 1.3898,
      "step": 1305
    },
    {
      "epoch": 0.03418430558221509,
      "grad_norm": 22.391338348388672,
      "learning_rate": 7.901596440722325e-06,
      "loss": 0.5096,
      "step": 1306
    },
    {
      "epoch": 0.034210480395065176,
      "grad_norm": 23.6744384765625,
      "learning_rate": 7.89845590159644e-06,
      "loss": 0.4855,
      "step": 1307
    },
    {
      "epoch": 0.034236655207915266,
      "grad_norm": 15.782029151916504,
      "learning_rate": 7.895315362470558e-06,
      "loss": 0.5178,
      "step": 1308
    },
    {
      "epoch": 0.03426283002076535,
      "grad_norm": 18.80600357055664,
      "learning_rate": 7.892174823344674e-06,
      "loss": 0.8294,
      "step": 1309
    },
    {
      "epoch": 0.03428900483361544,
      "grad_norm": 27.747169494628906,
      "learning_rate": 7.88903428421879e-06,
      "loss": 0.5137,
      "step": 1310
    },
    {
      "epoch": 0.03431517964646553,
      "grad_norm": 16.29921531677246,
      "learning_rate": 7.885893745092907e-06,
      "loss": 0.5639,
      "step": 1311
    },
    {
      "epoch": 0.03434135445931562,
      "grad_norm": 22.902124404907227,
      "learning_rate": 7.882753205967025e-06,
      "loss": 0.7483,
      "step": 1312
    },
    {
      "epoch": 0.03436752927216571,
      "grad_norm": 17.05113410949707,
      "learning_rate": 7.879612666841141e-06,
      "loss": 0.4996,
      "step": 1313
    },
    {
      "epoch": 0.03439370408501579,
      "grad_norm": 15.185234069824219,
      "learning_rate": 7.876472127715258e-06,
      "loss": 0.6176,
      "step": 1314
    },
    {
      "epoch": 0.03441987889786588,
      "grad_norm": 23.40630340576172,
      "learning_rate": 7.873331588589374e-06,
      "loss": 0.6632,
      "step": 1315
    },
    {
      "epoch": 0.03444605371071597,
      "grad_norm": 22.47783660888672,
      "learning_rate": 7.870191049463492e-06,
      "loss": 0.8285,
      "step": 1316
    },
    {
      "epoch": 0.03447222852356606,
      "grad_norm": 19.093719482421875,
      "learning_rate": 7.867050510337608e-06,
      "loss": 0.3702,
      "step": 1317
    },
    {
      "epoch": 0.03449840333641614,
      "grad_norm": 20.724027633666992,
      "learning_rate": 7.863909971211725e-06,
      "loss": 0.8617,
      "step": 1318
    },
    {
      "epoch": 0.03452457814926623,
      "grad_norm": 19.681331634521484,
      "learning_rate": 7.860769432085841e-06,
      "loss": 0.503,
      "step": 1319
    },
    {
      "epoch": 0.03455075296211632,
      "grad_norm": 20.066869735717773,
      "learning_rate": 7.857628892959959e-06,
      "loss": 0.6903,
      "step": 1320
    },
    {
      "epoch": 0.03457692777496641,
      "grad_norm": 16.57672691345215,
      "learning_rate": 7.854488353834076e-06,
      "loss": 0.5975,
      "step": 1321
    },
    {
      "epoch": 0.0346031025878165,
      "grad_norm": 15.874419212341309,
      "learning_rate": 7.851347814708192e-06,
      "loss": 0.254,
      "step": 1322
    },
    {
      "epoch": 0.03462927740066658,
      "grad_norm": 19.26849365234375,
      "learning_rate": 7.84820727558231e-06,
      "loss": 0.6495,
      "step": 1323
    },
    {
      "epoch": 0.03465545221351667,
      "grad_norm": 11.576543807983398,
      "learning_rate": 7.845066736456424e-06,
      "loss": 0.2715,
      "step": 1324
    },
    {
      "epoch": 0.03468162702636676,
      "grad_norm": 12.823216438293457,
      "learning_rate": 7.841926197330541e-06,
      "loss": 0.2618,
      "step": 1325
    },
    {
      "epoch": 0.03470780183921685,
      "grad_norm": 18.61086654663086,
      "learning_rate": 7.838785658204659e-06,
      "loss": 0.4962,
      "step": 1326
    },
    {
      "epoch": 0.034733976652066935,
      "grad_norm": 19.859888076782227,
      "learning_rate": 7.835645119078775e-06,
      "loss": 0.471,
      "step": 1327
    },
    {
      "epoch": 0.034760151464917025,
      "grad_norm": 17.080537796020508,
      "learning_rate": 7.832504579952892e-06,
      "loss": 0.3902,
      "step": 1328
    },
    {
      "epoch": 0.034786326277767114,
      "grad_norm": 17.70814323425293,
      "learning_rate": 7.829364040827008e-06,
      "loss": 0.4042,
      "step": 1329
    },
    {
      "epoch": 0.034812501090617204,
      "grad_norm": 19.63416290283203,
      "learning_rate": 7.826223501701126e-06,
      "loss": 0.6017,
      "step": 1330
    },
    {
      "epoch": 0.034838675903467294,
      "grad_norm": 15.477885246276855,
      "learning_rate": 7.823082962575242e-06,
      "loss": 0.3228,
      "step": 1331
    },
    {
      "epoch": 0.034864850716317376,
      "grad_norm": 14.766392707824707,
      "learning_rate": 7.81994242344936e-06,
      "loss": 0.5243,
      "step": 1332
    },
    {
      "epoch": 0.034891025529167466,
      "grad_norm": 19.511646270751953,
      "learning_rate": 7.816801884323477e-06,
      "loss": 0.7817,
      "step": 1333
    },
    {
      "epoch": 0.034917200342017556,
      "grad_norm": 23.270505905151367,
      "learning_rate": 7.813661345197593e-06,
      "loss": 0.8959,
      "step": 1334
    },
    {
      "epoch": 0.034943375154867645,
      "grad_norm": 23.08707618713379,
      "learning_rate": 7.81052080607171e-06,
      "loss": 0.5739,
      "step": 1335
    },
    {
      "epoch": 0.03496954996771773,
      "grad_norm": 31.27659034729004,
      "learning_rate": 7.807380266945826e-06,
      "loss": 0.8297,
      "step": 1336
    },
    {
      "epoch": 0.03499572478056782,
      "grad_norm": 12.797849655151367,
      "learning_rate": 7.804239727819944e-06,
      "loss": 0.2643,
      "step": 1337
    },
    {
      "epoch": 0.03502189959341791,
      "grad_norm": 11.073772430419922,
      "learning_rate": 7.80109918869406e-06,
      "loss": 0.199,
      "step": 1338
    },
    {
      "epoch": 0.035048074406268,
      "grad_norm": 18.169187545776367,
      "learning_rate": 7.797958649568177e-06,
      "loss": 0.46,
      "step": 1339
    },
    {
      "epoch": 0.03507424921911809,
      "grad_norm": 19.734333038330078,
      "learning_rate": 7.794818110442293e-06,
      "loss": 1.0124,
      "step": 1340
    },
    {
      "epoch": 0.03510042403196817,
      "grad_norm": 17.85934066772461,
      "learning_rate": 7.791677571316409e-06,
      "loss": 0.3997,
      "step": 1341
    },
    {
      "epoch": 0.03512659884481826,
      "grad_norm": 22.923633575439453,
      "learning_rate": 7.788537032190526e-06,
      "loss": 0.5123,
      "step": 1342
    },
    {
      "epoch": 0.03515277365766835,
      "grad_norm": 25.790788650512695,
      "learning_rate": 7.785396493064642e-06,
      "loss": 0.7546,
      "step": 1343
    },
    {
      "epoch": 0.03517894847051844,
      "grad_norm": 16.092100143432617,
      "learning_rate": 7.78225595393876e-06,
      "loss": 0.4006,
      "step": 1344
    },
    {
      "epoch": 0.03520512328336852,
      "grad_norm": 23.701419830322266,
      "learning_rate": 7.779115414812875e-06,
      "loss": 0.5325,
      "step": 1345
    },
    {
      "epoch": 0.03523129809621861,
      "grad_norm": 13.217493057250977,
      "learning_rate": 7.775974875686993e-06,
      "loss": 0.5302,
      "step": 1346
    },
    {
      "epoch": 0.0352574729090687,
      "grad_norm": 14.700857162475586,
      "learning_rate": 7.77283433656111e-06,
      "loss": 0.2953,
      "step": 1347
    },
    {
      "epoch": 0.03528364772191879,
      "grad_norm": 17.1502685546875,
      "learning_rate": 7.769693797435226e-06,
      "loss": 0.3497,
      "step": 1348
    },
    {
      "epoch": 0.03530982253476888,
      "grad_norm": 30.61972999572754,
      "learning_rate": 7.766553258309344e-06,
      "loss": 1.0732,
      "step": 1349
    },
    {
      "epoch": 0.03533599734761896,
      "grad_norm": 18.039615631103516,
      "learning_rate": 7.76341271918346e-06,
      "loss": 0.2422,
      "step": 1350
    },
    {
      "epoch": 0.03536217216046905,
      "grad_norm": 25.948238372802734,
      "learning_rate": 7.760272180057577e-06,
      "loss": 0.6422,
      "step": 1351
    },
    {
      "epoch": 0.03538834697331914,
      "grad_norm": 16.510948181152344,
      "learning_rate": 7.757131640931693e-06,
      "loss": 0.3472,
      "step": 1352
    },
    {
      "epoch": 0.03541452178616923,
      "grad_norm": 18.442420959472656,
      "learning_rate": 7.75399110180581e-06,
      "loss": 0.4011,
      "step": 1353
    },
    {
      "epoch": 0.035440696599019314,
      "grad_norm": 22.480772018432617,
      "learning_rate": 7.750850562679927e-06,
      "loss": 0.897,
      "step": 1354
    },
    {
      "epoch": 0.035466871411869404,
      "grad_norm": 49.46504211425781,
      "learning_rate": 7.747710023554043e-06,
      "loss": 0.7781,
      "step": 1355
    },
    {
      "epoch": 0.03549304622471949,
      "grad_norm": 19.405851364135742,
      "learning_rate": 7.74456948442816e-06,
      "loss": 0.6073,
      "step": 1356
    },
    {
      "epoch": 0.03551922103756958,
      "grad_norm": 16.261978149414062,
      "learning_rate": 7.741428945302276e-06,
      "loss": 0.4172,
      "step": 1357
    },
    {
      "epoch": 0.03554539585041967,
      "grad_norm": 15.889765739440918,
      "learning_rate": 7.738288406176394e-06,
      "loss": 0.6333,
      "step": 1358
    },
    {
      "epoch": 0.035571570663269755,
      "grad_norm": 18.155166625976562,
      "learning_rate": 7.735147867050511e-06,
      "loss": 0.4572,
      "step": 1359
    },
    {
      "epoch": 0.035597745476119845,
      "grad_norm": 24.878379821777344,
      "learning_rate": 7.732007327924627e-06,
      "loss": 0.4352,
      "step": 1360
    },
    {
      "epoch": 0.035623920288969935,
      "grad_norm": 16.14891815185547,
      "learning_rate": 7.728866788798744e-06,
      "loss": 0.3003,
      "step": 1361
    },
    {
      "epoch": 0.035650095101820024,
      "grad_norm": 21.342958450317383,
      "learning_rate": 7.72572624967286e-06,
      "loss": 0.584,
      "step": 1362
    },
    {
      "epoch": 0.03567626991467011,
      "grad_norm": 27.541357040405273,
      "learning_rate": 7.722585710546978e-06,
      "loss": 0.5162,
      "step": 1363
    },
    {
      "epoch": 0.0357024447275202,
      "grad_norm": 18.027944564819336,
      "learning_rate": 7.719445171421094e-06,
      "loss": 0.5747,
      "step": 1364
    },
    {
      "epoch": 0.035728619540370286,
      "grad_norm": 13.018811225891113,
      "learning_rate": 7.716304632295211e-06,
      "loss": 0.5648,
      "step": 1365
    },
    {
      "epoch": 0.035754794353220376,
      "grad_norm": 38.16303634643555,
      "learning_rate": 7.713164093169327e-06,
      "loss": 0.6872,
      "step": 1366
    },
    {
      "epoch": 0.035780969166070466,
      "grad_norm": 18.36562728881836,
      "learning_rate": 7.710023554043445e-06,
      "loss": 0.4778,
      "step": 1367
    },
    {
      "epoch": 0.03580714397892055,
      "grad_norm": 15.002086639404297,
      "learning_rate": 7.706883014917562e-06,
      "loss": 0.4369,
      "step": 1368
    },
    {
      "epoch": 0.03583331879177064,
      "grad_norm": 17.472349166870117,
      "learning_rate": 7.703742475791678e-06,
      "loss": 0.4633,
      "step": 1369
    },
    {
      "epoch": 0.03585949360462073,
      "grad_norm": 27.5150089263916,
      "learning_rate": 7.700601936665796e-06,
      "loss": 0.8142,
      "step": 1370
    },
    {
      "epoch": 0.03588566841747082,
      "grad_norm": 18.18723487854004,
      "learning_rate": 7.69746139753991e-06,
      "loss": 0.2918,
      "step": 1371
    },
    {
      "epoch": 0.0359118432303209,
      "grad_norm": 28.390600204467773,
      "learning_rate": 7.694320858414027e-06,
      "loss": 0.899,
      "step": 1372
    },
    {
      "epoch": 0.03593801804317099,
      "grad_norm": 21.234086990356445,
      "learning_rate": 7.691180319288145e-06,
      "loss": 0.6195,
      "step": 1373
    },
    {
      "epoch": 0.03596419285602108,
      "grad_norm": 25.316301345825195,
      "learning_rate": 7.68803978016226e-06,
      "loss": 0.4485,
      "step": 1374
    },
    {
      "epoch": 0.03599036766887117,
      "grad_norm": 16.25037384033203,
      "learning_rate": 7.684899241036378e-06,
      "loss": 0.7611,
      "step": 1375
    },
    {
      "epoch": 0.03601654248172126,
      "grad_norm": 25.999855041503906,
      "learning_rate": 7.681758701910494e-06,
      "loss": 1.1376,
      "step": 1376
    },
    {
      "epoch": 0.03604271729457134,
      "grad_norm": 21.143308639526367,
      "learning_rate": 7.678618162784612e-06,
      "loss": 0.7489,
      "step": 1377
    },
    {
      "epoch": 0.03606889210742143,
      "grad_norm": 16.70027732849121,
      "learning_rate": 7.675477623658728e-06,
      "loss": 0.7943,
      "step": 1378
    },
    {
      "epoch": 0.03609506692027152,
      "grad_norm": 17.25104522705078,
      "learning_rate": 7.672337084532845e-06,
      "loss": 0.4639,
      "step": 1379
    },
    {
      "epoch": 0.03612124173312161,
      "grad_norm": 16.244657516479492,
      "learning_rate": 7.669196545406963e-06,
      "loss": 0.5942,
      "step": 1380
    },
    {
      "epoch": 0.03614741654597169,
      "grad_norm": 22.37664031982422,
      "learning_rate": 7.666056006281079e-06,
      "loss": 0.8282,
      "step": 1381
    },
    {
      "epoch": 0.03617359135882178,
      "grad_norm": 21.173603057861328,
      "learning_rate": 7.662915467155196e-06,
      "loss": 0.6476,
      "step": 1382
    },
    {
      "epoch": 0.03619976617167187,
      "grad_norm": 16.887577056884766,
      "learning_rate": 7.659774928029312e-06,
      "loss": 0.5999,
      "step": 1383
    },
    {
      "epoch": 0.03622594098452196,
      "grad_norm": 27.966081619262695,
      "learning_rate": 7.65663438890343e-06,
      "loss": 0.5188,
      "step": 1384
    },
    {
      "epoch": 0.03625211579737205,
      "grad_norm": 17.79327392578125,
      "learning_rate": 7.653493849777545e-06,
      "loss": 0.4321,
      "step": 1385
    },
    {
      "epoch": 0.036278290610222134,
      "grad_norm": 16.45579719543457,
      "learning_rate": 7.650353310651661e-06,
      "loss": 0.6958,
      "step": 1386
    },
    {
      "epoch": 0.036304465423072224,
      "grad_norm": 19.377819061279297,
      "learning_rate": 7.647212771525779e-06,
      "loss": 1.051,
      "step": 1387
    },
    {
      "epoch": 0.036330640235922314,
      "grad_norm": 27.450275421142578,
      "learning_rate": 7.644072232399895e-06,
      "loss": 0.444,
      "step": 1388
    },
    {
      "epoch": 0.0363568150487724,
      "grad_norm": 9.35712718963623,
      "learning_rate": 7.640931693274012e-06,
      "loss": 0.1279,
      "step": 1389
    },
    {
      "epoch": 0.036382989861622486,
      "grad_norm": 20.082454681396484,
      "learning_rate": 7.637791154148128e-06,
      "loss": 0.6731,
      "step": 1390
    },
    {
      "epoch": 0.036409164674472576,
      "grad_norm": 10.431342124938965,
      "learning_rate": 7.634650615022246e-06,
      "loss": 0.2511,
      "step": 1391
    },
    {
      "epoch": 0.036435339487322665,
      "grad_norm": 18.055721282958984,
      "learning_rate": 7.631510075896362e-06,
      "loss": 0.4269,
      "step": 1392
    },
    {
      "epoch": 0.036461514300172755,
      "grad_norm": 14.912940979003906,
      "learning_rate": 7.628369536770479e-06,
      "loss": 0.4764,
      "step": 1393
    },
    {
      "epoch": 0.036487689113022845,
      "grad_norm": 14.448505401611328,
      "learning_rate": 7.625228997644596e-06,
      "loss": 0.7704,
      "step": 1394
    },
    {
      "epoch": 0.03651386392587293,
      "grad_norm": 13.894495010375977,
      "learning_rate": 7.6220884585187125e-06,
      "loss": 0.9547,
      "step": 1395
    },
    {
      "epoch": 0.03654003873872302,
      "grad_norm": 19.422962188720703,
      "learning_rate": 7.618947919392829e-06,
      "loss": 0.9213,
      "step": 1396
    },
    {
      "epoch": 0.03656621355157311,
      "grad_norm": 17.59050941467285,
      "learning_rate": 7.615807380266947e-06,
      "loss": 0.7612,
      "step": 1397
    },
    {
      "epoch": 0.036592388364423196,
      "grad_norm": 19.853649139404297,
      "learning_rate": 7.6126668411410635e-06,
      "loss": 0.5971,
      "step": 1398
    },
    {
      "epoch": 0.03661856317727328,
      "grad_norm": 20.637800216674805,
      "learning_rate": 7.60952630201518e-06,
      "loss": 0.5155,
      "step": 1399
    },
    {
      "epoch": 0.03664473799012337,
      "grad_norm": 15.478019714355469,
      "learning_rate": 7.606385762889297e-06,
      "loss": 0.5517,
      "step": 1400
    },
    {
      "epoch": 0.03667091280297346,
      "grad_norm": 14.141962051391602,
      "learning_rate": 7.603245223763414e-06,
      "loss": 0.4856,
      "step": 1401
    },
    {
      "epoch": 0.03669708761582355,
      "grad_norm": 23.931318283081055,
      "learning_rate": 7.6001046846375294e-06,
      "loss": 0.5104,
      "step": 1402
    },
    {
      "epoch": 0.03672326242867364,
      "grad_norm": 21.694766998291016,
      "learning_rate": 7.596964145511646e-06,
      "loss": 0.6666,
      "step": 1403
    },
    {
      "epoch": 0.03674943724152372,
      "grad_norm": 14.489320755004883,
      "learning_rate": 7.593823606385763e-06,
      "loss": 0.2401,
      "step": 1404
    },
    {
      "epoch": 0.03677561205437381,
      "grad_norm": 27.71939468383789,
      "learning_rate": 7.5906830672598796e-06,
      "loss": 0.5153,
      "step": 1405
    },
    {
      "epoch": 0.0368017868672239,
      "grad_norm": 17.918306350708008,
      "learning_rate": 7.587542528133996e-06,
      "loss": 0.5716,
      "step": 1406
    },
    {
      "epoch": 0.03682796168007399,
      "grad_norm": 23.813919067382812,
      "learning_rate": 7.584401989008113e-06,
      "loss": 0.453,
      "step": 1407
    },
    {
      "epoch": 0.03685413649292407,
      "grad_norm": 22.07146644592285,
      "learning_rate": 7.58126144988223e-06,
      "loss": 0.8975,
      "step": 1408
    },
    {
      "epoch": 0.03688031130577416,
      "grad_norm": 11.604959487915039,
      "learning_rate": 7.578120910756346e-06,
      "loss": 0.4966,
      "step": 1409
    },
    {
      "epoch": 0.03690648611862425,
      "grad_norm": 24.725427627563477,
      "learning_rate": 7.574980371630464e-06,
      "loss": 0.5359,
      "step": 1410
    },
    {
      "epoch": 0.03693266093147434,
      "grad_norm": 8.68175983428955,
      "learning_rate": 7.571839832504581e-06,
      "loss": 0.1128,
      "step": 1411
    },
    {
      "epoch": 0.03695883574432443,
      "grad_norm": 19.24297523498535,
      "learning_rate": 7.568699293378697e-06,
      "loss": 0.4394,
      "step": 1412
    },
    {
      "epoch": 0.036985010557174514,
      "grad_norm": 18.26197052001953,
      "learning_rate": 7.565558754252814e-06,
      "loss": 0.9472,
      "step": 1413
    },
    {
      "epoch": 0.0370111853700246,
      "grad_norm": 13.033663749694824,
      "learning_rate": 7.562418215126931e-06,
      "loss": 0.3853,
      "step": 1414
    },
    {
      "epoch": 0.03703736018287469,
      "grad_norm": 14.647751808166504,
      "learning_rate": 7.5592776760010475e-06,
      "loss": 0.5707,
      "step": 1415
    },
    {
      "epoch": 0.03706353499572478,
      "grad_norm": 20.48818016052246,
      "learning_rate": 7.556137136875164e-06,
      "loss": 0.8668,
      "step": 1416
    },
    {
      "epoch": 0.037089709808574865,
      "grad_norm": 21.985607147216797,
      "learning_rate": 7.55299659774928e-06,
      "loss": 0.7805,
      "step": 1417
    },
    {
      "epoch": 0.037115884621424955,
      "grad_norm": 12.50190258026123,
      "learning_rate": 7.549856058623397e-06,
      "loss": 0.3437,
      "step": 1418
    },
    {
      "epoch": 0.037142059434275045,
      "grad_norm": 12.195987701416016,
      "learning_rate": 7.5467155194975134e-06,
      "loss": 0.2682,
      "step": 1419
    },
    {
      "epoch": 0.037168234247125134,
      "grad_norm": 25.574716567993164,
      "learning_rate": 7.54357498037163e-06,
      "loss": 0.7044,
      "step": 1420
    },
    {
      "epoch": 0.037194409059975224,
      "grad_norm": 16.818355560302734,
      "learning_rate": 7.540434441245747e-06,
      "loss": 0.3136,
      "step": 1421
    },
    {
      "epoch": 0.03722058387282531,
      "grad_norm": 31.257579803466797,
      "learning_rate": 7.5372939021198636e-06,
      "loss": 0.6915,
      "step": 1422
    },
    {
      "epoch": 0.037246758685675396,
      "grad_norm": 16.990232467651367,
      "learning_rate": 7.534153362993981e-06,
      "loss": 0.2904,
      "step": 1423
    },
    {
      "epoch": 0.037272933498525486,
      "grad_norm": 22.440343856811523,
      "learning_rate": 7.531012823868098e-06,
      "loss": 0.7268,
      "step": 1424
    },
    {
      "epoch": 0.037299108311375576,
      "grad_norm": 21.59572982788086,
      "learning_rate": 7.5278722847422145e-06,
      "loss": 0.515,
      "step": 1425
    },
    {
      "epoch": 0.037325283124225665,
      "grad_norm": 14.902255058288574,
      "learning_rate": 7.524731745616331e-06,
      "loss": 0.508,
      "step": 1426
    },
    {
      "epoch": 0.03735145793707575,
      "grad_norm": 13.750914573669434,
      "learning_rate": 7.521591206490448e-06,
      "loss": 0.2901,
      "step": 1427
    },
    {
      "epoch": 0.03737763274992584,
      "grad_norm": 17.595788955688477,
      "learning_rate": 7.518450667364565e-06,
      "loss": 0.8069,
      "step": 1428
    },
    {
      "epoch": 0.03740380756277593,
      "grad_norm": 21.429454803466797,
      "learning_rate": 7.515310128238681e-06,
      "loss": 0.7075,
      "step": 1429
    },
    {
      "epoch": 0.03742998237562602,
      "grad_norm": 15.598773956298828,
      "learning_rate": 7.512169589112798e-06,
      "loss": 0.3924,
      "step": 1430
    },
    {
      "epoch": 0.0374561571884761,
      "grad_norm": 18.841466903686523,
      "learning_rate": 7.509029049986916e-06,
      "loss": 0.6932,
      "step": 1431
    },
    {
      "epoch": 0.03748233200132619,
      "grad_norm": 16.787240982055664,
      "learning_rate": 7.505888510861031e-06,
      "loss": 0.7044,
      "step": 1432
    },
    {
      "epoch": 0.03750850681417628,
      "grad_norm": 18.73250389099121,
      "learning_rate": 7.502747971735147e-06,
      "loss": 0.5133,
      "step": 1433
    },
    {
      "epoch": 0.03753468162702637,
      "grad_norm": 29.851219177246094,
      "learning_rate": 7.499607432609264e-06,
      "loss": 0.8465,
      "step": 1434
    },
    {
      "epoch": 0.03756085643987646,
      "grad_norm": 19.36211585998535,
      "learning_rate": 7.496466893483381e-06,
      "loss": 0.3958,
      "step": 1435
    },
    {
      "epoch": 0.03758703125272654,
      "grad_norm": 21.833572387695312,
      "learning_rate": 7.493326354357498e-06,
      "loss": 0.8468,
      "step": 1436
    },
    {
      "epoch": 0.03761320606557663,
      "grad_norm": 16.266891479492188,
      "learning_rate": 7.490185815231615e-06,
      "loss": 0.3284,
      "step": 1437
    },
    {
      "epoch": 0.03763938087842672,
      "grad_norm": 29.233888626098633,
      "learning_rate": 7.487045276105732e-06,
      "loss": 0.8143,
      "step": 1438
    },
    {
      "epoch": 0.03766555569127681,
      "grad_norm": 15.55520248413086,
      "learning_rate": 7.483904736979848e-06,
      "loss": 0.2281,
      "step": 1439
    },
    {
      "epoch": 0.03769173050412689,
      "grad_norm": 16.650053024291992,
      "learning_rate": 7.480764197853965e-06,
      "loss": 0.6471,
      "step": 1440
    },
    {
      "epoch": 0.03771790531697698,
      "grad_norm": 18.428329467773438,
      "learning_rate": 7.477623658728082e-06,
      "loss": 0.6192,
      "step": 1441
    },
    {
      "epoch": 0.03774408012982707,
      "grad_norm": 21.165912628173828,
      "learning_rate": 7.4744831196021985e-06,
      "loss": 0.6249,
      "step": 1442
    },
    {
      "epoch": 0.03777025494267716,
      "grad_norm": 26.311777114868164,
      "learning_rate": 7.471342580476315e-06,
      "loss": 0.5509,
      "step": 1443
    },
    {
      "epoch": 0.03779642975552725,
      "grad_norm": 11.426887512207031,
      "learning_rate": 7.468202041350433e-06,
      "loss": 0.29,
      "step": 1444
    },
    {
      "epoch": 0.037822604568377334,
      "grad_norm": 11.62671947479248,
      "learning_rate": 7.4650615022245495e-06,
      "loss": 0.3092,
      "step": 1445
    },
    {
      "epoch": 0.037848779381227424,
      "grad_norm": 18.783369064331055,
      "learning_rate": 7.461920963098666e-06,
      "loss": 0.8567,
      "step": 1446
    },
    {
      "epoch": 0.03787495419407751,
      "grad_norm": 23.087942123413086,
      "learning_rate": 7.458780423972783e-06,
      "loss": 0.6405,
      "step": 1447
    },
    {
      "epoch": 0.0379011290069276,
      "grad_norm": 26.76946258544922,
      "learning_rate": 7.455639884846898e-06,
      "loss": 0.6339,
      "step": 1448
    },
    {
      "epoch": 0.037927303819777686,
      "grad_norm": 22.75250244140625,
      "learning_rate": 7.4524993457210155e-06,
      "loss": 0.6436,
      "step": 1449
    },
    {
      "epoch": 0.037953478632627775,
      "grad_norm": 18.113096237182617,
      "learning_rate": 7.449358806595132e-06,
      "loss": 0.7094,
      "step": 1450
    },
    {
      "epoch": 0.037979653445477865,
      "grad_norm": 21.78819465637207,
      "learning_rate": 7.446218267469249e-06,
      "loss": 0.7182,
      "step": 1451
    },
    {
      "epoch": 0.038005828258327955,
      "grad_norm": 17.43453025817871,
      "learning_rate": 7.443077728343366e-06,
      "loss": 0.3352,
      "step": 1452
    },
    {
      "epoch": 0.038032003071178044,
      "grad_norm": 13.596834182739258,
      "learning_rate": 7.439937189217482e-06,
      "loss": 0.2728,
      "step": 1453
    },
    {
      "epoch": 0.03805817788402813,
      "grad_norm": 37.49578094482422,
      "learning_rate": 7.436796650091599e-06,
      "loss": 0.3714,
      "step": 1454
    },
    {
      "epoch": 0.03808435269687822,
      "grad_norm": 20.94324493408203,
      "learning_rate": 7.433656110965716e-06,
      "loss": 0.5018,
      "step": 1455
    },
    {
      "epoch": 0.038110527509728306,
      "grad_norm": 16.9849796295166,
      "learning_rate": 7.430515571839832e-06,
      "loss": 0.3451,
      "step": 1456
    },
    {
      "epoch": 0.038136702322578396,
      "grad_norm": 18.325794219970703,
      "learning_rate": 7.42737503271395e-06,
      "loss": 0.4887,
      "step": 1457
    },
    {
      "epoch": 0.03816287713542848,
      "grad_norm": 19.92972755432129,
      "learning_rate": 7.424234493588067e-06,
      "loss": 0.5192,
      "step": 1458
    },
    {
      "epoch": 0.03818905194827857,
      "grad_norm": 21.545682907104492,
      "learning_rate": 7.421093954462183e-06,
      "loss": 0.606,
      "step": 1459
    },
    {
      "epoch": 0.03821522676112866,
      "grad_norm": 17.892908096313477,
      "learning_rate": 7.4179534153363e-06,
      "loss": 0.6296,
      "step": 1460
    },
    {
      "epoch": 0.03824140157397875,
      "grad_norm": 15.057950973510742,
      "learning_rate": 7.414812876210417e-06,
      "loss": 0.2417,
      "step": 1461
    },
    {
      "epoch": 0.03826757638682884,
      "grad_norm": 30.626773834228516,
      "learning_rate": 7.4116723370845335e-06,
      "loss": 0.6107,
      "step": 1462
    },
    {
      "epoch": 0.03829375119967892,
      "grad_norm": 18.15503692626953,
      "learning_rate": 7.408531797958649e-06,
      "loss": 0.4327,
      "step": 1463
    },
    {
      "epoch": 0.03831992601252901,
      "grad_norm": 26.8110408782959,
      "learning_rate": 7.405391258832766e-06,
      "loss": 0.3955,
      "step": 1464
    },
    {
      "epoch": 0.0383461008253791,
      "grad_norm": 26.769874572753906,
      "learning_rate": 7.402250719706883e-06,
      "loss": 0.6507,
      "step": 1465
    },
    {
      "epoch": 0.03837227563822919,
      "grad_norm": 23.900054931640625,
      "learning_rate": 7.3991101805809995e-06,
      "loss": 0.4608,
      "step": 1466
    },
    {
      "epoch": 0.03839845045107927,
      "grad_norm": 15.968517303466797,
      "learning_rate": 7.395969641455116e-06,
      "loss": 0.5061,
      "step": 1467
    },
    {
      "epoch": 0.03842462526392936,
      "grad_norm": 24.709274291992188,
      "learning_rate": 7.392829102329233e-06,
      "loss": 0.7406,
      "step": 1468
    },
    {
      "epoch": 0.03845080007677945,
      "grad_norm": 21.211490631103516,
      "learning_rate": 7.38968856320335e-06,
      "loss": 0.4072,
      "step": 1469
    },
    {
      "epoch": 0.03847697488962954,
      "grad_norm": 30.598562240600586,
      "learning_rate": 7.386548024077467e-06,
      "loss": 0.4367,
      "step": 1470
    },
    {
      "epoch": 0.03850314970247963,
      "grad_norm": 20.80502700805664,
      "learning_rate": 7.383407484951584e-06,
      "loss": 0.5234,
      "step": 1471
    },
    {
      "epoch": 0.03852932451532971,
      "grad_norm": 19.792203903198242,
      "learning_rate": 7.3802669458257006e-06,
      "loss": 0.6647,
      "step": 1472
    },
    {
      "epoch": 0.0385554993281798,
      "grad_norm": 9.339995384216309,
      "learning_rate": 7.377126406699817e-06,
      "loss": 0.1764,
      "step": 1473
    },
    {
      "epoch": 0.03858167414102989,
      "grad_norm": 25.601465225219727,
      "learning_rate": 7.373985867573934e-06,
      "loss": 0.488,
      "step": 1474
    },
    {
      "epoch": 0.03860784895387998,
      "grad_norm": 21.86649513244629,
      "learning_rate": 7.370845328448051e-06,
      "loss": 0.7823,
      "step": 1475
    },
    {
      "epoch": 0.038634023766730065,
      "grad_norm": 22.11231803894043,
      "learning_rate": 7.367704789322167e-06,
      "loss": 0.4909,
      "step": 1476
    },
    {
      "epoch": 0.038660198579580154,
      "grad_norm": 18.584230422973633,
      "learning_rate": 7.364564250196284e-06,
      "loss": 0.8305,
      "step": 1477
    },
    {
      "epoch": 0.038686373392430244,
      "grad_norm": 15.03104019165039,
      "learning_rate": 7.361423711070402e-06,
      "loss": 0.4608,
      "step": 1478
    },
    {
      "epoch": 0.038712548205280334,
      "grad_norm": 16.96283721923828,
      "learning_rate": 7.358283171944517e-06,
      "loss": 0.3897,
      "step": 1479
    },
    {
      "epoch": 0.03873872301813042,
      "grad_norm": 17.228872299194336,
      "learning_rate": 7.355142632818633e-06,
      "loss": 0.3274,
      "step": 1480
    },
    {
      "epoch": 0.038764897830980506,
      "grad_norm": 28.528059005737305,
      "learning_rate": 7.35200209369275e-06,
      "loss": 0.3366,
      "step": 1481
    },
    {
      "epoch": 0.038791072643830596,
      "grad_norm": 26.39232063293457,
      "learning_rate": 7.348861554566867e-06,
      "loss": 0.5606,
      "step": 1482
    },
    {
      "epoch": 0.038817247456680685,
      "grad_norm": 15.871273040771484,
      "learning_rate": 7.345721015440984e-06,
      "loss": 0.4716,
      "step": 1483
    },
    {
      "epoch": 0.038843422269530775,
      "grad_norm": 22.237503051757812,
      "learning_rate": 7.342580476315101e-06,
      "loss": 0.686,
      "step": 1484
    },
    {
      "epoch": 0.03886959708238086,
      "grad_norm": 25.250869750976562,
      "learning_rate": 7.339439937189218e-06,
      "loss": 1.0876,
      "step": 1485
    },
    {
      "epoch": 0.03889577189523095,
      "grad_norm": 20.796119689941406,
      "learning_rate": 7.3362993980633345e-06,
      "loss": 0.6586,
      "step": 1486
    },
    {
      "epoch": 0.03892194670808104,
      "grad_norm": 15.70380687713623,
      "learning_rate": 7.333158858937451e-06,
      "loss": 0.4432,
      "step": 1487
    },
    {
      "epoch": 0.03894812152093113,
      "grad_norm": 22.912534713745117,
      "learning_rate": 7.330018319811568e-06,
      "loss": 0.919,
      "step": 1488
    },
    {
      "epoch": 0.038974296333781216,
      "grad_norm": 27.484203338623047,
      "learning_rate": 7.3268777806856846e-06,
      "loss": 0.7464,
      "step": 1489
    },
    {
      "epoch": 0.0390004711466313,
      "grad_norm": 20.035097122192383,
      "learning_rate": 7.323737241559801e-06,
      "loss": 0.9963,
      "step": 1490
    },
    {
      "epoch": 0.03902664595948139,
      "grad_norm": 21.095041275024414,
      "learning_rate": 7.320596702433919e-06,
      "loss": 0.4403,
      "step": 1491
    },
    {
      "epoch": 0.03905282077233148,
      "grad_norm": 27.941272735595703,
      "learning_rate": 7.3174561633080355e-06,
      "loss": 0.3236,
      "step": 1492
    },
    {
      "epoch": 0.03907899558518157,
      "grad_norm": 16.119701385498047,
      "learning_rate": 7.314315624182152e-06,
      "loss": 0.4987,
      "step": 1493
    },
    {
      "epoch": 0.03910517039803165,
      "grad_norm": 18.922319412231445,
      "learning_rate": 7.311175085056267e-06,
      "loss": 0.6455,
      "step": 1494
    },
    {
      "epoch": 0.03913134521088174,
      "grad_norm": 17.422975540161133,
      "learning_rate": 7.308034545930384e-06,
      "loss": 0.5608,
      "step": 1495
    },
    {
      "epoch": 0.03915752002373183,
      "grad_norm": 28.261371612548828,
      "learning_rate": 7.3048940068045015e-06,
      "loss": 0.8546,
      "step": 1496
    },
    {
      "epoch": 0.03918369483658192,
      "grad_norm": 15.347820281982422,
      "learning_rate": 7.301753467678618e-06,
      "loss": 0.4523,
      "step": 1497
    },
    {
      "epoch": 0.03920986964943201,
      "grad_norm": 14.259330749511719,
      "learning_rate": 7.298612928552735e-06,
      "loss": 0.2565,
      "step": 1498
    },
    {
      "epoch": 0.03923604446228209,
      "grad_norm": 13.254655838012695,
      "learning_rate": 7.295472389426852e-06,
      "loss": 0.275,
      "step": 1499
    },
    {
      "epoch": 0.03926221927513218,
      "grad_norm": 20.085859298706055,
      "learning_rate": 7.292331850300968e-06,
      "loss": 0.3967,
      "step": 1500
    },
    {
      "epoch": 0.03928839408798227,
      "grad_norm": 20.417455673217773,
      "learning_rate": 7.289191311175085e-06,
      "loss": 0.4526,
      "step": 1501
    },
    {
      "epoch": 0.03931456890083236,
      "grad_norm": 15.745759963989258,
      "learning_rate": 7.286050772049202e-06,
      "loss": 0.3586,
      "step": 1502
    },
    {
      "epoch": 0.039340743713682444,
      "grad_norm": 11.131237030029297,
      "learning_rate": 7.2829102329233185e-06,
      "loss": 0.1272,
      "step": 1503
    },
    {
      "epoch": 0.039366918526532534,
      "grad_norm": 21.445091247558594,
      "learning_rate": 7.279769693797436e-06,
      "loss": 0.9983,
      "step": 1504
    },
    {
      "epoch": 0.03939309333938262,
      "grad_norm": 12.682723045349121,
      "learning_rate": 7.276629154671553e-06,
      "loss": 0.264,
      "step": 1505
    },
    {
      "epoch": 0.03941926815223271,
      "grad_norm": 14.207160949707031,
      "learning_rate": 7.2734886155456694e-06,
      "loss": 0.3361,
      "step": 1506
    },
    {
      "epoch": 0.0394454429650828,
      "grad_norm": 19.93119239807129,
      "learning_rate": 7.270348076419786e-06,
      "loss": 0.5673,
      "step": 1507
    },
    {
      "epoch": 0.039471617777932885,
      "grad_norm": 24.537784576416016,
      "learning_rate": 7.267207537293903e-06,
      "loss": 0.4526,
      "step": 1508
    },
    {
      "epoch": 0.039497792590782975,
      "grad_norm": 13.335036277770996,
      "learning_rate": 7.2640669981680195e-06,
      "loss": 0.4321,
      "step": 1509
    },
    {
      "epoch": 0.039523967403633065,
      "grad_norm": 23.52393913269043,
      "learning_rate": 7.260926459042135e-06,
      "loss": 0.5907,
      "step": 1510
    },
    {
      "epoch": 0.039550142216483154,
      "grad_norm": 23.500255584716797,
      "learning_rate": 7.257785919916252e-06,
      "loss": 0.8508,
      "step": 1511
    },
    {
      "epoch": 0.03957631702933324,
      "grad_norm": 19.876989364624023,
      "learning_rate": 7.254645380790369e-06,
      "loss": 0.6755,
      "step": 1512
    },
    {
      "epoch": 0.03960249184218333,
      "grad_norm": 17.56342887878418,
      "learning_rate": 7.2515048416644855e-06,
      "loss": 0.3855,
      "step": 1513
    },
    {
      "epoch": 0.039628666655033416,
      "grad_norm": 29.988645553588867,
      "learning_rate": 7.248364302538602e-06,
      "loss": 1.0037,
      "step": 1514
    },
    {
      "epoch": 0.039654841467883506,
      "grad_norm": 19.924352645874023,
      "learning_rate": 7.245223763412719e-06,
      "loss": 0.704,
      "step": 1515
    },
    {
      "epoch": 0.039681016280733596,
      "grad_norm": 26.299592971801758,
      "learning_rate": 7.242083224286836e-06,
      "loss": 0.6431,
      "step": 1516
    },
    {
      "epoch": 0.03970719109358368,
      "grad_norm": 14.917527198791504,
      "learning_rate": 7.238942685160953e-06,
      "loss": 0.3656,
      "step": 1517
    },
    {
      "epoch": 0.03973336590643377,
      "grad_norm": 24.789949417114258,
      "learning_rate": 7.23580214603507e-06,
      "loss": 0.698,
      "step": 1518
    },
    {
      "epoch": 0.03975954071928386,
      "grad_norm": 14.641473770141602,
      "learning_rate": 7.232661606909187e-06,
      "loss": 0.3581,
      "step": 1519
    },
    {
      "epoch": 0.03978571553213395,
      "grad_norm": 11.580229759216309,
      "learning_rate": 7.229521067783303e-06,
      "loss": 0.4992,
      "step": 1520
    },
    {
      "epoch": 0.03981189034498403,
      "grad_norm": 19.23541259765625,
      "learning_rate": 7.22638052865742e-06,
      "loss": 0.5053,
      "step": 1521
    },
    {
      "epoch": 0.03983806515783412,
      "grad_norm": 12.09150505065918,
      "learning_rate": 7.223239989531537e-06,
      "loss": 0.4139,
      "step": 1522
    },
    {
      "epoch": 0.03986423997068421,
      "grad_norm": 16.358095169067383,
      "learning_rate": 7.2200994504056534e-06,
      "loss": 0.5373,
      "step": 1523
    },
    {
      "epoch": 0.0398904147835343,
      "grad_norm": 20.556411743164062,
      "learning_rate": 7.21695891127977e-06,
      "loss": 0.8092,
      "step": 1524
    },
    {
      "epoch": 0.03991658959638439,
      "grad_norm": 31.819297790527344,
      "learning_rate": 7.213818372153886e-06,
      "loss": 0.8352,
      "step": 1525
    },
    {
      "epoch": 0.03994276440923447,
      "grad_norm": 19.40224266052246,
      "learning_rate": 7.210677833028003e-06,
      "loss": 0.3701,
      "step": 1526
    },
    {
      "epoch": 0.03996893922208456,
      "grad_norm": 11.746187210083008,
      "learning_rate": 7.207537293902119e-06,
      "loss": 0.1679,
      "step": 1527
    },
    {
      "epoch": 0.03999511403493465,
      "grad_norm": 15.90229320526123,
      "learning_rate": 7.204396754776236e-06,
      "loss": 0.6186,
      "step": 1528
    },
    {
      "epoch": 0.04002128884778474,
      "grad_norm": 15.919110298156738,
      "learning_rate": 7.201256215650353e-06,
      "loss": 0.2381,
      "step": 1529
    },
    {
      "epoch": 0.04004746366063482,
      "grad_norm": 25.457807540893555,
      "learning_rate": 7.19811567652447e-06,
      "loss": 0.3232,
      "step": 1530
    },
    {
      "epoch": 0.04007363847348491,
      "grad_norm": 21.614940643310547,
      "learning_rate": 7.194975137398587e-06,
      "loss": 0.7257,
      "step": 1531
    },
    {
      "epoch": 0.040099813286335,
      "grad_norm": 19.384973526000977,
      "learning_rate": 7.191834598272704e-06,
      "loss": 0.6483,
      "step": 1532
    },
    {
      "epoch": 0.04012598809918509,
      "grad_norm": 35.22947692871094,
      "learning_rate": 7.1886940591468205e-06,
      "loss": 0.5972,
      "step": 1533
    },
    {
      "epoch": 0.04015216291203518,
      "grad_norm": 17.173900604248047,
      "learning_rate": 7.185553520020937e-06,
      "loss": 0.5226,
      "step": 1534
    },
    {
      "epoch": 0.040178337724885264,
      "grad_norm": 20.028762817382812,
      "learning_rate": 7.182412980895054e-06,
      "loss": 0.8096,
      "step": 1535
    },
    {
      "epoch": 0.040204512537735354,
      "grad_norm": 16.551254272460938,
      "learning_rate": 7.179272441769171e-06,
      "loss": 0.4704,
      "step": 1536
    },
    {
      "epoch": 0.040230687350585444,
      "grad_norm": 16.76205825805664,
      "learning_rate": 7.176131902643287e-06,
      "loss": 0.2969,
      "step": 1537
    },
    {
      "epoch": 0.04025686216343553,
      "grad_norm": 16.00514030456543,
      "learning_rate": 7.172991363517405e-06,
      "loss": 0.4854,
      "step": 1538
    },
    {
      "epoch": 0.04028303697628562,
      "grad_norm": 26.246049880981445,
      "learning_rate": 7.169850824391522e-06,
      "loss": 0.5445,
      "step": 1539
    },
    {
      "epoch": 0.040309211789135706,
      "grad_norm": 21.721416473388672,
      "learning_rate": 7.166710285265638e-06,
      "loss": 0.5438,
      "step": 1540
    },
    {
      "epoch": 0.040335386601985795,
      "grad_norm": 16.250337600708008,
      "learning_rate": 7.163569746139753e-06,
      "loss": 0.3795,
      "step": 1541
    },
    {
      "epoch": 0.040361561414835885,
      "grad_norm": 16.707557678222656,
      "learning_rate": 7.16042920701387e-06,
      "loss": 0.7633,
      "step": 1542
    },
    {
      "epoch": 0.040387736227685975,
      "grad_norm": 23.28533935546875,
      "learning_rate": 7.1572886678879876e-06,
      "loss": 0.4408,
      "step": 1543
    },
    {
      "epoch": 0.04041391104053606,
      "grad_norm": 16.354564666748047,
      "learning_rate": 7.154148128762104e-06,
      "loss": 0.5497,
      "step": 1544
    },
    {
      "epoch": 0.04044008585338615,
      "grad_norm": 17.887359619140625,
      "learning_rate": 7.151007589636221e-06,
      "loss": 0.3604,
      "step": 1545
    },
    {
      "epoch": 0.04046626066623624,
      "grad_norm": 24.26658058166504,
      "learning_rate": 7.147867050510338e-06,
      "loss": 0.6854,
      "step": 1546
    },
    {
      "epoch": 0.040492435479086326,
      "grad_norm": 26.819042205810547,
      "learning_rate": 7.144726511384454e-06,
      "loss": 0.737,
      "step": 1547
    },
    {
      "epoch": 0.040518610291936416,
      "grad_norm": 22.899566650390625,
      "learning_rate": 7.141585972258571e-06,
      "loss": 0.7999,
      "step": 1548
    },
    {
      "epoch": 0.0405447851047865,
      "grad_norm": 16.340356826782227,
      "learning_rate": 7.138445433132688e-06,
      "loss": 0.3367,
      "step": 1549
    },
    {
      "epoch": 0.04057095991763659,
      "grad_norm": 26.436843872070312,
      "learning_rate": 7.1353048940068045e-06,
      "loss": 0.552,
      "step": 1550
    },
    {
      "epoch": 0.04059713473048668,
      "grad_norm": 26.401174545288086,
      "learning_rate": 7.132164354880922e-06,
      "loss": 0.4909,
      "step": 1551
    },
    {
      "epoch": 0.04062330954333677,
      "grad_norm": 19.42303466796875,
      "learning_rate": 7.129023815755039e-06,
      "loss": 0.529,
      "step": 1552
    },
    {
      "epoch": 0.04064948435618685,
      "grad_norm": 14.917716026306152,
      "learning_rate": 7.1258832766291555e-06,
      "loss": 0.3057,
      "step": 1553
    },
    {
      "epoch": 0.04067565916903694,
      "grad_norm": 17.406084060668945,
      "learning_rate": 7.122742737503272e-06,
      "loss": 0.5126,
      "step": 1554
    },
    {
      "epoch": 0.04070183398188703,
      "grad_norm": 20.506393432617188,
      "learning_rate": 7.119602198377389e-06,
      "loss": 0.6362,
      "step": 1555
    },
    {
      "epoch": 0.04072800879473712,
      "grad_norm": 19.33434295654297,
      "learning_rate": 7.116461659251505e-06,
      "loss": 0.3688,
      "step": 1556
    },
    {
      "epoch": 0.04075418360758721,
      "grad_norm": 18.103010177612305,
      "learning_rate": 7.1133211201256214e-06,
      "loss": 0.7706,
      "step": 1557
    },
    {
      "epoch": 0.04078035842043729,
      "grad_norm": 18.411474227905273,
      "learning_rate": 7.110180580999738e-06,
      "loss": 0.8046,
      "step": 1558
    },
    {
      "epoch": 0.04080653323328738,
      "grad_norm": 22.779672622680664,
      "learning_rate": 7.107040041873855e-06,
      "loss": 0.5869,
      "step": 1559
    },
    {
      "epoch": 0.04083270804613747,
      "grad_norm": 17.125019073486328,
      "learning_rate": 7.1038995027479716e-06,
      "loss": 0.5268,
      "step": 1560
    },
    {
      "epoch": 0.04085888285898756,
      "grad_norm": 21.77474594116211,
      "learning_rate": 7.100758963622088e-06,
      "loss": 1.4906,
      "step": 1561
    },
    {
      "epoch": 0.040885057671837644,
      "grad_norm": 16.19542121887207,
      "learning_rate": 7.097618424496205e-06,
      "loss": 0.6025,
      "step": 1562
    },
    {
      "epoch": 0.04091123248468773,
      "grad_norm": 13.3380708694458,
      "learning_rate": 7.094477885370322e-06,
      "loss": 0.278,
      "step": 1563
    },
    {
      "epoch": 0.04093740729753782,
      "grad_norm": 24.506296157836914,
      "learning_rate": 7.091337346244439e-06,
      "loss": 1.184,
      "step": 1564
    },
    {
      "epoch": 0.04096358211038791,
      "grad_norm": 21.887365341186523,
      "learning_rate": 7.088196807118556e-06,
      "loss": 0.5315,
      "step": 1565
    },
    {
      "epoch": 0.040989756923238,
      "grad_norm": 16.260459899902344,
      "learning_rate": 7.085056267992673e-06,
      "loss": 0.34,
      "step": 1566
    },
    {
      "epoch": 0.041015931736088085,
      "grad_norm": 13.498217582702637,
      "learning_rate": 7.081915728866789e-06,
      "loss": 0.4517,
      "step": 1567
    },
    {
      "epoch": 0.041042106548938175,
      "grad_norm": 35.78712844848633,
      "learning_rate": 7.078775189740906e-06,
      "loss": 0.672,
      "step": 1568
    },
    {
      "epoch": 0.041068281361788264,
      "grad_norm": 26.025020599365234,
      "learning_rate": 7.075634650615023e-06,
      "loss": 0.5432,
      "step": 1569
    },
    {
      "epoch": 0.041094456174638354,
      "grad_norm": 19.246278762817383,
      "learning_rate": 7.0724941114891395e-06,
      "loss": 0.4049,
      "step": 1570
    },
    {
      "epoch": 0.04112063098748844,
      "grad_norm": 50.2352409362793,
      "learning_rate": 7.069353572363256e-06,
      "loss": 0.7072,
      "step": 1571
    },
    {
      "epoch": 0.041146805800338526,
      "grad_norm": 23.259685516357422,
      "learning_rate": 7.066213033237372e-06,
      "loss": 0.7076,
      "step": 1572
    },
    {
      "epoch": 0.041172980613188616,
      "grad_norm": 24.470945358276367,
      "learning_rate": 7.063072494111489e-06,
      "loss": 0.7352,
      "step": 1573
    },
    {
      "epoch": 0.041199155426038706,
      "grad_norm": 17.035778045654297,
      "learning_rate": 7.0599319549856054e-06,
      "loss": 0.5027,
      "step": 1574
    },
    {
      "epoch": 0.041225330238888795,
      "grad_norm": 37.40512466430664,
      "learning_rate": 7.056791415859722e-06,
      "loss": 0.6441,
      "step": 1575
    },
    {
      "epoch": 0.04125150505173888,
      "grad_norm": 23.68398094177246,
      "learning_rate": 7.053650876733839e-06,
      "loss": 0.3673,
      "step": 1576
    },
    {
      "epoch": 0.04127767986458897,
      "grad_norm": 20.751331329345703,
      "learning_rate": 7.050510337607956e-06,
      "loss": 0.7636,
      "step": 1577
    },
    {
      "epoch": 0.04130385467743906,
      "grad_norm": 24.84385108947754,
      "learning_rate": 7.047369798482073e-06,
      "loss": 0.5494,
      "step": 1578
    },
    {
      "epoch": 0.04133002949028915,
      "grad_norm": 18.96379280090332,
      "learning_rate": 7.04422925935619e-06,
      "loss": 0.644,
      "step": 1579
    },
    {
      "epoch": 0.04135620430313923,
      "grad_norm": 22.08588218688965,
      "learning_rate": 7.0410887202303065e-06,
      "loss": 0.6097,
      "step": 1580
    },
    {
      "epoch": 0.04138237911598932,
      "grad_norm": 13.562819480895996,
      "learning_rate": 7.037948181104423e-06,
      "loss": 0.3529,
      "step": 1581
    },
    {
      "epoch": 0.04140855392883941,
      "grad_norm": 11.147430419921875,
      "learning_rate": 7.03480764197854e-06,
      "loss": 0.2779,
      "step": 1582
    },
    {
      "epoch": 0.0414347287416895,
      "grad_norm": 19.585811614990234,
      "learning_rate": 7.031667102852657e-06,
      "loss": 0.669,
      "step": 1583
    },
    {
      "epoch": 0.04146090355453959,
      "grad_norm": 18.94293785095215,
      "learning_rate": 7.028526563726773e-06,
      "loss": 0.5527,
      "step": 1584
    },
    {
      "epoch": 0.04148707836738967,
      "grad_norm": 17.71603012084961,
      "learning_rate": 7.025386024600891e-06,
      "loss": 0.3222,
      "step": 1585
    },
    {
      "epoch": 0.04151325318023976,
      "grad_norm": 24.929441452026367,
      "learning_rate": 7.022245485475008e-06,
      "loss": 0.6467,
      "step": 1586
    },
    {
      "epoch": 0.04153942799308985,
      "grad_norm": 24.290382385253906,
      "learning_rate": 7.019104946349123e-06,
      "loss": 0.5777,
      "step": 1587
    },
    {
      "epoch": 0.04156560280593994,
      "grad_norm": 20.127899169921875,
      "learning_rate": 7.015964407223239e-06,
      "loss": 0.4984,
      "step": 1588
    },
    {
      "epoch": 0.04159177761879002,
      "grad_norm": 14.831968307495117,
      "learning_rate": 7.012823868097356e-06,
      "loss": 0.2991,
      "step": 1589
    },
    {
      "epoch": 0.04161795243164011,
      "grad_norm": 41.57707214355469,
      "learning_rate": 7.009683328971474e-06,
      "loss": 0.6375,
      "step": 1590
    },
    {
      "epoch": 0.0416441272444902,
      "grad_norm": 33.108280181884766,
      "learning_rate": 7.00654278984559e-06,
      "loss": 0.7877,
      "step": 1591
    },
    {
      "epoch": 0.04167030205734029,
      "grad_norm": 13.50328254699707,
      "learning_rate": 7.003402250719707e-06,
      "loss": 0.3753,
      "step": 1592
    },
    {
      "epoch": 0.04169647687019038,
      "grad_norm": 25.52310562133789,
      "learning_rate": 7.000261711593824e-06,
      "loss": 0.2816,
      "step": 1593
    },
    {
      "epoch": 0.041722651683040464,
      "grad_norm": 14.572945594787598,
      "learning_rate": 6.99712117246794e-06,
      "loss": 0.3113,
      "step": 1594
    },
    {
      "epoch": 0.041748826495890554,
      "grad_norm": 27.59979248046875,
      "learning_rate": 6.993980633342057e-06,
      "loss": 0.9665,
      "step": 1595
    },
    {
      "epoch": 0.04177500130874064,
      "grad_norm": 14.75218677520752,
      "learning_rate": 6.990840094216174e-06,
      "loss": 0.3952,
      "step": 1596
    },
    {
      "epoch": 0.04180117612159073,
      "grad_norm": 18.661041259765625,
      "learning_rate": 6.9876995550902905e-06,
      "loss": 0.4587,
      "step": 1597
    },
    {
      "epoch": 0.041827350934440816,
      "grad_norm": 24.291776657104492,
      "learning_rate": 6.984559015964408e-06,
      "loss": 0.5395,
      "step": 1598
    },
    {
      "epoch": 0.041853525747290905,
      "grad_norm": 19.072357177734375,
      "learning_rate": 6.981418476838525e-06,
      "loss": 0.6265,
      "step": 1599
    },
    {
      "epoch": 0.041879700560140995,
      "grad_norm": 17.399372100830078,
      "learning_rate": 6.9782779377126415e-06,
      "loss": 0.3327,
      "step": 1600
    },
    {
      "epoch": 0.041905875372991085,
      "grad_norm": 16.511123657226562,
      "learning_rate": 6.975137398586758e-06,
      "loss": 0.4353,
      "step": 1601
    },
    {
      "epoch": 0.041932050185841174,
      "grad_norm": 25.537254333496094,
      "learning_rate": 6.971996859460875e-06,
      "loss": 0.7863,
      "step": 1602
    },
    {
      "epoch": 0.04195822499869126,
      "grad_norm": 24.744041442871094,
      "learning_rate": 6.968856320334991e-06,
      "loss": 0.7921,
      "step": 1603
    },
    {
      "epoch": 0.04198439981154135,
      "grad_norm": 12.644044876098633,
      "learning_rate": 6.9657157812091075e-06,
      "loss": 0.2631,
      "step": 1604
    },
    {
      "epoch": 0.042010574624391436,
      "grad_norm": 14.11544132232666,
      "learning_rate": 6.962575242083224e-06,
      "loss": 0.3785,
      "step": 1605
    },
    {
      "epoch": 0.042036749437241526,
      "grad_norm": 16.891685485839844,
      "learning_rate": 6.959434702957341e-06,
      "loss": 0.426,
      "step": 1606
    },
    {
      "epoch": 0.04206292425009161,
      "grad_norm": 16.004175186157227,
      "learning_rate": 6.956294163831458e-06,
      "loss": 0.312,
      "step": 1607
    },
    {
      "epoch": 0.0420890990629417,
      "grad_norm": 20.601465225219727,
      "learning_rate": 6.953153624705574e-06,
      "loss": 0.5532,
      "step": 1608
    },
    {
      "epoch": 0.04211527387579179,
      "grad_norm": 24.50422477722168,
      "learning_rate": 6.950013085579691e-06,
      "loss": 0.6064,
      "step": 1609
    },
    {
      "epoch": 0.04214144868864188,
      "grad_norm": 16.972768783569336,
      "learning_rate": 6.946872546453808e-06,
      "loss": 0.6054,
      "step": 1610
    },
    {
      "epoch": 0.04216762350149197,
      "grad_norm": 23.216629028320312,
      "learning_rate": 6.943732007327925e-06,
      "loss": 0.369,
      "step": 1611
    },
    {
      "epoch": 0.04219379831434205,
      "grad_norm": 23.014625549316406,
      "learning_rate": 6.940591468202042e-06,
      "loss": 0.4893,
      "step": 1612
    },
    {
      "epoch": 0.04221997312719214,
      "grad_norm": 26.30803108215332,
      "learning_rate": 6.937450929076159e-06,
      "loss": 0.7322,
      "step": 1613
    },
    {
      "epoch": 0.04224614794004223,
      "grad_norm": 22.46484375,
      "learning_rate": 6.934310389950275e-06,
      "loss": 0.6556,
      "step": 1614
    },
    {
      "epoch": 0.04227232275289232,
      "grad_norm": 14.025008201599121,
      "learning_rate": 6.931169850824392e-06,
      "loss": 0.4295,
      "step": 1615
    },
    {
      "epoch": 0.0422984975657424,
      "grad_norm": 16.0169677734375,
      "learning_rate": 6.928029311698509e-06,
      "loss": 0.5473,
      "step": 1616
    },
    {
      "epoch": 0.04232467237859249,
      "grad_norm": 18.321598052978516,
      "learning_rate": 6.9248887725726255e-06,
      "loss": 0.7,
      "step": 1617
    },
    {
      "epoch": 0.04235084719144258,
      "grad_norm": 29.60526466369629,
      "learning_rate": 6.921748233446741e-06,
      "loss": 0.4783,
      "step": 1618
    },
    {
      "epoch": 0.04237702200429267,
      "grad_norm": 23.54730224609375,
      "learning_rate": 6.918607694320858e-06,
      "loss": 0.3803,
      "step": 1619
    },
    {
      "epoch": 0.04240319681714276,
      "grad_norm": 15.319228172302246,
      "learning_rate": 6.915467155194975e-06,
      "loss": 0.6878,
      "step": 1620
    },
    {
      "epoch": 0.04242937162999284,
      "grad_norm": 18.52412223815918,
      "learning_rate": 6.9123266160690915e-06,
      "loss": 0.4184,
      "step": 1621
    },
    {
      "epoch": 0.04245554644284293,
      "grad_norm": 13.506760597229004,
      "learning_rate": 6.909186076943208e-06,
      "loss": 0.3843,
      "step": 1622
    },
    {
      "epoch": 0.04248172125569302,
      "grad_norm": 21.843969345092773,
      "learning_rate": 6.906045537817325e-06,
      "loss": 0.7068,
      "step": 1623
    },
    {
      "epoch": 0.04250789606854311,
      "grad_norm": 12.779033660888672,
      "learning_rate": 6.9029049986914425e-06,
      "loss": 0.5424,
      "step": 1624
    },
    {
      "epoch": 0.042534070881393195,
      "grad_norm": 16.891822814941406,
      "learning_rate": 6.899764459565559e-06,
      "loss": 0.5771,
      "step": 1625
    },
    {
      "epoch": 0.042560245694243284,
      "grad_norm": 12.203009605407715,
      "learning_rate": 6.896623920439676e-06,
      "loss": 0.2686,
      "step": 1626
    },
    {
      "epoch": 0.042586420507093374,
      "grad_norm": 18.242231369018555,
      "learning_rate": 6.8934833813137926e-06,
      "loss": 0.5773,
      "step": 1627
    },
    {
      "epoch": 0.042612595319943464,
      "grad_norm": 19.35545539855957,
      "learning_rate": 6.890342842187909e-06,
      "loss": 0.4991,
      "step": 1628
    },
    {
      "epoch": 0.04263877013279355,
      "grad_norm": 22.183658599853516,
      "learning_rate": 6.887202303062026e-06,
      "loss": 0.6799,
      "step": 1629
    },
    {
      "epoch": 0.042664944945643636,
      "grad_norm": 29.17975425720215,
      "learning_rate": 6.884061763936143e-06,
      "loss": 0.6933,
      "step": 1630
    },
    {
      "epoch": 0.042691119758493726,
      "grad_norm": 18.433395385742188,
      "learning_rate": 6.880921224810259e-06,
      "loss": 0.7572,
      "step": 1631
    },
    {
      "epoch": 0.042717294571343815,
      "grad_norm": 24.27157211303711,
      "learning_rate": 6.877780685684377e-06,
      "loss": 0.7872,
      "step": 1632
    },
    {
      "epoch": 0.042743469384193905,
      "grad_norm": 14.453280448913574,
      "learning_rate": 6.874640146558494e-06,
      "loss": 0.4774,
      "step": 1633
    },
    {
      "epoch": 0.04276964419704399,
      "grad_norm": 30.061294555664062,
      "learning_rate": 6.871499607432609e-06,
      "loss": 0.4781,
      "step": 1634
    },
    {
      "epoch": 0.04279581900989408,
      "grad_norm": 41.66142654418945,
      "learning_rate": 6.868359068306725e-06,
      "loss": 0.5644,
      "step": 1635
    },
    {
      "epoch": 0.04282199382274417,
      "grad_norm": 28.123624801635742,
      "learning_rate": 6.865218529180842e-06,
      "loss": 0.6836,
      "step": 1636
    },
    {
      "epoch": 0.04284816863559426,
      "grad_norm": 22.478628158569336,
      "learning_rate": 6.86207799005496e-06,
      "loss": 0.6129,
      "step": 1637
    },
    {
      "epoch": 0.042874343448444346,
      "grad_norm": 22.82772445678711,
      "learning_rate": 6.858937450929076e-06,
      "loss": 0.5679,
      "step": 1638
    },
    {
      "epoch": 0.04290051826129443,
      "grad_norm": 16.3467960357666,
      "learning_rate": 6.855796911803193e-06,
      "loss": 0.6493,
      "step": 1639
    },
    {
      "epoch": 0.04292669307414452,
      "grad_norm": 19.201677322387695,
      "learning_rate": 6.85265637267731e-06,
      "loss": 0.3877,
      "step": 1640
    },
    {
      "epoch": 0.04295286788699461,
      "grad_norm": 17.967382431030273,
      "learning_rate": 6.8495158335514265e-06,
      "loss": 0.5378,
      "step": 1641
    },
    {
      "epoch": 0.0429790426998447,
      "grad_norm": 19.262195587158203,
      "learning_rate": 6.846375294425543e-06,
      "loss": 0.3812,
      "step": 1642
    },
    {
      "epoch": 0.04300521751269478,
      "grad_norm": 21.059282302856445,
      "learning_rate": 6.84323475529966e-06,
      "loss": 0.5209,
      "step": 1643
    },
    {
      "epoch": 0.04303139232554487,
      "grad_norm": 16.5162410736084,
      "learning_rate": 6.840094216173777e-06,
      "loss": 0.5783,
      "step": 1644
    },
    {
      "epoch": 0.04305756713839496,
      "grad_norm": 17.10845184326172,
      "learning_rate": 6.836953677047894e-06,
      "loss": 0.5538,
      "step": 1645
    },
    {
      "epoch": 0.04308374195124505,
      "grad_norm": 15.206286430358887,
      "learning_rate": 6.833813137922011e-06,
      "loss": 0.3637,
      "step": 1646
    },
    {
      "epoch": 0.04310991676409514,
      "grad_norm": 14.547956466674805,
      "learning_rate": 6.8306725987961275e-06,
      "loss": 0.3945,
      "step": 1647
    },
    {
      "epoch": 0.04313609157694522,
      "grad_norm": 13.490609169006348,
      "learning_rate": 6.827532059670244e-06,
      "loss": 0.3846,
      "step": 1648
    },
    {
      "epoch": 0.04316226638979531,
      "grad_norm": 21.559545516967773,
      "learning_rate": 6.824391520544359e-06,
      "loss": 0.5798,
      "step": 1649
    },
    {
      "epoch": 0.0431884412026454,
      "grad_norm": 22.716554641723633,
      "learning_rate": 6.821250981418477e-06,
      "loss": 0.8034,
      "step": 1650
    },
    {
      "epoch": 0.04321461601549549,
      "grad_norm": 22.185558319091797,
      "learning_rate": 6.8181104422925935e-06,
      "loss": 0.6648,
      "step": 1651
    },
    {
      "epoch": 0.04324079082834558,
      "grad_norm": 22.03158187866211,
      "learning_rate": 6.81496990316671e-06,
      "loss": 0.5414,
      "step": 1652
    },
    {
      "epoch": 0.043266965641195664,
      "grad_norm": 15.67612075805664,
      "learning_rate": 6.811829364040827e-06,
      "loss": 0.4298,
      "step": 1653
    },
    {
      "epoch": 0.04329314045404575,
      "grad_norm": 15.116412162780762,
      "learning_rate": 6.808688824914944e-06,
      "loss": 0.3484,
      "step": 1654
    },
    {
      "epoch": 0.04331931526689584,
      "grad_norm": 18.404340744018555,
      "learning_rate": 6.80554828578906e-06,
      "loss": 0.4363,
      "step": 1655
    },
    {
      "epoch": 0.04334549007974593,
      "grad_norm": 17.632648468017578,
      "learning_rate": 6.802407746663177e-06,
      "loss": 0.4503,
      "step": 1656
    },
    {
      "epoch": 0.043371664892596015,
      "grad_norm": 17.336153030395508,
      "learning_rate": 6.799267207537294e-06,
      "loss": 0.539,
      "step": 1657
    },
    {
      "epoch": 0.043397839705446105,
      "grad_norm": 21.730690002441406,
      "learning_rate": 6.796126668411411e-06,
      "loss": 0.8876,
      "step": 1658
    },
    {
      "epoch": 0.043424014518296195,
      "grad_norm": 11.551973342895508,
      "learning_rate": 6.792986129285528e-06,
      "loss": 0.352,
      "step": 1659
    },
    {
      "epoch": 0.043450189331146284,
      "grad_norm": 20.080446243286133,
      "learning_rate": 6.789845590159645e-06,
      "loss": 0.496,
      "step": 1660
    },
    {
      "epoch": 0.043476364143996374,
      "grad_norm": 22.795211791992188,
      "learning_rate": 6.7867050510337614e-06,
      "loss": 0.444,
      "step": 1661
    },
    {
      "epoch": 0.04350253895684646,
      "grad_norm": 19.992345809936523,
      "learning_rate": 6.783564511907878e-06,
      "loss": 0.552,
      "step": 1662
    },
    {
      "epoch": 0.043528713769696546,
      "grad_norm": 19.50979232788086,
      "learning_rate": 6.780423972781995e-06,
      "loss": 0.4497,
      "step": 1663
    },
    {
      "epoch": 0.043554888582546636,
      "grad_norm": 19.60716438293457,
      "learning_rate": 6.777283433656111e-06,
      "loss": 0.7922,
      "step": 1664
    },
    {
      "epoch": 0.043581063395396726,
      "grad_norm": 17.300325393676758,
      "learning_rate": 6.774142894530227e-06,
      "loss": 0.4319,
      "step": 1665
    },
    {
      "epoch": 0.04360723820824681,
      "grad_norm": 17.776199340820312,
      "learning_rate": 6.771002355404344e-06,
      "loss": 0.3575,
      "step": 1666
    },
    {
      "epoch": 0.0436334130210969,
      "grad_norm": 17.03463363647461,
      "learning_rate": 6.767861816278461e-06,
      "loss": 0.4289,
      "step": 1667
    },
    {
      "epoch": 0.04365958783394699,
      "grad_norm": 14.031790733337402,
      "learning_rate": 6.7647212771525775e-06,
      "loss": 0.2539,
      "step": 1668
    },
    {
      "epoch": 0.04368576264679708,
      "grad_norm": 19.68433952331543,
      "learning_rate": 6.761580738026694e-06,
      "loss": 0.6045,
      "step": 1669
    },
    {
      "epoch": 0.04371193745964717,
      "grad_norm": 16.460479736328125,
      "learning_rate": 6.758440198900811e-06,
      "loss": 0.6343,
      "step": 1670
    },
    {
      "epoch": 0.04373811227249725,
      "grad_norm": 16.76729965209961,
      "learning_rate": 6.7552996597749285e-06,
      "loss": 0.3374,
      "step": 1671
    },
    {
      "epoch": 0.04376428708534734,
      "grad_norm": 15.69357681274414,
      "learning_rate": 6.752159120649045e-06,
      "loss": 0.3822,
      "step": 1672
    },
    {
      "epoch": 0.04379046189819743,
      "grad_norm": 22.493928909301758,
      "learning_rate": 6.749018581523162e-06,
      "loss": 0.5167,
      "step": 1673
    },
    {
      "epoch": 0.04381663671104752,
      "grad_norm": 12.543985366821289,
      "learning_rate": 6.745878042397279e-06,
      "loss": 0.2753,
      "step": 1674
    },
    {
      "epoch": 0.0438428115238976,
      "grad_norm": 29.070297241210938,
      "learning_rate": 6.742737503271395e-06,
      "loss": 0.5518,
      "step": 1675
    },
    {
      "epoch": 0.04386898633674769,
      "grad_norm": 21.78032875061035,
      "learning_rate": 6.739596964145512e-06,
      "loss": 0.3869,
      "step": 1676
    },
    {
      "epoch": 0.04389516114959778,
      "grad_norm": 17.2291202545166,
      "learning_rate": 6.736456425019629e-06,
      "loss": 0.4415,
      "step": 1677
    },
    {
      "epoch": 0.04392133596244787,
      "grad_norm": 16.686180114746094,
      "learning_rate": 6.7333158858937454e-06,
      "loss": 0.538,
      "step": 1678
    },
    {
      "epoch": 0.04394751077529796,
      "grad_norm": 42.567752838134766,
      "learning_rate": 6.730175346767863e-06,
      "loss": 0.6577,
      "step": 1679
    },
    {
      "epoch": 0.04397368558814804,
      "grad_norm": 17.721986770629883,
      "learning_rate": 6.727034807641978e-06,
      "loss": 0.4442,
      "step": 1680
    },
    {
      "epoch": 0.04399986040099813,
      "grad_norm": 16.05643081665039,
      "learning_rate": 6.723894268516095e-06,
      "loss": 0.3927,
      "step": 1681
    },
    {
      "epoch": 0.04402603521384822,
      "grad_norm": 11.313304901123047,
      "learning_rate": 6.720753729390211e-06,
      "loss": 0.2289,
      "step": 1682
    },
    {
      "epoch": 0.04405221002669831,
      "grad_norm": 15.933414459228516,
      "learning_rate": 6.717613190264328e-06,
      "loss": 0.395,
      "step": 1683
    },
    {
      "epoch": 0.044078384839548394,
      "grad_norm": 13.822834014892578,
      "learning_rate": 6.714472651138446e-06,
      "loss": 0.3002,
      "step": 1684
    },
    {
      "epoch": 0.044104559652398484,
      "grad_norm": 23.21363639831543,
      "learning_rate": 6.711332112012562e-06,
      "loss": 0.5733,
      "step": 1685
    },
    {
      "epoch": 0.044130734465248574,
      "grad_norm": 17.761587142944336,
      "learning_rate": 6.708191572886679e-06,
      "loss": 0.4422,
      "step": 1686
    },
    {
      "epoch": 0.04415690927809866,
      "grad_norm": 15.780046463012695,
      "learning_rate": 6.705051033760796e-06,
      "loss": 0.4683,
      "step": 1687
    },
    {
      "epoch": 0.04418308409094875,
      "grad_norm": 18.512514114379883,
      "learning_rate": 6.7019104946349125e-06,
      "loss": 0.4455,
      "step": 1688
    },
    {
      "epoch": 0.044209258903798836,
      "grad_norm": 25.937116622924805,
      "learning_rate": 6.698769955509029e-06,
      "loss": 0.8904,
      "step": 1689
    },
    {
      "epoch": 0.044235433716648925,
      "grad_norm": 21.87374496459961,
      "learning_rate": 6.695629416383146e-06,
      "loss": 0.4242,
      "step": 1690
    },
    {
      "epoch": 0.044261608529499015,
      "grad_norm": 17.366445541381836,
      "learning_rate": 6.692488877257263e-06,
      "loss": 0.3369,
      "step": 1691
    },
    {
      "epoch": 0.044287783342349105,
      "grad_norm": 20.816694259643555,
      "learning_rate": 6.68934833813138e-06,
      "loss": 0.4593,
      "step": 1692
    },
    {
      "epoch": 0.04431395815519919,
      "grad_norm": 18.874021530151367,
      "learning_rate": 6.686207799005497e-06,
      "loss": 0.7509,
      "step": 1693
    },
    {
      "epoch": 0.04434013296804928,
      "grad_norm": 23.495283126831055,
      "learning_rate": 6.683067259879614e-06,
      "loss": 0.4257,
      "step": 1694
    },
    {
      "epoch": 0.04436630778089937,
      "grad_norm": 19.232725143432617,
      "learning_rate": 6.679926720753729e-06,
      "loss": 0.4981,
      "step": 1695
    },
    {
      "epoch": 0.044392482593749456,
      "grad_norm": 19.710052490234375,
      "learning_rate": 6.676786181627845e-06,
      "loss": 0.3513,
      "step": 1696
    },
    {
      "epoch": 0.044418657406599546,
      "grad_norm": 21.478679656982422,
      "learning_rate": 6.673645642501963e-06,
      "loss": 0.4448,
      "step": 1697
    },
    {
      "epoch": 0.04444483221944963,
      "grad_norm": 27.89900779724121,
      "learning_rate": 6.6705051033760796e-06,
      "loss": 0.3563,
      "step": 1698
    },
    {
      "epoch": 0.04447100703229972,
      "grad_norm": 12.505231857299805,
      "learning_rate": 6.667364564250196e-06,
      "loss": 0.2716,
      "step": 1699
    },
    {
      "epoch": 0.04449718184514981,
      "grad_norm": 22.65135955810547,
      "learning_rate": 6.664224025124313e-06,
      "loss": 0.5465,
      "step": 1700
    },
    {
      "epoch": 0.0445233566579999,
      "grad_norm": 14.045134544372559,
      "learning_rate": 6.66108348599843e-06,
      "loss": 0.2475,
      "step": 1701
    },
    {
      "epoch": 0.04454953147084998,
      "grad_norm": 18.442283630371094,
      "learning_rate": 6.657942946872546e-06,
      "loss": 0.352,
      "step": 1702
    },
    {
      "epoch": 0.04457570628370007,
      "grad_norm": 17.553909301757812,
      "learning_rate": 6.654802407746663e-06,
      "loss": 0.2374,
      "step": 1703
    },
    {
      "epoch": 0.04460188109655016,
      "grad_norm": 16.177038192749023,
      "learning_rate": 6.65166186862078e-06,
      "loss": 0.4838,
      "step": 1704
    },
    {
      "epoch": 0.04462805590940025,
      "grad_norm": 15.81316089630127,
      "learning_rate": 6.648521329494897e-06,
      "loss": 0.2373,
      "step": 1705
    },
    {
      "epoch": 0.04465423072225034,
      "grad_norm": 18.334447860717773,
      "learning_rate": 6.645380790369014e-06,
      "loss": 0.5192,
      "step": 1706
    },
    {
      "epoch": 0.04468040553510042,
      "grad_norm": 24.311220169067383,
      "learning_rate": 6.642240251243131e-06,
      "loss": 0.5958,
      "step": 1707
    },
    {
      "epoch": 0.04470658034795051,
      "grad_norm": 15.971857070922852,
      "learning_rate": 6.6390997121172475e-06,
      "loss": 0.39,
      "step": 1708
    },
    {
      "epoch": 0.0447327551608006,
      "grad_norm": 27.879444122314453,
      "learning_rate": 6.635959172991364e-06,
      "loss": 0.4427,
      "step": 1709
    },
    {
      "epoch": 0.04475892997365069,
      "grad_norm": 26.84519386291504,
      "learning_rate": 6.632818633865481e-06,
      "loss": 0.5916,
      "step": 1710
    },
    {
      "epoch": 0.04478510478650077,
      "grad_norm": 15.153976440429688,
      "learning_rate": 6.629678094739597e-06,
      "loss": 0.2291,
      "step": 1711
    },
    {
      "epoch": 0.04481127959935086,
      "grad_norm": 17.044496536254883,
      "learning_rate": 6.6265375556137134e-06,
      "loss": 0.3882,
      "step": 1712
    },
    {
      "epoch": 0.04483745441220095,
      "grad_norm": 21.49253273010254,
      "learning_rate": 6.62339701648783e-06,
      "loss": 0.3392,
      "step": 1713
    },
    {
      "epoch": 0.04486362922505104,
      "grad_norm": 17.5216007232666,
      "learning_rate": 6.620256477361947e-06,
      "loss": 0.7094,
      "step": 1714
    },
    {
      "epoch": 0.04488980403790113,
      "grad_norm": 20.515024185180664,
      "learning_rate": 6.6171159382360636e-06,
      "loss": 0.5833,
      "step": 1715
    },
    {
      "epoch": 0.044915978850751215,
      "grad_norm": 18.340322494506836,
      "learning_rate": 6.61397539911018e-06,
      "loss": 0.4833,
      "step": 1716
    },
    {
      "epoch": 0.044942153663601304,
      "grad_norm": 24.520559310913086,
      "learning_rate": 6.610834859984297e-06,
      "loss": 0.2965,
      "step": 1717
    },
    {
      "epoch": 0.044968328476451394,
      "grad_norm": 22.35999298095703,
      "learning_rate": 6.6076943208584145e-06,
      "loss": 0.4171,
      "step": 1718
    },
    {
      "epoch": 0.044994503289301484,
      "grad_norm": 13.21886157989502,
      "learning_rate": 6.604553781732531e-06,
      "loss": 0.3192,
      "step": 1719
    },
    {
      "epoch": 0.045020678102151566,
      "grad_norm": 22.987323760986328,
      "learning_rate": 6.601413242606648e-06,
      "loss": 0.3003,
      "step": 1720
    },
    {
      "epoch": 0.045046852915001656,
      "grad_norm": 19.725765228271484,
      "learning_rate": 6.598272703480765e-06,
      "loss": 0.4471,
      "step": 1721
    },
    {
      "epoch": 0.045073027727851746,
      "grad_norm": 18.468791961669922,
      "learning_rate": 6.595132164354881e-06,
      "loss": 0.513,
      "step": 1722
    },
    {
      "epoch": 0.045099202540701835,
      "grad_norm": 27.54574966430664,
      "learning_rate": 6.591991625228998e-06,
      "loss": 0.4296,
      "step": 1723
    },
    {
      "epoch": 0.045125377353551925,
      "grad_norm": 17.236135482788086,
      "learning_rate": 6.588851086103115e-06,
      "loss": 0.3207,
      "step": 1724
    },
    {
      "epoch": 0.04515155216640201,
      "grad_norm": 25.08324432373047,
      "learning_rate": 6.5857105469772315e-06,
      "loss": 0.3445,
      "step": 1725
    },
    {
      "epoch": 0.0451777269792521,
      "grad_norm": 27.531036376953125,
      "learning_rate": 6.582570007851347e-06,
      "loss": 0.5932,
      "step": 1726
    },
    {
      "epoch": 0.04520390179210219,
      "grad_norm": 19.761964797973633,
      "learning_rate": 6.579429468725464e-06,
      "loss": 0.2488,
      "step": 1727
    },
    {
      "epoch": 0.04523007660495228,
      "grad_norm": 27.60628890991211,
      "learning_rate": 6.576288929599581e-06,
      "loss": 0.537,
      "step": 1728
    },
    {
      "epoch": 0.04525625141780236,
      "grad_norm": 20.337360382080078,
      "learning_rate": 6.5731483904736975e-06,
      "loss": 0.4893,
      "step": 1729
    },
    {
      "epoch": 0.04528242623065245,
      "grad_norm": 19.424663543701172,
      "learning_rate": 6.570007851347814e-06,
      "loss": 0.285,
      "step": 1730
    },
    {
      "epoch": 0.04530860104350254,
      "grad_norm": 26.21988868713379,
      "learning_rate": 6.566867312221932e-06,
      "loss": 0.3595,
      "step": 1731
    },
    {
      "epoch": 0.04533477585635263,
      "grad_norm": 18.148439407348633,
      "learning_rate": 6.563726773096048e-06,
      "loss": 0.3714,
      "step": 1732
    },
    {
      "epoch": 0.04536095066920272,
      "grad_norm": 26.3016300201416,
      "learning_rate": 6.560586233970165e-06,
      "loss": 0.5748,
      "step": 1733
    },
    {
      "epoch": 0.0453871254820528,
      "grad_norm": 15.891339302062988,
      "learning_rate": 6.557445694844282e-06,
      "loss": 0.3781,
      "step": 1734
    },
    {
      "epoch": 0.04541330029490289,
      "grad_norm": 26.114225387573242,
      "learning_rate": 6.5543051557183985e-06,
      "loss": 0.4129,
      "step": 1735
    },
    {
      "epoch": 0.04543947510775298,
      "grad_norm": 28.872968673706055,
      "learning_rate": 6.551164616592515e-06,
      "loss": 0.4756,
      "step": 1736
    },
    {
      "epoch": 0.04546564992060307,
      "grad_norm": 19.525726318359375,
      "learning_rate": 6.548024077466632e-06,
      "loss": 0.1762,
      "step": 1737
    },
    {
      "epoch": 0.04549182473345315,
      "grad_norm": 9.863844871520996,
      "learning_rate": 6.544883538340749e-06,
      "loss": 0.1901,
      "step": 1738
    },
    {
      "epoch": 0.04551799954630324,
      "grad_norm": 26.41478729248047,
      "learning_rate": 6.541742999214866e-06,
      "loss": 0.829,
      "step": 1739
    },
    {
      "epoch": 0.04554417435915333,
      "grad_norm": 26.661487579345703,
      "learning_rate": 6.538602460088983e-06,
      "loss": 0.415,
      "step": 1740
    },
    {
      "epoch": 0.04557034917200342,
      "grad_norm": 13.486980438232422,
      "learning_rate": 6.5354619209631e-06,
      "loss": 0.382,
      "step": 1741
    },
    {
      "epoch": 0.04559652398485351,
      "grad_norm": 19.94925308227539,
      "learning_rate": 6.532321381837215e-06,
      "loss": 0.7341,
      "step": 1742
    },
    {
      "epoch": 0.045622698797703594,
      "grad_norm": 32.04853057861328,
      "learning_rate": 6.529180842711331e-06,
      "loss": 0.6424,
      "step": 1743
    },
    {
      "epoch": 0.045648873610553684,
      "grad_norm": 9.677535057067871,
      "learning_rate": 6.526040303585449e-06,
      "loss": 0.2679,
      "step": 1744
    },
    {
      "epoch": 0.04567504842340377,
      "grad_norm": 17.988576889038086,
      "learning_rate": 6.522899764459566e-06,
      "loss": 0.5187,
      "step": 1745
    },
    {
      "epoch": 0.04570122323625386,
      "grad_norm": 12.195405006408691,
      "learning_rate": 6.519759225333682e-06,
      "loss": 0.4959,
      "step": 1746
    },
    {
      "epoch": 0.045727398049103946,
      "grad_norm": 29.976871490478516,
      "learning_rate": 6.516618686207799e-06,
      "loss": 0.6346,
      "step": 1747
    },
    {
      "epoch": 0.045753572861954035,
      "grad_norm": 28.849769592285156,
      "learning_rate": 6.513478147081916e-06,
      "loss": 0.5416,
      "step": 1748
    },
    {
      "epoch": 0.045779747674804125,
      "grad_norm": 34.53510284423828,
      "learning_rate": 6.5103376079560324e-06,
      "loss": 0.701,
      "step": 1749
    },
    {
      "epoch": 0.045805922487654215,
      "grad_norm": 41.515316009521484,
      "learning_rate": 6.507197068830149e-06,
      "loss": 0.7443,
      "step": 1750
    },
    {
      "epoch": 0.045832097300504304,
      "grad_norm": 16.461427688598633,
      "learning_rate": 6.504056529704266e-06,
      "loss": 0.3482,
      "step": 1751
    },
    {
      "epoch": 0.04585827211335439,
      "grad_norm": 28.787324905395508,
      "learning_rate": 6.500915990578383e-06,
      "loss": 0.8399,
      "step": 1752
    },
    {
      "epoch": 0.04588444692620448,
      "grad_norm": 32.85402297973633,
      "learning_rate": 6.4977754514525e-06,
      "loss": 0.7017,
      "step": 1753
    },
    {
      "epoch": 0.045910621739054566,
      "grad_norm": 18.567480087280273,
      "learning_rate": 6.494634912326617e-06,
      "loss": 0.5477,
      "step": 1754
    },
    {
      "epoch": 0.045936796551904656,
      "grad_norm": 20.300264358520508,
      "learning_rate": 6.4914943732007335e-06,
      "loss": 0.5715,
      "step": 1755
    },
    {
      "epoch": 0.04596297136475474,
      "grad_norm": 20.508020401000977,
      "learning_rate": 6.48835383407485e-06,
      "loss": 0.4999,
      "step": 1756
    },
    {
      "epoch": 0.04598914617760483,
      "grad_norm": 19.049518585205078,
      "learning_rate": 6.485213294948966e-06,
      "loss": 0.438,
      "step": 1757
    },
    {
      "epoch": 0.04601532099045492,
      "grad_norm": 20.815380096435547,
      "learning_rate": 6.482072755823083e-06,
      "loss": 0.4737,
      "step": 1758
    },
    {
      "epoch": 0.04604149580330501,
      "grad_norm": 15.061354637145996,
      "learning_rate": 6.4789322166971995e-06,
      "loss": 0.6701,
      "step": 1759
    },
    {
      "epoch": 0.0460676706161551,
      "grad_norm": 28.88490104675293,
      "learning_rate": 6.475791677571316e-06,
      "loss": 0.6718,
      "step": 1760
    },
    {
      "epoch": 0.04609384542900518,
      "grad_norm": 16.75082015991211,
      "learning_rate": 6.472651138445433e-06,
      "loss": 0.3792,
      "step": 1761
    },
    {
      "epoch": 0.04612002024185527,
      "grad_norm": 20.554100036621094,
      "learning_rate": 6.46951059931955e-06,
      "loss": 0.4944,
      "step": 1762
    },
    {
      "epoch": 0.04614619505470536,
      "grad_norm": 20.556318283081055,
      "learning_rate": 6.466370060193666e-06,
      "loss": 0.6144,
      "step": 1763
    },
    {
      "epoch": 0.04617236986755545,
      "grad_norm": 18.216205596923828,
      "learning_rate": 6.463229521067783e-06,
      "loss": 0.5348,
      "step": 1764
    },
    {
      "epoch": 0.04619854468040553,
      "grad_norm": 17.813291549682617,
      "learning_rate": 6.4600889819419006e-06,
      "loss": 0.2763,
      "step": 1765
    },
    {
      "epoch": 0.04622471949325562,
      "grad_norm": 40.885986328125,
      "learning_rate": 6.456948442816017e-06,
      "loss": 0.8567,
      "step": 1766
    },
    {
      "epoch": 0.04625089430610571,
      "grad_norm": 15.528423309326172,
      "learning_rate": 6.453807903690134e-06,
      "loss": 0.3401,
      "step": 1767
    },
    {
      "epoch": 0.0462770691189558,
      "grad_norm": 21.32138442993164,
      "learning_rate": 6.450667364564251e-06,
      "loss": 0.7681,
      "step": 1768
    },
    {
      "epoch": 0.04630324393180589,
      "grad_norm": 26.177709579467773,
      "learning_rate": 6.447526825438367e-06,
      "loss": 0.5174,
      "step": 1769
    },
    {
      "epoch": 0.04632941874465597,
      "grad_norm": 18.40180015563965,
      "learning_rate": 6.444386286312484e-06,
      "loss": 0.3086,
      "step": 1770
    },
    {
      "epoch": 0.04635559355750606,
      "grad_norm": 29.29621124267578,
      "learning_rate": 6.441245747186601e-06,
      "loss": 0.5961,
      "step": 1771
    },
    {
      "epoch": 0.04638176837035615,
      "grad_norm": 23.477174758911133,
      "learning_rate": 6.4381052080607175e-06,
      "loss": 0.3461,
      "step": 1772
    },
    {
      "epoch": 0.04640794318320624,
      "grad_norm": 12.780303001403809,
      "learning_rate": 6.434964668934833e-06,
      "loss": 0.363,
      "step": 1773
    },
    {
      "epoch": 0.04643411799605633,
      "grad_norm": 19.009124755859375,
      "learning_rate": 6.43182412980895e-06,
      "loss": 0.223,
      "step": 1774
    },
    {
      "epoch": 0.046460292808906414,
      "grad_norm": 22.319992065429688,
      "learning_rate": 6.428683590683067e-06,
      "loss": 0.5023,
      "step": 1775
    },
    {
      "epoch": 0.046486467621756504,
      "grad_norm": 22.431541442871094,
      "learning_rate": 6.4255430515571835e-06,
      "loss": 0.6029,
      "step": 1776
    },
    {
      "epoch": 0.046512642434606594,
      "grad_norm": 17.202951431274414,
      "learning_rate": 6.4224025124313e-06,
      "loss": 0.5451,
      "step": 1777
    },
    {
      "epoch": 0.04653881724745668,
      "grad_norm": 25.560394287109375,
      "learning_rate": 6.419261973305418e-06,
      "loss": 0.6164,
      "step": 1778
    },
    {
      "epoch": 0.046564992060306766,
      "grad_norm": 17.311355590820312,
      "learning_rate": 6.4161214341795345e-06,
      "loss": 0.3657,
      "step": 1779
    },
    {
      "epoch": 0.046591166873156856,
      "grad_norm": 18.197486877441406,
      "learning_rate": 6.412980895053651e-06,
      "loss": 0.7238,
      "step": 1780
    },
    {
      "epoch": 0.046617341686006945,
      "grad_norm": 21.268207550048828,
      "learning_rate": 6.409840355927768e-06,
      "loss": 0.3738,
      "step": 1781
    },
    {
      "epoch": 0.046643516498857035,
      "grad_norm": 21.35114097595215,
      "learning_rate": 6.406699816801885e-06,
      "loss": 0.6853,
      "step": 1782
    },
    {
      "epoch": 0.046669691311707125,
      "grad_norm": 21.582212448120117,
      "learning_rate": 6.403559277676001e-06,
      "loss": 0.289,
      "step": 1783
    },
    {
      "epoch": 0.04669586612455721,
      "grad_norm": 10.24063491821289,
      "learning_rate": 6.400418738550118e-06,
      "loss": 0.2586,
      "step": 1784
    },
    {
      "epoch": 0.0467220409374073,
      "grad_norm": 19.553749084472656,
      "learning_rate": 6.397278199424235e-06,
      "loss": 0.3634,
      "step": 1785
    },
    {
      "epoch": 0.04674821575025739,
      "grad_norm": 20.234750747680664,
      "learning_rate": 6.394137660298352e-06,
      "loss": 0.5169,
      "step": 1786
    },
    {
      "epoch": 0.046774390563107476,
      "grad_norm": 18.00937271118164,
      "learning_rate": 6.390997121172469e-06,
      "loss": 0.2551,
      "step": 1787
    },
    {
      "epoch": 0.04680056537595756,
      "grad_norm": 27.53620147705078,
      "learning_rate": 6.387856582046584e-06,
      "loss": 0.4251,
      "step": 1788
    },
    {
      "epoch": 0.04682674018880765,
      "grad_norm": 20.54437828063965,
      "learning_rate": 6.384716042920701e-06,
      "loss": 0.3713,
      "step": 1789
    },
    {
      "epoch": 0.04685291500165774,
      "grad_norm": 18.622783660888672,
      "learning_rate": 6.381575503794817e-06,
      "loss": 0.3438,
      "step": 1790
    },
    {
      "epoch": 0.04687908981450783,
      "grad_norm": 17.350894927978516,
      "learning_rate": 6.378434964668935e-06,
      "loss": 0.5309,
      "step": 1791
    },
    {
      "epoch": 0.04690526462735792,
      "grad_norm": 22.68339729309082,
      "learning_rate": 6.375294425543052e-06,
      "loss": 0.6348,
      "step": 1792
    },
    {
      "epoch": 0.046931439440208,
      "grad_norm": 20.66924285888672,
      "learning_rate": 6.372153886417168e-06,
      "loss": 0.419,
      "step": 1793
    },
    {
      "epoch": 0.04695761425305809,
      "grad_norm": 22.286006927490234,
      "learning_rate": 6.369013347291285e-06,
      "loss": 0.3793,
      "step": 1794
    },
    {
      "epoch": 0.04698378906590818,
      "grad_norm": 26.354549407958984,
      "learning_rate": 6.365872808165402e-06,
      "loss": 0.3412,
      "step": 1795
    },
    {
      "epoch": 0.04700996387875827,
      "grad_norm": 29.538564682006836,
      "learning_rate": 6.3627322690395185e-06,
      "loss": 0.6,
      "step": 1796
    },
    {
      "epoch": 0.04703613869160835,
      "grad_norm": 13.749777793884277,
      "learning_rate": 6.359591729913635e-06,
      "loss": 0.3061,
      "step": 1797
    },
    {
      "epoch": 0.04706231350445844,
      "grad_norm": 26.346084594726562,
      "learning_rate": 6.356451190787752e-06,
      "loss": 0.8349,
      "step": 1798
    },
    {
      "epoch": 0.04708848831730853,
      "grad_norm": 21.264202117919922,
      "learning_rate": 6.3533106516618694e-06,
      "loss": 0.6284,
      "step": 1799
    },
    {
      "epoch": 0.04711466313015862,
      "grad_norm": 19.56787872314453,
      "learning_rate": 6.350170112535986e-06,
      "loss": 0.4608,
      "step": 1800
    },
    {
      "epoch": 0.04714083794300871,
      "grad_norm": 21.894548416137695,
      "learning_rate": 6.347029573410103e-06,
      "loss": 0.6758,
      "step": 1801
    },
    {
      "epoch": 0.04716701275585879,
      "grad_norm": 33.68335723876953,
      "learning_rate": 6.3438890342842196e-06,
      "loss": 0.5966,
      "step": 1802
    },
    {
      "epoch": 0.04719318756870888,
      "grad_norm": 23.5930233001709,
      "learning_rate": 6.340748495158336e-06,
      "loss": 0.4153,
      "step": 1803
    },
    {
      "epoch": 0.04721936238155897,
      "grad_norm": 25.791425704956055,
      "learning_rate": 6.337607956032452e-06,
      "loss": 0.595,
      "step": 1804
    },
    {
      "epoch": 0.04724553719440906,
      "grad_norm": 17.527841567993164,
      "learning_rate": 6.334467416906569e-06,
      "loss": 0.5436,
      "step": 1805
    },
    {
      "epoch": 0.047271712007259145,
      "grad_norm": 17.68760871887207,
      "learning_rate": 6.3313268777806855e-06,
      "loss": 0.2937,
      "step": 1806
    },
    {
      "epoch": 0.047297886820109235,
      "grad_norm": 18.121618270874023,
      "learning_rate": 6.328186338654802e-06,
      "loss": 0.3882,
      "step": 1807
    },
    {
      "epoch": 0.047324061632959324,
      "grad_norm": 27.594627380371094,
      "learning_rate": 6.325045799528919e-06,
      "loss": 0.7588,
      "step": 1808
    },
    {
      "epoch": 0.047350236445809414,
      "grad_norm": 19.87004280090332,
      "learning_rate": 6.321905260403036e-06,
      "loss": 0.3545,
      "step": 1809
    },
    {
      "epoch": 0.047376411258659504,
      "grad_norm": 23.849550247192383,
      "learning_rate": 6.318764721277152e-06,
      "loss": 0.4467,
      "step": 1810
    },
    {
      "epoch": 0.047402586071509586,
      "grad_norm": 18.41518211364746,
      "learning_rate": 6.315624182151269e-06,
      "loss": 0.2116,
      "step": 1811
    },
    {
      "epoch": 0.047428760884359676,
      "grad_norm": 29.568086624145508,
      "learning_rate": 6.312483643025387e-06,
      "loss": 0.8594,
      "step": 1812
    },
    {
      "epoch": 0.047454935697209766,
      "grad_norm": 18.056278228759766,
      "learning_rate": 6.309343103899503e-06,
      "loss": 0.5006,
      "step": 1813
    },
    {
      "epoch": 0.047481110510059855,
      "grad_norm": 30.499340057373047,
      "learning_rate": 6.30620256477362e-06,
      "loss": 0.5797,
      "step": 1814
    },
    {
      "epoch": 0.04750728532290994,
      "grad_norm": 16.621728897094727,
      "learning_rate": 6.303062025647737e-06,
      "loss": 0.325,
      "step": 1815
    },
    {
      "epoch": 0.04753346013576003,
      "grad_norm": 21.25823211669922,
      "learning_rate": 6.2999214865218534e-06,
      "loss": 0.552,
      "step": 1816
    },
    {
      "epoch": 0.04755963494861012,
      "grad_norm": 11.833663940429688,
      "learning_rate": 6.29678094739597e-06,
      "loss": 0.2772,
      "step": 1817
    },
    {
      "epoch": 0.04758580976146021,
      "grad_norm": 17.3842716217041,
      "learning_rate": 6.293640408270087e-06,
      "loss": 0.2846,
      "step": 1818
    },
    {
      "epoch": 0.0476119845743103,
      "grad_norm": 12.912117958068848,
      "learning_rate": 6.290499869144203e-06,
      "loss": 0.2078,
      "step": 1819
    },
    {
      "epoch": 0.04763815938716038,
      "grad_norm": 21.432798385620117,
      "learning_rate": 6.287359330018319e-06,
      "loss": 0.4319,
      "step": 1820
    },
    {
      "epoch": 0.04766433420001047,
      "grad_norm": 12.669456481933594,
      "learning_rate": 6.284218790892436e-06,
      "loss": 0.3721,
      "step": 1821
    },
    {
      "epoch": 0.04769050901286056,
      "grad_norm": 12.681093215942383,
      "learning_rate": 6.281078251766553e-06,
      "loss": 0.3959,
      "step": 1822
    },
    {
      "epoch": 0.04771668382571065,
      "grad_norm": 19.55695152282715,
      "learning_rate": 6.2779377126406695e-06,
      "loss": 0.5156,
      "step": 1823
    },
    {
      "epoch": 0.04774285863856073,
      "grad_norm": 14.966981887817383,
      "learning_rate": 6.274797173514786e-06,
      "loss": 0.4215,
      "step": 1824
    },
    {
      "epoch": 0.04776903345141082,
      "grad_norm": 14.616561889648438,
      "learning_rate": 6.271656634388904e-06,
      "loss": 0.3552,
      "step": 1825
    },
    {
      "epoch": 0.04779520826426091,
      "grad_norm": 18.904003143310547,
      "learning_rate": 6.2685160952630205e-06,
      "loss": 0.9028,
      "step": 1826
    },
    {
      "epoch": 0.047821383077111,
      "grad_norm": 25.196205139160156,
      "learning_rate": 6.265375556137137e-06,
      "loss": 0.6635,
      "step": 1827
    },
    {
      "epoch": 0.04784755788996109,
      "grad_norm": 26.524694442749023,
      "learning_rate": 6.262235017011254e-06,
      "loss": 0.6916,
      "step": 1828
    },
    {
      "epoch": 0.04787373270281117,
      "grad_norm": 16.92926025390625,
      "learning_rate": 6.259094477885371e-06,
      "loss": 0.4794,
      "step": 1829
    },
    {
      "epoch": 0.04789990751566126,
      "grad_norm": 20.66340446472168,
      "learning_rate": 6.255953938759487e-06,
      "loss": 0.6742,
      "step": 1830
    },
    {
      "epoch": 0.04792608232851135,
      "grad_norm": 16.326017379760742,
      "learning_rate": 6.252813399633604e-06,
      "loss": 0.3529,
      "step": 1831
    },
    {
      "epoch": 0.04795225714136144,
      "grad_norm": 21.74686050415039,
      "learning_rate": 6.249672860507721e-06,
      "loss": 0.4755,
      "step": 1832
    },
    {
      "epoch": 0.047978431954211524,
      "grad_norm": 20.26451301574707,
      "learning_rate": 6.246532321381838e-06,
      "loss": 0.4113,
      "step": 1833
    },
    {
      "epoch": 0.048004606767061614,
      "grad_norm": 19.881282806396484,
      "learning_rate": 6.243391782255955e-06,
      "loss": 0.4955,
      "step": 1834
    },
    {
      "epoch": 0.048030781579911704,
      "grad_norm": 29.852529525756836,
      "learning_rate": 6.24025124313007e-06,
      "loss": 0.3732,
      "step": 1835
    },
    {
      "epoch": 0.04805695639276179,
      "grad_norm": 34.76167678833008,
      "learning_rate": 6.237110704004187e-06,
      "loss": 0.8395,
      "step": 1836
    },
    {
      "epoch": 0.04808313120561188,
      "grad_norm": 30.08296012878418,
      "learning_rate": 6.233970164878304e-06,
      "loss": 0.5047,
      "step": 1837
    },
    {
      "epoch": 0.048109306018461966,
      "grad_norm": 23.906007766723633,
      "learning_rate": 6.230829625752421e-06,
      "loss": 0.5033,
      "step": 1838
    },
    {
      "epoch": 0.048135480831312055,
      "grad_norm": 15.087021827697754,
      "learning_rate": 6.227689086626538e-06,
      "loss": 0.3652,
      "step": 1839
    },
    {
      "epoch": 0.048161655644162145,
      "grad_norm": 19.94037437438965,
      "learning_rate": 6.224548547500654e-06,
      "loss": 0.6708,
      "step": 1840
    },
    {
      "epoch": 0.048187830457012235,
      "grad_norm": 14.821314811706543,
      "learning_rate": 6.221408008374771e-06,
      "loss": 0.3206,
      "step": 1841
    },
    {
      "epoch": 0.04821400526986232,
      "grad_norm": 13.727360725402832,
      "learning_rate": 6.218267469248888e-06,
      "loss": 0.3515,
      "step": 1842
    },
    {
      "epoch": 0.04824018008271241,
      "grad_norm": 19.614986419677734,
      "learning_rate": 6.2151269301230045e-06,
      "loss": 0.6392,
      "step": 1843
    },
    {
      "epoch": 0.0482663548955625,
      "grad_norm": 12.120353698730469,
      "learning_rate": 6.211986390997121e-06,
      "loss": 0.1543,
      "step": 1844
    },
    {
      "epoch": 0.048292529708412586,
      "grad_norm": 26.940898895263672,
      "learning_rate": 6.208845851871238e-06,
      "loss": 0.4013,
      "step": 1845
    },
    {
      "epoch": 0.048318704521262676,
      "grad_norm": 18.462343215942383,
      "learning_rate": 6.2057053127453555e-06,
      "loss": 0.3417,
      "step": 1846
    },
    {
      "epoch": 0.04834487933411276,
      "grad_norm": 23.768470764160156,
      "learning_rate": 6.202564773619472e-06,
      "loss": 0.59,
      "step": 1847
    },
    {
      "epoch": 0.04837105414696285,
      "grad_norm": 16.164400100708008,
      "learning_rate": 6.199424234493589e-06,
      "loss": 0.3434,
      "step": 1848
    },
    {
      "epoch": 0.04839722895981294,
      "grad_norm": 13.516417503356934,
      "learning_rate": 6.196283695367706e-06,
      "loss": 0.2981,
      "step": 1849
    },
    {
      "epoch": 0.04842340377266303,
      "grad_norm": 12.776290893554688,
      "learning_rate": 6.1931431562418214e-06,
      "loss": 0.367,
      "step": 1850
    },
    {
      "epoch": 0.04844957858551311,
      "grad_norm": 21.78403091430664,
      "learning_rate": 6.190002617115938e-06,
      "loss": 0.4291,
      "step": 1851
    },
    {
      "epoch": 0.0484757533983632,
      "grad_norm": 13.928585052490234,
      "learning_rate": 6.186862077990055e-06,
      "loss": 0.5425,
      "step": 1852
    },
    {
      "epoch": 0.04850192821121329,
      "grad_norm": 29.622968673706055,
      "learning_rate": 6.1837215388641716e-06,
      "loss": 0.4357,
      "step": 1853
    },
    {
      "epoch": 0.04852810302406338,
      "grad_norm": 28.131620407104492,
      "learning_rate": 6.180580999738288e-06,
      "loss": 0.5304,
      "step": 1854
    },
    {
      "epoch": 0.04855427783691347,
      "grad_norm": 14.910948753356934,
      "learning_rate": 6.177440460612405e-06,
      "loss": 0.3983,
      "step": 1855
    },
    {
      "epoch": 0.04858045264976355,
      "grad_norm": 25.571208953857422,
      "learning_rate": 6.174299921486522e-06,
      "loss": 0.5901,
      "step": 1856
    },
    {
      "epoch": 0.04860662746261364,
      "grad_norm": 23.14423370361328,
      "learning_rate": 6.171159382360638e-06,
      "loss": 0.7142,
      "step": 1857
    },
    {
      "epoch": 0.04863280227546373,
      "grad_norm": 18.91185188293457,
      "learning_rate": 6.168018843234755e-06,
      "loss": 0.569,
      "step": 1858
    },
    {
      "epoch": 0.04865897708831382,
      "grad_norm": 33.62759017944336,
      "learning_rate": 6.164878304108873e-06,
      "loss": 0.5257,
      "step": 1859
    },
    {
      "epoch": 0.0486851519011639,
      "grad_norm": 22.139812469482422,
      "learning_rate": 6.161737764982989e-06,
      "loss": 0.2892,
      "step": 1860
    },
    {
      "epoch": 0.04871132671401399,
      "grad_norm": 11.809596061706543,
      "learning_rate": 6.158597225857106e-06,
      "loss": 0.3049,
      "step": 1861
    },
    {
      "epoch": 0.04873750152686408,
      "grad_norm": 22.68961524963379,
      "learning_rate": 6.155456686731223e-06,
      "loss": 0.3946,
      "step": 1862
    },
    {
      "epoch": 0.04876367633971417,
      "grad_norm": 14.15758991241455,
      "learning_rate": 6.1523161476053395e-06,
      "loss": 0.419,
      "step": 1863
    },
    {
      "epoch": 0.04878985115256426,
      "grad_norm": 21.744443893432617,
      "learning_rate": 6.149175608479456e-06,
      "loss": 0.7646,
      "step": 1864
    },
    {
      "epoch": 0.048816025965414345,
      "grad_norm": 15.109386444091797,
      "learning_rate": 6.146035069353573e-06,
      "loss": 0.5512,
      "step": 1865
    },
    {
      "epoch": 0.048842200778264434,
      "grad_norm": 18.736827850341797,
      "learning_rate": 6.142894530227689e-06,
      "loss": 0.5042,
      "step": 1866
    },
    {
      "epoch": 0.048868375591114524,
      "grad_norm": 28.879104614257812,
      "learning_rate": 6.1397539911018055e-06,
      "loss": 0.3734,
      "step": 1867
    },
    {
      "epoch": 0.048894550403964614,
      "grad_norm": 24.503995895385742,
      "learning_rate": 6.136613451975922e-06,
      "loss": 0.4926,
      "step": 1868
    },
    {
      "epoch": 0.048920725216814696,
      "grad_norm": 18.27947998046875,
      "learning_rate": 6.133472912850039e-06,
      "loss": 0.3308,
      "step": 1869
    },
    {
      "epoch": 0.048946900029664786,
      "grad_norm": 20.58304786682129,
      "learning_rate": 6.1303323737241556e-06,
      "loss": 0.569,
      "step": 1870
    },
    {
      "epoch": 0.048973074842514876,
      "grad_norm": 16.689777374267578,
      "learning_rate": 6.127191834598272e-06,
      "loss": 0.2242,
      "step": 1871
    },
    {
      "epoch": 0.048999249655364965,
      "grad_norm": 15.344295501708984,
      "learning_rate": 6.12405129547239e-06,
      "loss": 0.2179,
      "step": 1872
    },
    {
      "epoch": 0.049025424468215055,
      "grad_norm": 10.384450912475586,
      "learning_rate": 6.1209107563465065e-06,
      "loss": 0.3149,
      "step": 1873
    },
    {
      "epoch": 0.04905159928106514,
      "grad_norm": 23.300443649291992,
      "learning_rate": 6.117770217220623e-06,
      "loss": 0.4681,
      "step": 1874
    },
    {
      "epoch": 0.04907777409391523,
      "grad_norm": 21.16755485534668,
      "learning_rate": 6.11462967809474e-06,
      "loss": 0.4859,
      "step": 1875
    },
    {
      "epoch": 0.04910394890676532,
      "grad_norm": 28.940296173095703,
      "learning_rate": 6.111489138968857e-06,
      "loss": 0.4066,
      "step": 1876
    },
    {
      "epoch": 0.04913012371961541,
      "grad_norm": 28.1897029876709,
      "learning_rate": 6.108348599842973e-06,
      "loss": 1.0701,
      "step": 1877
    },
    {
      "epoch": 0.04915629853246549,
      "grad_norm": 25.434425354003906,
      "learning_rate": 6.10520806071709e-06,
      "loss": 0.5103,
      "step": 1878
    },
    {
      "epoch": 0.04918247334531558,
      "grad_norm": 26.27858543395996,
      "learning_rate": 6.102067521591207e-06,
      "loss": 0.3271,
      "step": 1879
    },
    {
      "epoch": 0.04920864815816567,
      "grad_norm": 26.23378562927246,
      "learning_rate": 6.098926982465324e-06,
      "loss": 0.6632,
      "step": 1880
    },
    {
      "epoch": 0.04923482297101576,
      "grad_norm": 22.501707077026367,
      "learning_rate": 6.095786443339439e-06,
      "loss": 0.4924,
      "step": 1881
    },
    {
      "epoch": 0.04926099778386585,
      "grad_norm": 18.798492431640625,
      "learning_rate": 6.092645904213556e-06,
      "loss": 0.3107,
      "step": 1882
    },
    {
      "epoch": 0.04928717259671593,
      "grad_norm": 31.845190048217773,
      "learning_rate": 6.089505365087673e-06,
      "loss": 0.5814,
      "step": 1883
    },
    {
      "epoch": 0.04931334740956602,
      "grad_norm": 32.05630111694336,
      "learning_rate": 6.08636482596179e-06,
      "loss": 0.5524,
      "step": 1884
    },
    {
      "epoch": 0.04933952222241611,
      "grad_norm": 27.330278396606445,
      "learning_rate": 6.083224286835907e-06,
      "loss": 0.8292,
      "step": 1885
    },
    {
      "epoch": 0.0493656970352662,
      "grad_norm": 13.933496475219727,
      "learning_rate": 6.080083747710024e-06,
      "loss": 0.3806,
      "step": 1886
    },
    {
      "epoch": 0.04939187184811629,
      "grad_norm": 15.35069465637207,
      "learning_rate": 6.0769432085841404e-06,
      "loss": 0.3576,
      "step": 1887
    },
    {
      "epoch": 0.04941804666096637,
      "grad_norm": 22.96913719177246,
      "learning_rate": 6.073802669458257e-06,
      "loss": 0.4427,
      "step": 1888
    },
    {
      "epoch": 0.04944422147381646,
      "grad_norm": 18.454082489013672,
      "learning_rate": 6.070662130332374e-06,
      "loss": 0.3521,
      "step": 1889
    },
    {
      "epoch": 0.04947039628666655,
      "grad_norm": 27.389324188232422,
      "learning_rate": 6.0675215912064905e-06,
      "loss": 0.574,
      "step": 1890
    },
    {
      "epoch": 0.04949657109951664,
      "grad_norm": 26.634817123413086,
      "learning_rate": 6.064381052080607e-06,
      "loss": 0.4697,
      "step": 1891
    },
    {
      "epoch": 0.049522745912366724,
      "grad_norm": 30.7195987701416,
      "learning_rate": 6.061240512954724e-06,
      "loss": 0.3436,
      "step": 1892
    },
    {
      "epoch": 0.04954892072521681,
      "grad_norm": 15.968905448913574,
      "learning_rate": 6.0580999738288415e-06,
      "loss": 0.3729,
      "step": 1893
    },
    {
      "epoch": 0.0495750955380669,
      "grad_norm": 27.332460403442383,
      "learning_rate": 6.054959434702958e-06,
      "loss": 0.4127,
      "step": 1894
    },
    {
      "epoch": 0.04960127035091699,
      "grad_norm": 15.229080200195312,
      "learning_rate": 6.051818895577075e-06,
      "loss": 0.3811,
      "step": 1895
    },
    {
      "epoch": 0.04962744516376708,
      "grad_norm": 22.047008514404297,
      "learning_rate": 6.048678356451192e-06,
      "loss": 0.6427,
      "step": 1896
    },
    {
      "epoch": 0.049653619976617165,
      "grad_norm": 19.317222595214844,
      "learning_rate": 6.0455378173253075e-06,
      "loss": 0.4339,
      "step": 1897
    },
    {
      "epoch": 0.049679794789467255,
      "grad_norm": 21.702919006347656,
      "learning_rate": 6.042397278199424e-06,
      "loss": 0.5047,
      "step": 1898
    },
    {
      "epoch": 0.049705969602317344,
      "grad_norm": 34.45197296142578,
      "learning_rate": 6.039256739073541e-06,
      "loss": 0.4785,
      "step": 1899
    },
    {
      "epoch": 0.049732144415167434,
      "grad_norm": 23.90642547607422,
      "learning_rate": 6.036116199947658e-06,
      "loss": 0.4303,
      "step": 1900
    },
    {
      "epoch": 0.04975831922801752,
      "grad_norm": 23.00672721862793,
      "learning_rate": 6.032975660821774e-06,
      "loss": 0.5141,
      "step": 1901
    },
    {
      "epoch": 0.049784494040867606,
      "grad_norm": 24.9351863861084,
      "learning_rate": 6.029835121695891e-06,
      "loss": 0.5818,
      "step": 1902
    },
    {
      "epoch": 0.049810668853717696,
      "grad_norm": 16.299230575561523,
      "learning_rate": 6.026694582570008e-06,
      "loss": 0.3285,
      "step": 1903
    },
    {
      "epoch": 0.049836843666567786,
      "grad_norm": 20.02703285217285,
      "learning_rate": 6.0235540434441244e-06,
      "loss": 0.4796,
      "step": 1904
    },
    {
      "epoch": 0.049863018479417875,
      "grad_norm": 18.203857421875,
      "learning_rate": 6.020413504318241e-06,
      "loss": 0.4411,
      "step": 1905
    },
    {
      "epoch": 0.04988919329226796,
      "grad_norm": 25.76224708557129,
      "learning_rate": 6.017272965192359e-06,
      "loss": 0.5601,
      "step": 1906
    },
    {
      "epoch": 0.04991536810511805,
      "grad_norm": 23.672866821289062,
      "learning_rate": 6.014132426066475e-06,
      "loss": 0.2822,
      "step": 1907
    },
    {
      "epoch": 0.04994154291796814,
      "grad_norm": 13.974005699157715,
      "learning_rate": 6.010991886940592e-06,
      "loss": 0.3659,
      "step": 1908
    },
    {
      "epoch": 0.04996771773081823,
      "grad_norm": 15.881807327270508,
      "learning_rate": 6.007851347814709e-06,
      "loss": 0.291,
      "step": 1909
    },
    {
      "epoch": 0.04999389254366831,
      "grad_norm": 24.326814651489258,
      "learning_rate": 6.0047108086888255e-06,
      "loss": 0.5805,
      "step": 1910
    },
    {
      "epoch": 0.0500200673565184,
      "grad_norm": 10.757325172424316,
      "learning_rate": 6.001570269562942e-06,
      "loss": 0.2531,
      "step": 1911
    },
    {
      "epoch": 0.05004624216936849,
      "grad_norm": 25.570661544799805,
      "learning_rate": 5.998429730437059e-06,
      "loss": 0.4863,
      "step": 1912
    },
    {
      "epoch": 0.05007241698221858,
      "grad_norm": 26.41615867614746,
      "learning_rate": 5.995289191311176e-06,
      "loss": 0.5846,
      "step": 1913
    },
    {
      "epoch": 0.05009859179506867,
      "grad_norm": 20.862131118774414,
      "learning_rate": 5.992148652185292e-06,
      "loss": 0.5912,
      "step": 1914
    },
    {
      "epoch": 0.05012476660791875,
      "grad_norm": 23.713539123535156,
      "learning_rate": 5.989008113059409e-06,
      "loss": 0.4479,
      "step": 1915
    },
    {
      "epoch": 0.05015094142076884,
      "grad_norm": 13.150395393371582,
      "learning_rate": 5.985867573933525e-06,
      "loss": 0.3808,
      "step": 1916
    },
    {
      "epoch": 0.05017711623361893,
      "grad_norm": 10.910043716430664,
      "learning_rate": 5.982727034807642e-06,
      "loss": 0.2915,
      "step": 1917
    },
    {
      "epoch": 0.05020329104646902,
      "grad_norm": 27.741195678710938,
      "learning_rate": 5.979586495681759e-06,
      "loss": 0.6732,
      "step": 1918
    },
    {
      "epoch": 0.0502294658593191,
      "grad_norm": 21.201013565063477,
      "learning_rate": 5.976445956555876e-06,
      "loss": 0.3645,
      "step": 1919
    },
    {
      "epoch": 0.05025564067216919,
      "grad_norm": 12.218515396118164,
      "learning_rate": 5.973305417429993e-06,
      "loss": 0.1996,
      "step": 1920
    },
    {
      "epoch": 0.05028181548501928,
      "grad_norm": 17.504098892211914,
      "learning_rate": 5.970164878304109e-06,
      "loss": 0.3502,
      "step": 1921
    },
    {
      "epoch": 0.05030799029786937,
      "grad_norm": 14.447199821472168,
      "learning_rate": 5.967024339178226e-06,
      "loss": 0.2935,
      "step": 1922
    },
    {
      "epoch": 0.05033416511071946,
      "grad_norm": 22.652414321899414,
      "learning_rate": 5.963883800052343e-06,
      "loss": 0.3938,
      "step": 1923
    },
    {
      "epoch": 0.050360339923569544,
      "grad_norm": 9.935552597045898,
      "learning_rate": 5.9607432609264586e-06,
      "loss": 0.1546,
      "step": 1924
    },
    {
      "epoch": 0.050386514736419634,
      "grad_norm": 29.66168785095215,
      "learning_rate": 5.957602721800576e-06,
      "loss": 0.603,
      "step": 1925
    },
    {
      "epoch": 0.050412689549269724,
      "grad_norm": 28.338693618774414,
      "learning_rate": 5.954462182674693e-06,
      "loss": 0.6258,
      "step": 1926
    },
    {
      "epoch": 0.05043886436211981,
      "grad_norm": 22.297893524169922,
      "learning_rate": 5.9513216435488095e-06,
      "loss": 0.4279,
      "step": 1927
    },
    {
      "epoch": 0.050465039174969896,
      "grad_norm": 18.809865951538086,
      "learning_rate": 5.948181104422926e-06,
      "loss": 0.4104,
      "step": 1928
    },
    {
      "epoch": 0.050491213987819986,
      "grad_norm": 17.23413848876953,
      "learning_rate": 5.945040565297043e-06,
      "loss": 0.217,
      "step": 1929
    },
    {
      "epoch": 0.050517388800670075,
      "grad_norm": 27.4323673248291,
      "learning_rate": 5.94190002617116e-06,
      "loss": 0.6866,
      "step": 1930
    },
    {
      "epoch": 0.050543563613520165,
      "grad_norm": 29.229013442993164,
      "learning_rate": 5.938759487045276e-06,
      "loss": 0.5855,
      "step": 1931
    },
    {
      "epoch": 0.050569738426370255,
      "grad_norm": 25.138225555419922,
      "learning_rate": 5.935618947919393e-06,
      "loss": 0.4445,
      "step": 1932
    },
    {
      "epoch": 0.05059591323922034,
      "grad_norm": 13.029709815979004,
      "learning_rate": 5.93247840879351e-06,
      "loss": 0.1531,
      "step": 1933
    },
    {
      "epoch": 0.05062208805207043,
      "grad_norm": 26.164079666137695,
      "learning_rate": 5.9293378696676265e-06,
      "loss": 0.3175,
      "step": 1934
    },
    {
      "epoch": 0.05064826286492052,
      "grad_norm": 19.733869552612305,
      "learning_rate": 5.926197330541743e-06,
      "loss": 0.4045,
      "step": 1935
    },
    {
      "epoch": 0.050674437677770606,
      "grad_norm": 17.62079429626465,
      "learning_rate": 5.92305679141586e-06,
      "loss": 0.3988,
      "step": 1936
    },
    {
      "epoch": 0.05070061249062069,
      "grad_norm": 19.016889572143555,
      "learning_rate": 5.919916252289977e-06,
      "loss": 0.2164,
      "step": 1937
    },
    {
      "epoch": 0.05072678730347078,
      "grad_norm": 27.33681297302246,
      "learning_rate": 5.916775713164093e-06,
      "loss": 0.4471,
      "step": 1938
    },
    {
      "epoch": 0.05075296211632087,
      "grad_norm": 17.596036911010742,
      "learning_rate": 5.91363517403821e-06,
      "loss": 0.359,
      "step": 1939
    },
    {
      "epoch": 0.05077913692917096,
      "grad_norm": 26.58290672302246,
      "learning_rate": 5.910494634912327e-06,
      "loss": 0.3785,
      "step": 1940
    },
    {
      "epoch": 0.05080531174202105,
      "grad_norm": 26.54778289794922,
      "learning_rate": 5.907354095786443e-06,
      "loss": 0.485,
      "step": 1941
    },
    {
      "epoch": 0.05083148655487113,
      "grad_norm": 9.275626182556152,
      "learning_rate": 5.90421355666056e-06,
      "loss": 0.1416,
      "step": 1942
    },
    {
      "epoch": 0.05085766136772122,
      "grad_norm": 22.555585861206055,
      "learning_rate": 5.901073017534677e-06,
      "loss": 0.4304,
      "step": 1943
    },
    {
      "epoch": 0.05088383618057131,
      "grad_norm": 13.906248092651367,
      "learning_rate": 5.8979324784087935e-06,
      "loss": 0.3579,
      "step": 1944
    },
    {
      "epoch": 0.0509100109934214,
      "grad_norm": 12.611931800842285,
      "learning_rate": 5.89479193928291e-06,
      "loss": 0.3313,
      "step": 1945
    },
    {
      "epoch": 0.05093618580627148,
      "grad_norm": 12.772148132324219,
      "learning_rate": 5.891651400157028e-06,
      "loss": 0.1964,
      "step": 1946
    },
    {
      "epoch": 0.05096236061912157,
      "grad_norm": 15.006802558898926,
      "learning_rate": 5.888510861031144e-06,
      "loss": 0.2782,
      "step": 1947
    },
    {
      "epoch": 0.05098853543197166,
      "grad_norm": 26.295921325683594,
      "learning_rate": 5.88537032190526e-06,
      "loss": 0.4836,
      "step": 1948
    },
    {
      "epoch": 0.05101471024482175,
      "grad_norm": 21.144651412963867,
      "learning_rate": 5.882229782779377e-06,
      "loss": 0.4152,
      "step": 1949
    },
    {
      "epoch": 0.05104088505767184,
      "grad_norm": 13.739953994750977,
      "learning_rate": 5.879089243653494e-06,
      "loss": 0.1639,
      "step": 1950
    },
    {
      "epoch": 0.05106705987052192,
      "grad_norm": 12.706110000610352,
      "learning_rate": 5.8759487045276105e-06,
      "loss": 0.3864,
      "step": 1951
    },
    {
      "epoch": 0.05109323468337201,
      "grad_norm": 14.207633018493652,
      "learning_rate": 5.872808165401728e-06,
      "loss": 0.2944,
      "step": 1952
    },
    {
      "epoch": 0.0511194094962221,
      "grad_norm": 24.581987380981445,
      "learning_rate": 5.869667626275845e-06,
      "loss": 0.3968,
      "step": 1953
    },
    {
      "epoch": 0.05114558430907219,
      "grad_norm": 15.251934051513672,
      "learning_rate": 5.8665270871499614e-06,
      "loss": 0.2981,
      "step": 1954
    },
    {
      "epoch": 0.051171759121922275,
      "grad_norm": 21.184207916259766,
      "learning_rate": 5.863386548024077e-06,
      "loss": 0.5645,
      "step": 1955
    },
    {
      "epoch": 0.051197933934772365,
      "grad_norm": 25.259746551513672,
      "learning_rate": 5.860246008898194e-06,
      "loss": 0.6726,
      "step": 1956
    },
    {
      "epoch": 0.051224108747622454,
      "grad_norm": 19.568742752075195,
      "learning_rate": 5.857105469772311e-06,
      "loss": 0.6342,
      "step": 1957
    },
    {
      "epoch": 0.051250283560472544,
      "grad_norm": 17.600393295288086,
      "learning_rate": 5.853964930646427e-06,
      "loss": 0.3374,
      "step": 1958
    },
    {
      "epoch": 0.051276458373322634,
      "grad_norm": 23.3824520111084,
      "learning_rate": 5.850824391520545e-06,
      "loss": 0.6964,
      "step": 1959
    },
    {
      "epoch": 0.051302633186172716,
      "grad_norm": 32.27641677856445,
      "learning_rate": 5.847683852394662e-06,
      "loss": 0.3664,
      "step": 1960
    },
    {
      "epoch": 0.051328807999022806,
      "grad_norm": 21.127262115478516,
      "learning_rate": 5.844543313268778e-06,
      "loss": 0.5448,
      "step": 1961
    },
    {
      "epoch": 0.051354982811872896,
      "grad_norm": 14.544451713562012,
      "learning_rate": 5.841402774142894e-06,
      "loss": 0.1801,
      "step": 1962
    },
    {
      "epoch": 0.051381157624722985,
      "grad_norm": 23.095849990844727,
      "learning_rate": 5.838262235017011e-06,
      "loss": 0.3766,
      "step": 1963
    },
    {
      "epoch": 0.05140733243757307,
      "grad_norm": 17.233413696289062,
      "learning_rate": 5.835121695891128e-06,
      "loss": 0.32,
      "step": 1964
    },
    {
      "epoch": 0.05143350725042316,
      "grad_norm": 20.5921688079834,
      "learning_rate": 5.831981156765245e-06,
      "loss": 0.26,
      "step": 1965
    },
    {
      "epoch": 0.05145968206327325,
      "grad_norm": 18.353200912475586,
      "learning_rate": 5.828840617639362e-06,
      "loss": 0.3616,
      "step": 1966
    },
    {
      "epoch": 0.05148585687612334,
      "grad_norm": 16.804946899414062,
      "learning_rate": 5.825700078513479e-06,
      "loss": 0.5098,
      "step": 1967
    },
    {
      "epoch": 0.05151203168897343,
      "grad_norm": 22.988969802856445,
      "learning_rate": 5.822559539387595e-06,
      "loss": 0.6255,
      "step": 1968
    },
    {
      "epoch": 0.05153820650182351,
      "grad_norm": 26.147871017456055,
      "learning_rate": 5.819419000261712e-06,
      "loss": 0.6722,
      "step": 1969
    },
    {
      "epoch": 0.0515643813146736,
      "grad_norm": 16.867206573486328,
      "learning_rate": 5.816278461135828e-06,
      "loss": 0.4968,
      "step": 1970
    },
    {
      "epoch": 0.05159055612752369,
      "grad_norm": 14.877659797668457,
      "learning_rate": 5.813137922009945e-06,
      "loss": 0.2998,
      "step": 1971
    },
    {
      "epoch": 0.05161673094037378,
      "grad_norm": 17.209110260009766,
      "learning_rate": 5.809997382884062e-06,
      "loss": 0.4023,
      "step": 1972
    },
    {
      "epoch": 0.05164290575322386,
      "grad_norm": 30.67165756225586,
      "learning_rate": 5.806856843758179e-06,
      "loss": 0.6603,
      "step": 1973
    },
    {
      "epoch": 0.05166908056607395,
      "grad_norm": 15.235483169555664,
      "learning_rate": 5.8037163046322956e-06,
      "loss": 0.3198,
      "step": 1974
    },
    {
      "epoch": 0.05169525537892404,
      "grad_norm": 17.70393180847168,
      "learning_rate": 5.800575765506412e-06,
      "loss": 0.2966,
      "step": 1975
    },
    {
      "epoch": 0.05172143019177413,
      "grad_norm": 28.220251083374023,
      "learning_rate": 5.797435226380529e-06,
      "loss": 0.5768,
      "step": 1976
    },
    {
      "epoch": 0.05174760500462422,
      "grad_norm": 39.63892364501953,
      "learning_rate": 5.794294687254646e-06,
      "loss": 0.5412,
      "step": 1977
    },
    {
      "epoch": 0.0517737798174743,
      "grad_norm": 13.794517517089844,
      "learning_rate": 5.791154148128762e-06,
      "loss": 0.3852,
      "step": 1978
    },
    {
      "epoch": 0.05179995463032439,
      "grad_norm": 21.95945167541504,
      "learning_rate": 5.788013609002879e-06,
      "loss": 0.5676,
      "step": 1979
    },
    {
      "epoch": 0.05182612944317448,
      "grad_norm": 18.91503143310547,
      "learning_rate": 5.784873069876996e-06,
      "loss": 0.5024,
      "step": 1980
    },
    {
      "epoch": 0.05185230425602457,
      "grad_norm": 21.386022567749023,
      "learning_rate": 5.7817325307511125e-06,
      "loss": 0.3549,
      "step": 1981
    },
    {
      "epoch": 0.051878479068874654,
      "grad_norm": 23.307342529296875,
      "learning_rate": 5.778591991625229e-06,
      "loss": 0.4522,
      "step": 1982
    },
    {
      "epoch": 0.051904653881724744,
      "grad_norm": 16.208959579467773,
      "learning_rate": 5.775451452499346e-06,
      "loss": 0.3125,
      "step": 1983
    },
    {
      "epoch": 0.051930828694574833,
      "grad_norm": 18.888792037963867,
      "learning_rate": 5.772310913373463e-06,
      "loss": 0.4348,
      "step": 1984
    },
    {
      "epoch": 0.05195700350742492,
      "grad_norm": 18.631681442260742,
      "learning_rate": 5.769170374247579e-06,
      "loss": 0.2062,
      "step": 1985
    },
    {
      "epoch": 0.05198317832027501,
      "grad_norm": 21.393291473388672,
      "learning_rate": 5.766029835121696e-06,
      "loss": 0.4369,
      "step": 1986
    },
    {
      "epoch": 0.052009353133125096,
      "grad_norm": 24.282522201538086,
      "learning_rate": 5.762889295995813e-06,
      "loss": 0.4095,
      "step": 1987
    },
    {
      "epoch": 0.052035527945975185,
      "grad_norm": 22.369741439819336,
      "learning_rate": 5.7597487568699294e-06,
      "loss": 0.2701,
      "step": 1988
    },
    {
      "epoch": 0.052061702758825275,
      "grad_norm": 24.611772537231445,
      "learning_rate": 5.756608217744046e-06,
      "loss": 0.4402,
      "step": 1989
    },
    {
      "epoch": 0.052087877571675364,
      "grad_norm": 21.7196102142334,
      "learning_rate": 5.753467678618163e-06,
      "loss": 0.2831,
      "step": 1990
    },
    {
      "epoch": 0.05211405238452545,
      "grad_norm": 22.496620178222656,
      "learning_rate": 5.7503271394922796e-06,
      "loss": 0.5706,
      "step": 1991
    },
    {
      "epoch": 0.05214022719737554,
      "grad_norm": 17.6638240814209,
      "learning_rate": 5.747186600366396e-06,
      "loss": 0.7077,
      "step": 1992
    },
    {
      "epoch": 0.052166402010225627,
      "grad_norm": 19.79182243347168,
      "learning_rate": 5.744046061240513e-06,
      "loss": 0.3583,
      "step": 1993
    },
    {
      "epoch": 0.052192576823075716,
      "grad_norm": 17.038799285888672,
      "learning_rate": 5.74090552211463e-06,
      "loss": 0.2913,
      "step": 1994
    },
    {
      "epoch": 0.052218751635925806,
      "grad_norm": 9.806241035461426,
      "learning_rate": 5.737764982988746e-06,
      "loss": 0.1264,
      "step": 1995
    },
    {
      "epoch": 0.05224492644877589,
      "grad_norm": 25.00699806213379,
      "learning_rate": 5.734624443862863e-06,
      "loss": 0.3353,
      "step": 1996
    },
    {
      "epoch": 0.05227110126162598,
      "grad_norm": 29.918323516845703,
      "learning_rate": 5.73148390473698e-06,
      "loss": 0.5411,
      "step": 1997
    },
    {
      "epoch": 0.05229727607447607,
      "grad_norm": 21.09404182434082,
      "learning_rate": 5.7283433656110965e-06,
      "loss": 0.4182,
      "step": 1998
    },
    {
      "epoch": 0.05232345088732616,
      "grad_norm": 40.086570739746094,
      "learning_rate": 5.725202826485214e-06,
      "loss": 0.6157,
      "step": 1999
    },
    {
      "epoch": 0.05234962570017625,
      "grad_norm": 15.975175857543945,
      "learning_rate": 5.722062287359331e-06,
      "loss": 0.3581,
      "step": 2000
    },
    {
      "epoch": 0.05237580051302633,
      "grad_norm": 18.967275619506836,
      "learning_rate": 5.718921748233447e-06,
      "loss": 0.4609,
      "step": 2001
    },
    {
      "epoch": 0.05240197532587642,
      "grad_norm": 22.584800720214844,
      "learning_rate": 5.715781209107563e-06,
      "loss": 0.3666,
      "step": 2002
    },
    {
      "epoch": 0.05242815013872651,
      "grad_norm": 17.58822250366211,
      "learning_rate": 5.71264066998168e-06,
      "loss": 0.2965,
      "step": 2003
    },
    {
      "epoch": 0.0524543249515766,
      "grad_norm": 22.043916702270508,
      "learning_rate": 5.709500130855797e-06,
      "loss": 0.4817,
      "step": 2004
    },
    {
      "epoch": 0.05248049976442668,
      "grad_norm": 17.575075149536133,
      "learning_rate": 5.7063595917299135e-06,
      "loss": 0.1584,
      "step": 2005
    },
    {
      "epoch": 0.05250667457727677,
      "grad_norm": 19.279226303100586,
      "learning_rate": 5.703219052604031e-06,
      "loss": 0.4597,
      "step": 2006
    },
    {
      "epoch": 0.05253284939012686,
      "grad_norm": 25.127849578857422,
      "learning_rate": 5.700078513478148e-06,
      "loss": 0.4779,
      "step": 2007
    },
    {
      "epoch": 0.05255902420297695,
      "grad_norm": 23.462642669677734,
      "learning_rate": 5.696937974352264e-06,
      "loss": 0.6481,
      "step": 2008
    },
    {
      "epoch": 0.05258519901582704,
      "grad_norm": 21.95708656311035,
      "learning_rate": 5.69379743522638e-06,
      "loss": 0.4535,
      "step": 2009
    },
    {
      "epoch": 0.05261137382867712,
      "grad_norm": 13.311134338378906,
      "learning_rate": 5.690656896100497e-06,
      "loss": 0.2568,
      "step": 2010
    },
    {
      "epoch": 0.05263754864152721,
      "grad_norm": 21.20689582824707,
      "learning_rate": 5.687516356974614e-06,
      "loss": 0.3486,
      "step": 2011
    },
    {
      "epoch": 0.0526637234543773,
      "grad_norm": 30.32314109802246,
      "learning_rate": 5.684375817848731e-06,
      "loss": 0.5325,
      "step": 2012
    },
    {
      "epoch": 0.05268989826722739,
      "grad_norm": 25.06962776184082,
      "learning_rate": 5.681235278722848e-06,
      "loss": 0.7684,
      "step": 2013
    },
    {
      "epoch": 0.052716073080077475,
      "grad_norm": 23.698575973510742,
      "learning_rate": 5.678094739596965e-06,
      "loss": 0.47,
      "step": 2014
    },
    {
      "epoch": 0.052742247892927564,
      "grad_norm": 12.528018951416016,
      "learning_rate": 5.674954200471081e-06,
      "loss": 0.2104,
      "step": 2015
    },
    {
      "epoch": 0.052768422705777654,
      "grad_norm": 29.303152084350586,
      "learning_rate": 5.671813661345197e-06,
      "loss": 0.4872,
      "step": 2016
    },
    {
      "epoch": 0.052794597518627744,
      "grad_norm": 17.574420928955078,
      "learning_rate": 5.668673122219314e-06,
      "loss": 0.2855,
      "step": 2017
    },
    {
      "epoch": 0.05282077233147783,
      "grad_norm": 17.65651512145996,
      "learning_rate": 5.665532583093431e-06,
      "loss": 0.3642,
      "step": 2018
    },
    {
      "epoch": 0.052846947144327916,
      "grad_norm": 16.863204956054688,
      "learning_rate": 5.662392043967548e-06,
      "loss": 0.3324,
      "step": 2019
    },
    {
      "epoch": 0.052873121957178006,
      "grad_norm": 13.753533363342285,
      "learning_rate": 5.659251504841665e-06,
      "loss": 0.3817,
      "step": 2020
    },
    {
      "epoch": 0.052899296770028095,
      "grad_norm": 13.88978385925293,
      "learning_rate": 5.656110965715782e-06,
      "loss": 0.4932,
      "step": 2021
    },
    {
      "epoch": 0.052925471582878185,
      "grad_norm": 27.949993133544922,
      "learning_rate": 5.652970426589898e-06,
      "loss": 0.9588,
      "step": 2022
    },
    {
      "epoch": 0.05295164639572827,
      "grad_norm": 17.469411849975586,
      "learning_rate": 5.649829887464015e-06,
      "loss": 0.4961,
      "step": 2023
    },
    {
      "epoch": 0.05297782120857836,
      "grad_norm": 16.053001403808594,
      "learning_rate": 5.646689348338131e-06,
      "loss": 0.4316,
      "step": 2024
    },
    {
      "epoch": 0.05300399602142845,
      "grad_norm": 25.073213577270508,
      "learning_rate": 5.6435488092122484e-06,
      "loss": 0.3741,
      "step": 2025
    },
    {
      "epoch": 0.05303017083427854,
      "grad_norm": 26.756397247314453,
      "learning_rate": 5.640408270086365e-06,
      "loss": 0.5553,
      "step": 2026
    },
    {
      "epoch": 0.053056345647128626,
      "grad_norm": 17.8182315826416,
      "learning_rate": 5.637267730960482e-06,
      "loss": 0.4063,
      "step": 2027
    },
    {
      "epoch": 0.05308252045997871,
      "grad_norm": 31.33966064453125,
      "learning_rate": 5.6341271918345985e-06,
      "loss": 0.2459,
      "step": 2028
    },
    {
      "epoch": 0.0531086952728288,
      "grad_norm": 21.29205894470215,
      "learning_rate": 5.630986652708715e-06,
      "loss": 0.3535,
      "step": 2029
    },
    {
      "epoch": 0.05313487008567889,
      "grad_norm": 15.377302169799805,
      "learning_rate": 5.627846113582832e-06,
      "loss": 0.249,
      "step": 2030
    },
    {
      "epoch": 0.05316104489852898,
      "grad_norm": 22.145448684692383,
      "learning_rate": 5.624705574456949e-06,
      "loss": 0.7387,
      "step": 2031
    },
    {
      "epoch": 0.05318721971137906,
      "grad_norm": 34.37648391723633,
      "learning_rate": 5.621565035331065e-06,
      "loss": 0.3427,
      "step": 2032
    },
    {
      "epoch": 0.05321339452422915,
      "grad_norm": 30.204763412475586,
      "learning_rate": 5.618424496205182e-06,
      "loss": 0.4876,
      "step": 2033
    },
    {
      "epoch": 0.05323956933707924,
      "grad_norm": 37.700870513916016,
      "learning_rate": 5.615283957079299e-06,
      "loss": 0.8894,
      "step": 2034
    },
    {
      "epoch": 0.05326574414992933,
      "grad_norm": 21.006263732910156,
      "learning_rate": 5.6121434179534155e-06,
      "loss": 0.3491,
      "step": 2035
    },
    {
      "epoch": 0.05329191896277942,
      "grad_norm": 22.384843826293945,
      "learning_rate": 5.609002878827532e-06,
      "loss": 0.3794,
      "step": 2036
    },
    {
      "epoch": 0.0533180937756295,
      "grad_norm": 22.058996200561523,
      "learning_rate": 5.605862339701649e-06,
      "loss": 0.5033,
      "step": 2037
    },
    {
      "epoch": 0.05334426858847959,
      "grad_norm": 22.14150047302246,
      "learning_rate": 5.602721800575766e-06,
      "loss": 0.55,
      "step": 2038
    },
    {
      "epoch": 0.05337044340132968,
      "grad_norm": 13.204755783081055,
      "learning_rate": 5.599581261449882e-06,
      "loss": 0.2087,
      "step": 2039
    },
    {
      "epoch": 0.05339661821417977,
      "grad_norm": 9.960838317871094,
      "learning_rate": 5.596440722323999e-06,
      "loss": 0.1973,
      "step": 2040
    },
    {
      "epoch": 0.053422793027029854,
      "grad_norm": 28.187740325927734,
      "learning_rate": 5.593300183198116e-06,
      "loss": 0.4848,
      "step": 2041
    },
    {
      "epoch": 0.05344896783987994,
      "grad_norm": 21.20586585998535,
      "learning_rate": 5.5901596440722324e-06,
      "loss": 0.3074,
      "step": 2042
    },
    {
      "epoch": 0.05347514265273003,
      "grad_norm": 15.340209007263184,
      "learning_rate": 5.587019104946349e-06,
      "loss": 0.413,
      "step": 2043
    },
    {
      "epoch": 0.05350131746558012,
      "grad_norm": 18.499187469482422,
      "learning_rate": 5.583878565820466e-06,
      "loss": 0.6072,
      "step": 2044
    },
    {
      "epoch": 0.05352749227843021,
      "grad_norm": 22.318227767944336,
      "learning_rate": 5.5807380266945826e-06,
      "loss": 0.6892,
      "step": 2045
    },
    {
      "epoch": 0.053553667091280295,
      "grad_norm": 29.269710540771484,
      "learning_rate": 5.5775974875687e-06,
      "loss": 0.5806,
      "step": 2046
    },
    {
      "epoch": 0.053579841904130385,
      "grad_norm": 13.902078628540039,
      "learning_rate": 5.574456948442816e-06,
      "loss": 0.1188,
      "step": 2047
    },
    {
      "epoch": 0.053606016716980474,
      "grad_norm": 28.622047424316406,
      "learning_rate": 5.571316409316933e-06,
      "loss": 0.384,
      "step": 2048
    },
    {
      "epoch": 0.053632191529830564,
      "grad_norm": 29.5849609375,
      "learning_rate": 5.568175870191049e-06,
      "loss": 0.6745,
      "step": 2049
    },
    {
      "epoch": 0.05365836634268065,
      "grad_norm": 21.074508666992188,
      "learning_rate": 5.565035331065166e-06,
      "loss": 0.7237,
      "step": 2050
    },
    {
      "epoch": 0.053684541155530736,
      "grad_norm": 15.892409324645996,
      "learning_rate": 5.561894791939283e-06,
      "loss": 0.2565,
      "step": 2051
    },
    {
      "epoch": 0.053710715968380826,
      "grad_norm": 24.76845932006836,
      "learning_rate": 5.5587542528133995e-06,
      "loss": 0.733,
      "step": 2052
    },
    {
      "epoch": 0.053736890781230916,
      "grad_norm": 23.633739471435547,
      "learning_rate": 5.555613713687517e-06,
      "loss": 0.4365,
      "step": 2053
    },
    {
      "epoch": 0.053763065594081005,
      "grad_norm": 15.338064193725586,
      "learning_rate": 5.552473174561634e-06,
      "loss": 0.2336,
      "step": 2054
    },
    {
      "epoch": 0.05378924040693109,
      "grad_norm": 17.44646644592285,
      "learning_rate": 5.54933263543575e-06,
      "loss": 0.5267,
      "step": 2055
    },
    {
      "epoch": 0.05381541521978118,
      "grad_norm": 17.927734375,
      "learning_rate": 5.546192096309866e-06,
      "loss": 0.3552,
      "step": 2056
    },
    {
      "epoch": 0.05384159003263127,
      "grad_norm": 16.404600143432617,
      "learning_rate": 5.543051557183983e-06,
      "loss": 0.412,
      "step": 2057
    },
    {
      "epoch": 0.05386776484548136,
      "grad_norm": 15.542909622192383,
      "learning_rate": 5.5399110180581e-06,
      "loss": 0.3205,
      "step": 2058
    },
    {
      "epoch": 0.05389393965833144,
      "grad_norm": 17.42000961303711,
      "learning_rate": 5.536770478932217e-06,
      "loss": 0.4747,
      "step": 2059
    },
    {
      "epoch": 0.05392011447118153,
      "grad_norm": 18.806886672973633,
      "learning_rate": 5.533629939806334e-06,
      "loss": 0.3826,
      "step": 2060
    },
    {
      "epoch": 0.05394628928403162,
      "grad_norm": 15.121427536010742,
      "learning_rate": 5.530489400680451e-06,
      "loss": 0.2826,
      "step": 2061
    },
    {
      "epoch": 0.05397246409688171,
      "grad_norm": 24.61280059814453,
      "learning_rate": 5.527348861554567e-06,
      "loss": 0.4571,
      "step": 2062
    },
    {
      "epoch": 0.0539986389097318,
      "grad_norm": 21.013893127441406,
      "learning_rate": 5.524208322428683e-06,
      "loss": 0.5256,
      "step": 2063
    },
    {
      "epoch": 0.05402481372258188,
      "grad_norm": 12.455665588378906,
      "learning_rate": 5.5210677833028e-06,
      "loss": 0.3233,
      "step": 2064
    },
    {
      "epoch": 0.05405098853543197,
      "grad_norm": 24.779170989990234,
      "learning_rate": 5.517927244176917e-06,
      "loss": 0.4252,
      "step": 2065
    },
    {
      "epoch": 0.05407716334828206,
      "grad_norm": 16.499025344848633,
      "learning_rate": 5.514786705051034e-06,
      "loss": 0.482,
      "step": 2066
    },
    {
      "epoch": 0.05410333816113215,
      "grad_norm": 28.558523178100586,
      "learning_rate": 5.511646165925151e-06,
      "loss": 0.5045,
      "step": 2067
    },
    {
      "epoch": 0.05412951297398223,
      "grad_norm": 22.308773040771484,
      "learning_rate": 5.508505626799268e-06,
      "loss": 0.5698,
      "step": 2068
    },
    {
      "epoch": 0.05415568778683232,
      "grad_norm": 19.00848960876465,
      "learning_rate": 5.505365087673384e-06,
      "loss": 0.363,
      "step": 2069
    },
    {
      "epoch": 0.05418186259968241,
      "grad_norm": 16.68853759765625,
      "learning_rate": 5.502224548547501e-06,
      "loss": 0.3596,
      "step": 2070
    },
    {
      "epoch": 0.0542080374125325,
      "grad_norm": 18.87465476989746,
      "learning_rate": 5.499084009421617e-06,
      "loss": 0.396,
      "step": 2071
    },
    {
      "epoch": 0.05423421222538259,
      "grad_norm": 24.144742965698242,
      "learning_rate": 5.4959434702957345e-06,
      "loss": 0.4311,
      "step": 2072
    },
    {
      "epoch": 0.054260387038232674,
      "grad_norm": 11.344558715820312,
      "learning_rate": 5.492802931169851e-06,
      "loss": 0.2794,
      "step": 2073
    },
    {
      "epoch": 0.054286561851082764,
      "grad_norm": 16.29212760925293,
      "learning_rate": 5.489662392043968e-06,
      "loss": 0.441,
      "step": 2074
    },
    {
      "epoch": 0.054312736663932853,
      "grad_norm": 17.83623504638672,
      "learning_rate": 5.486521852918085e-06,
      "loss": 0.3319,
      "step": 2075
    },
    {
      "epoch": 0.05433891147678294,
      "grad_norm": 16.7191219329834,
      "learning_rate": 5.483381313792201e-06,
      "loss": 0.2954,
      "step": 2076
    },
    {
      "epoch": 0.054365086289633026,
      "grad_norm": 19.14940071105957,
      "learning_rate": 5.480240774666318e-06,
      "loss": 0.4049,
      "step": 2077
    },
    {
      "epoch": 0.054391261102483116,
      "grad_norm": 25.11846923828125,
      "learning_rate": 5.477100235540434e-06,
      "loss": 0.6279,
      "step": 2078
    },
    {
      "epoch": 0.054417435915333205,
      "grad_norm": 47.919124603271484,
      "learning_rate": 5.473959696414551e-06,
      "loss": 0.5845,
      "step": 2079
    },
    {
      "epoch": 0.054443610728183295,
      "grad_norm": 23.86409568786621,
      "learning_rate": 5.470819157288668e-06,
      "loss": 0.3278,
      "step": 2080
    },
    {
      "epoch": 0.054469785541033384,
      "grad_norm": 14.500106811523438,
      "learning_rate": 5.467678618162785e-06,
      "loss": 0.2883,
      "step": 2081
    },
    {
      "epoch": 0.05449596035388347,
      "grad_norm": 19.335567474365234,
      "learning_rate": 5.4645380790369015e-06,
      "loss": 0.151,
      "step": 2082
    },
    {
      "epoch": 0.05452213516673356,
      "grad_norm": 13.740371704101562,
      "learning_rate": 5.461397539911018e-06,
      "loss": 0.3088,
      "step": 2083
    },
    {
      "epoch": 0.054548309979583647,
      "grad_norm": 13.936662673950195,
      "learning_rate": 5.458257000785135e-06,
      "loss": 0.3865,
      "step": 2084
    },
    {
      "epoch": 0.054574484792433736,
      "grad_norm": 20.736095428466797,
      "learning_rate": 5.455116461659252e-06,
      "loss": 0.2429,
      "step": 2085
    },
    {
      "epoch": 0.05460065960528382,
      "grad_norm": 19.1993350982666,
      "learning_rate": 5.451975922533368e-06,
      "loss": 0.4528,
      "step": 2086
    },
    {
      "epoch": 0.05462683441813391,
      "grad_norm": 26.143178939819336,
      "learning_rate": 5.448835383407485e-06,
      "loss": 0.644,
      "step": 2087
    },
    {
      "epoch": 0.054653009230984,
      "grad_norm": 19.510967254638672,
      "learning_rate": 5.445694844281602e-06,
      "loss": 0.4799,
      "step": 2088
    },
    {
      "epoch": 0.05467918404383409,
      "grad_norm": 21.278804779052734,
      "learning_rate": 5.4425543051557185e-06,
      "loss": 0.6795,
      "step": 2089
    },
    {
      "epoch": 0.05470535885668418,
      "grad_norm": 24.920244216918945,
      "learning_rate": 5.439413766029835e-06,
      "loss": 0.4774,
      "step": 2090
    },
    {
      "epoch": 0.05473153366953426,
      "grad_norm": 15.634344100952148,
      "learning_rate": 5.436273226903952e-06,
      "loss": 0.4628,
      "step": 2091
    },
    {
      "epoch": 0.05475770848238435,
      "grad_norm": 11.112319946289062,
      "learning_rate": 5.433132687778069e-06,
      "loss": 0.1991,
      "step": 2092
    },
    {
      "epoch": 0.05478388329523444,
      "grad_norm": 24.829647064208984,
      "learning_rate": 5.429992148652186e-06,
      "loss": 0.5774,
      "step": 2093
    },
    {
      "epoch": 0.05481005810808453,
      "grad_norm": 12.118178367614746,
      "learning_rate": 5.426851609526302e-06,
      "loss": 0.2018,
      "step": 2094
    },
    {
      "epoch": 0.05483623292093461,
      "grad_norm": 21.71137237548828,
      "learning_rate": 5.423711070400419e-06,
      "loss": 0.5013,
      "step": 2095
    },
    {
      "epoch": 0.0548624077337847,
      "grad_norm": 22.573965072631836,
      "learning_rate": 5.420570531274535e-06,
      "loss": 0.3584,
      "step": 2096
    },
    {
      "epoch": 0.05488858254663479,
      "grad_norm": 29.388010025024414,
      "learning_rate": 5.417429992148652e-06,
      "loss": 0.3825,
      "step": 2097
    },
    {
      "epoch": 0.05491475735948488,
      "grad_norm": 5.26355504989624,
      "learning_rate": 5.414289453022769e-06,
      "loss": 0.0485,
      "step": 2098
    },
    {
      "epoch": 0.05494093217233497,
      "grad_norm": 22.727529525756836,
      "learning_rate": 5.4111489138968855e-06,
      "loss": 0.7656,
      "step": 2099
    },
    {
      "epoch": 0.05496710698518505,
      "grad_norm": 21.769676208496094,
      "learning_rate": 5.408008374771003e-06,
      "loss": 0.6856,
      "step": 2100
    },
    {
      "epoch": 0.05499328179803514,
      "grad_norm": 17.072118759155273,
      "learning_rate": 5.40486783564512e-06,
      "loss": 0.6848,
      "step": 2101
    },
    {
      "epoch": 0.05501945661088523,
      "grad_norm": 44.72522735595703,
      "learning_rate": 5.401727296519236e-06,
      "loss": 0.8214,
      "step": 2102
    },
    {
      "epoch": 0.05504563142373532,
      "grad_norm": 19.431623458862305,
      "learning_rate": 5.398586757393352e-06,
      "loss": 0.5175,
      "step": 2103
    },
    {
      "epoch": 0.055071806236585405,
      "grad_norm": 17.057085037231445,
      "learning_rate": 5.395446218267469e-06,
      "loss": 0.6432,
      "step": 2104
    },
    {
      "epoch": 0.055097981049435495,
      "grad_norm": 28.238933563232422,
      "learning_rate": 5.392305679141586e-06,
      "loss": 0.7673,
      "step": 2105
    },
    {
      "epoch": 0.055124155862285584,
      "grad_norm": 21.83881950378418,
      "learning_rate": 5.389165140015703e-06,
      "loss": 0.6037,
      "step": 2106
    },
    {
      "epoch": 0.055150330675135674,
      "grad_norm": 25.47792625427246,
      "learning_rate": 5.38602460088982e-06,
      "loss": 0.348,
      "step": 2107
    },
    {
      "epoch": 0.055176505487985764,
      "grad_norm": 36.0902099609375,
      "learning_rate": 5.382884061763937e-06,
      "loss": 0.6526,
      "step": 2108
    },
    {
      "epoch": 0.055202680300835846,
      "grad_norm": 25.27216911315918,
      "learning_rate": 5.379743522638053e-06,
      "loss": 0.467,
      "step": 2109
    },
    {
      "epoch": 0.055228855113685936,
      "grad_norm": 18.512786865234375,
      "learning_rate": 5.376602983512169e-06,
      "loss": 0.4,
      "step": 2110
    },
    {
      "epoch": 0.055255029926536026,
      "grad_norm": 38.42414474487305,
      "learning_rate": 5.373462444386286e-06,
      "loss": 0.4771,
      "step": 2111
    },
    {
      "epoch": 0.055281204739386115,
      "grad_norm": 25.52336311340332,
      "learning_rate": 5.370321905260403e-06,
      "loss": 0.6265,
      "step": 2112
    },
    {
      "epoch": 0.055307379552236205,
      "grad_norm": 26.620622634887695,
      "learning_rate": 5.36718136613452e-06,
      "loss": 0.3303,
      "step": 2113
    },
    {
      "epoch": 0.05533355436508629,
      "grad_norm": 23.267398834228516,
      "learning_rate": 5.364040827008637e-06,
      "loss": 0.3195,
      "step": 2114
    },
    {
      "epoch": 0.05535972917793638,
      "grad_norm": 24.473112106323242,
      "learning_rate": 5.360900287882754e-06,
      "loss": 0.4372,
      "step": 2115
    },
    {
      "epoch": 0.05538590399078647,
      "grad_norm": 16.481266021728516,
      "learning_rate": 5.35775974875687e-06,
      "loss": 0.3483,
      "step": 2116
    },
    {
      "epoch": 0.05541207880363656,
      "grad_norm": 17.836196899414062,
      "learning_rate": 5.354619209630986e-06,
      "loss": 0.4124,
      "step": 2117
    },
    {
      "epoch": 0.05543825361648664,
      "grad_norm": 13.136251449584961,
      "learning_rate": 5.351478670505103e-06,
      "loss": 0.2859,
      "step": 2118
    },
    {
      "epoch": 0.05546442842933673,
      "grad_norm": 18.5010929107666,
      "learning_rate": 5.3483381313792205e-06,
      "loss": 0.3846,
      "step": 2119
    },
    {
      "epoch": 0.05549060324218682,
      "grad_norm": 12.54349136352539,
      "learning_rate": 5.345197592253337e-06,
      "loss": 0.2891,
      "step": 2120
    },
    {
      "epoch": 0.05551677805503691,
      "grad_norm": 21.67892074584961,
      "learning_rate": 5.342057053127454e-06,
      "loss": 0.276,
      "step": 2121
    },
    {
      "epoch": 0.055542952867887,
      "grad_norm": 22.016136169433594,
      "learning_rate": 5.338916514001571e-06,
      "loss": 0.4363,
      "step": 2122
    },
    {
      "epoch": 0.05556912768073708,
      "grad_norm": 11.692338943481445,
      "learning_rate": 5.335775974875687e-06,
      "loss": 0.1892,
      "step": 2123
    },
    {
      "epoch": 0.05559530249358717,
      "grad_norm": 37.437564849853516,
      "learning_rate": 5.332635435749804e-06,
      "loss": 0.8204,
      "step": 2124
    },
    {
      "epoch": 0.05562147730643726,
      "grad_norm": 27.237586975097656,
      "learning_rate": 5.32949489662392e-06,
      "loss": 0.6138,
      "step": 2125
    },
    {
      "epoch": 0.05564765211928735,
      "grad_norm": 10.734899520874023,
      "learning_rate": 5.3263543574980374e-06,
      "loss": 0.2382,
      "step": 2126
    },
    {
      "epoch": 0.05567382693213743,
      "grad_norm": 20.324880599975586,
      "learning_rate": 5.323213818372154e-06,
      "loss": 0.288,
      "step": 2127
    },
    {
      "epoch": 0.05570000174498752,
      "grad_norm": 23.302186965942383,
      "learning_rate": 5.320073279246271e-06,
      "loss": 0.3014,
      "step": 2128
    },
    {
      "epoch": 0.05572617655783761,
      "grad_norm": 21.766252517700195,
      "learning_rate": 5.3169327401203876e-06,
      "loss": 0.368,
      "step": 2129
    },
    {
      "epoch": 0.0557523513706877,
      "grad_norm": 19.442155838012695,
      "learning_rate": 5.313792200994504e-06,
      "loss": 0.4198,
      "step": 2130
    },
    {
      "epoch": 0.05577852618353779,
      "grad_norm": 17.189556121826172,
      "learning_rate": 5.310651661868621e-06,
      "loss": 0.3214,
      "step": 2131
    },
    {
      "epoch": 0.055804700996387874,
      "grad_norm": 31.147642135620117,
      "learning_rate": 5.307511122742738e-06,
      "loss": 0.4249,
      "step": 2132
    },
    {
      "epoch": 0.05583087580923796,
      "grad_norm": 16.954578399658203,
      "learning_rate": 5.304370583616854e-06,
      "loss": 0.3112,
      "step": 2133
    },
    {
      "epoch": 0.05585705062208805,
      "grad_norm": 15.970924377441406,
      "learning_rate": 5.301230044490971e-06,
      "loss": 0.2905,
      "step": 2134
    },
    {
      "epoch": 0.05588322543493814,
      "grad_norm": 18.114315032958984,
      "learning_rate": 5.298089505365088e-06,
      "loss": 0.3805,
      "step": 2135
    },
    {
      "epoch": 0.055909400247788225,
      "grad_norm": 28.090883255004883,
      "learning_rate": 5.2949489662392045e-06,
      "loss": 0.5218,
      "step": 2136
    },
    {
      "epoch": 0.055935575060638315,
      "grad_norm": 14.584859848022461,
      "learning_rate": 5.291808427113321e-06,
      "loss": 0.3807,
      "step": 2137
    },
    {
      "epoch": 0.055961749873488405,
      "grad_norm": 20.808547973632812,
      "learning_rate": 5.288667887987438e-06,
      "loss": 0.5425,
      "step": 2138
    },
    {
      "epoch": 0.055987924686338494,
      "grad_norm": 16.373830795288086,
      "learning_rate": 5.285527348861555e-06,
      "loss": 0.1996,
      "step": 2139
    },
    {
      "epoch": 0.056014099499188584,
      "grad_norm": 16.099390029907227,
      "learning_rate": 5.282386809735671e-06,
      "loss": 0.2434,
      "step": 2140
    },
    {
      "epoch": 0.05604027431203867,
      "grad_norm": 25.453414916992188,
      "learning_rate": 5.279246270609788e-06,
      "loss": 0.4264,
      "step": 2141
    },
    {
      "epoch": 0.056066449124888756,
      "grad_norm": 15.937348365783691,
      "learning_rate": 5.276105731483905e-06,
      "loss": 0.179,
      "step": 2142
    },
    {
      "epoch": 0.056092623937738846,
      "grad_norm": 23.500062942504883,
      "learning_rate": 5.2729651923580215e-06,
      "loss": 0.5904,
      "step": 2143
    },
    {
      "epoch": 0.056118798750588936,
      "grad_norm": 22.93499183654785,
      "learning_rate": 5.269824653232138e-06,
      "loss": 0.6213,
      "step": 2144
    },
    {
      "epoch": 0.05614497356343902,
      "grad_norm": 25.736841201782227,
      "learning_rate": 5.266684114106255e-06,
      "loss": 0.4701,
      "step": 2145
    },
    {
      "epoch": 0.05617114837628911,
      "grad_norm": 18.704618453979492,
      "learning_rate": 5.2635435749803716e-06,
      "loss": 0.3382,
      "step": 2146
    },
    {
      "epoch": 0.0561973231891392,
      "grad_norm": 19.30751609802246,
      "learning_rate": 5.260403035854489e-06,
      "loss": 0.4667,
      "step": 2147
    },
    {
      "epoch": 0.05622349800198929,
      "grad_norm": 19.518234252929688,
      "learning_rate": 5.257262496728605e-06,
      "loss": 0.3453,
      "step": 2148
    },
    {
      "epoch": 0.05624967281483938,
      "grad_norm": 22.279211044311523,
      "learning_rate": 5.254121957602722e-06,
      "loss": 0.464,
      "step": 2149
    },
    {
      "epoch": 0.05627584762768946,
      "grad_norm": 20.52585792541504,
      "learning_rate": 5.250981418476838e-06,
      "loss": 0.5112,
      "step": 2150
    },
    {
      "epoch": 0.05630202244053955,
      "grad_norm": 28.339649200439453,
      "learning_rate": 5.247840879350955e-06,
      "loss": 0.4232,
      "step": 2151
    },
    {
      "epoch": 0.05632819725338964,
      "grad_norm": 17.16951560974121,
      "learning_rate": 5.244700340225072e-06,
      "loss": 0.3581,
      "step": 2152
    },
    {
      "epoch": 0.05635437206623973,
      "grad_norm": 24.63799476623535,
      "learning_rate": 5.241559801099189e-06,
      "loss": 0.4826,
      "step": 2153
    },
    {
      "epoch": 0.05638054687908981,
      "grad_norm": 11.403420448303223,
      "learning_rate": 5.238419261973306e-06,
      "loss": 0.2016,
      "step": 2154
    },
    {
      "epoch": 0.0564067216919399,
      "grad_norm": 29.246339797973633,
      "learning_rate": 5.235278722847423e-06,
      "loss": 0.4023,
      "step": 2155
    },
    {
      "epoch": 0.05643289650478999,
      "grad_norm": 30.800003051757812,
      "learning_rate": 5.232138183721539e-06,
      "loss": 0.4854,
      "step": 2156
    },
    {
      "epoch": 0.05645907131764008,
      "grad_norm": 38.70536422729492,
      "learning_rate": 5.228997644595655e-06,
      "loss": 0.7932,
      "step": 2157
    },
    {
      "epoch": 0.05648524613049017,
      "grad_norm": 18.307180404663086,
      "learning_rate": 5.225857105469772e-06,
      "loss": 0.3275,
      "step": 2158
    },
    {
      "epoch": 0.05651142094334025,
      "grad_norm": 23.157611846923828,
      "learning_rate": 5.222716566343889e-06,
      "loss": 0.3827,
      "step": 2159
    },
    {
      "epoch": 0.05653759575619034,
      "grad_norm": 29.438823699951172,
      "learning_rate": 5.219576027218006e-06,
      "loss": 0.4146,
      "step": 2160
    },
    {
      "epoch": 0.05656377056904043,
      "grad_norm": 14.448984146118164,
      "learning_rate": 5.216435488092123e-06,
      "loss": 0.5522,
      "step": 2161
    },
    {
      "epoch": 0.05658994538189052,
      "grad_norm": 18.98543930053711,
      "learning_rate": 5.21329494896624e-06,
      "loss": 0.6133,
      "step": 2162
    },
    {
      "epoch": 0.056616120194740605,
      "grad_norm": 15.578160285949707,
      "learning_rate": 5.210154409840356e-06,
      "loss": 0.5581,
      "step": 2163
    },
    {
      "epoch": 0.056642295007590694,
      "grad_norm": 21.264514923095703,
      "learning_rate": 5.207013870714472e-06,
      "loss": 0.3096,
      "step": 2164
    },
    {
      "epoch": 0.056668469820440784,
      "grad_norm": 23.866355895996094,
      "learning_rate": 5.203873331588589e-06,
      "loss": 0.5284,
      "step": 2165
    },
    {
      "epoch": 0.056694644633290874,
      "grad_norm": 21.515766143798828,
      "learning_rate": 5.2007327924627065e-06,
      "loss": 0.3848,
      "step": 2166
    },
    {
      "epoch": 0.05672081944614096,
      "grad_norm": 26.587404251098633,
      "learning_rate": 5.197592253336823e-06,
      "loss": 0.4829,
      "step": 2167
    },
    {
      "epoch": 0.056746994258991046,
      "grad_norm": 15.177087783813477,
      "learning_rate": 5.19445171421094e-06,
      "loss": 0.4405,
      "step": 2168
    },
    {
      "epoch": 0.056773169071841136,
      "grad_norm": 16.489826202392578,
      "learning_rate": 5.191311175085057e-06,
      "loss": 0.5992,
      "step": 2169
    },
    {
      "epoch": 0.056799343884691225,
      "grad_norm": 23.835365295410156,
      "learning_rate": 5.188170635959173e-06,
      "loss": 0.4785,
      "step": 2170
    },
    {
      "epoch": 0.056825518697541315,
      "grad_norm": 20.686721801757812,
      "learning_rate": 5.185030096833289e-06,
      "loss": 0.4644,
      "step": 2171
    },
    {
      "epoch": 0.0568516935103914,
      "grad_norm": 29.27372932434082,
      "learning_rate": 5.181889557707406e-06,
      "loss": 0.8097,
      "step": 2172
    },
    {
      "epoch": 0.05687786832324149,
      "grad_norm": 19.50539779663086,
      "learning_rate": 5.1787490185815235e-06,
      "loss": 0.5297,
      "step": 2173
    },
    {
      "epoch": 0.05690404313609158,
      "grad_norm": 16.574010848999023,
      "learning_rate": 5.17560847945564e-06,
      "loss": 0.5774,
      "step": 2174
    },
    {
      "epoch": 0.05693021794894167,
      "grad_norm": 16.583431243896484,
      "learning_rate": 5.172467940329757e-06,
      "loss": 0.3388,
      "step": 2175
    },
    {
      "epoch": 0.056956392761791756,
      "grad_norm": 10.801560401916504,
      "learning_rate": 5.169327401203874e-06,
      "loss": 0.2998,
      "step": 2176
    },
    {
      "epoch": 0.05698256757464184,
      "grad_norm": 25.58047103881836,
      "learning_rate": 5.16618686207799e-06,
      "loss": 0.5534,
      "step": 2177
    },
    {
      "epoch": 0.05700874238749193,
      "grad_norm": 22.653905868530273,
      "learning_rate": 5.163046322952107e-06,
      "loss": 0.496,
      "step": 2178
    },
    {
      "epoch": 0.05703491720034202,
      "grad_norm": 12.825448989868164,
      "learning_rate": 5.159905783826224e-06,
      "loss": 0.1791,
      "step": 2179
    },
    {
      "epoch": 0.05706109201319211,
      "grad_norm": 13.295766830444336,
      "learning_rate": 5.1567652447003404e-06,
      "loss": 0.2995,
      "step": 2180
    },
    {
      "epoch": 0.05708726682604219,
      "grad_norm": 28.87529754638672,
      "learning_rate": 5.153624705574457e-06,
      "loss": 0.4951,
      "step": 2181
    },
    {
      "epoch": 0.05711344163889228,
      "grad_norm": 23.27678871154785,
      "learning_rate": 5.150484166448574e-06,
      "loss": 0.304,
      "step": 2182
    },
    {
      "epoch": 0.05713961645174237,
      "grad_norm": 18.578466415405273,
      "learning_rate": 5.1473436273226905e-06,
      "loss": 0.3572,
      "step": 2183
    },
    {
      "epoch": 0.05716579126459246,
      "grad_norm": 22.815126419067383,
      "learning_rate": 5.144203088196807e-06,
      "loss": 0.5904,
      "step": 2184
    },
    {
      "epoch": 0.05719196607744255,
      "grad_norm": 29.255250930786133,
      "learning_rate": 5.141062549070924e-06,
      "loss": 0.45,
      "step": 2185
    },
    {
      "epoch": 0.05721814089029263,
      "grad_norm": 18.1992130279541,
      "learning_rate": 5.137922009945041e-06,
      "loss": 0.4574,
      "step": 2186
    },
    {
      "epoch": 0.05724431570314272,
      "grad_norm": 25.687274932861328,
      "learning_rate": 5.134781470819157e-06,
      "loss": 0.4879,
      "step": 2187
    },
    {
      "epoch": 0.05727049051599281,
      "grad_norm": 10.8041353225708,
      "learning_rate": 5.131640931693274e-06,
      "loss": 0.1243,
      "step": 2188
    },
    {
      "epoch": 0.0572966653288429,
      "grad_norm": 25.144296646118164,
      "learning_rate": 5.128500392567391e-06,
      "loss": 0.551,
      "step": 2189
    },
    {
      "epoch": 0.057322840141692984,
      "grad_norm": 25.47746467590332,
      "learning_rate": 5.1253598534415075e-06,
      "loss": 0.4639,
      "step": 2190
    },
    {
      "epoch": 0.05734901495454307,
      "grad_norm": 17.455787658691406,
      "learning_rate": 5.122219314315624e-06,
      "loss": 0.3995,
      "step": 2191
    },
    {
      "epoch": 0.05737518976739316,
      "grad_norm": 16.494773864746094,
      "learning_rate": 5.119078775189741e-06,
      "loss": 0.4376,
      "step": 2192
    },
    {
      "epoch": 0.05740136458024325,
      "grad_norm": 23.631343841552734,
      "learning_rate": 5.115938236063858e-06,
      "loss": 0.4621,
      "step": 2193
    },
    {
      "epoch": 0.05742753939309334,
      "grad_norm": 17.499542236328125,
      "learning_rate": 5.112797696937974e-06,
      "loss": 0.1808,
      "step": 2194
    },
    {
      "epoch": 0.057453714205943425,
      "grad_norm": 21.53891944885254,
      "learning_rate": 5.109657157812091e-06,
      "loss": 0.4153,
      "step": 2195
    },
    {
      "epoch": 0.057479889018793515,
      "grad_norm": 16.096330642700195,
      "learning_rate": 5.106516618686208e-06,
      "loss": 0.5155,
      "step": 2196
    },
    {
      "epoch": 0.057506063831643604,
      "grad_norm": 21.326282501220703,
      "learning_rate": 5.1033760795603244e-06,
      "loss": 0.373,
      "step": 2197
    },
    {
      "epoch": 0.057532238644493694,
      "grad_norm": 16.372989654541016,
      "learning_rate": 5.100235540434441e-06,
      "loss": 0.3188,
      "step": 2198
    },
    {
      "epoch": 0.05755841345734378,
      "grad_norm": 21.052217483520508,
      "learning_rate": 5.097095001308558e-06,
      "loss": 0.4748,
      "step": 2199
    },
    {
      "epoch": 0.057584588270193866,
      "grad_norm": 16.87690544128418,
      "learning_rate": 5.093954462182675e-06,
      "loss": 0.2336,
      "step": 2200
    },
    {
      "epoch": 0.057610763083043956,
      "grad_norm": 9.869336128234863,
      "learning_rate": 5.090813923056792e-06,
      "loss": 0.2764,
      "step": 2201
    },
    {
      "epoch": 0.057636937895894046,
      "grad_norm": 17.234376907348633,
      "learning_rate": 5.087673383930908e-06,
      "loss": 0.3746,
      "step": 2202
    },
    {
      "epoch": 0.057663112708744135,
      "grad_norm": 21.913917541503906,
      "learning_rate": 5.084532844805025e-06,
      "loss": 0.2648,
      "step": 2203
    },
    {
      "epoch": 0.05768928752159422,
      "grad_norm": 16.715423583984375,
      "learning_rate": 5.081392305679141e-06,
      "loss": 0.5361,
      "step": 2204
    },
    {
      "epoch": 0.05771546233444431,
      "grad_norm": 33.91401290893555,
      "learning_rate": 5.078251766553258e-06,
      "loss": 0.7839,
      "step": 2205
    },
    {
      "epoch": 0.0577416371472944,
      "grad_norm": 29.2191219329834,
      "learning_rate": 5.075111227427375e-06,
      "loss": 0.4765,
      "step": 2206
    },
    {
      "epoch": 0.05776781196014449,
      "grad_norm": 18.336505889892578,
      "learning_rate": 5.071970688301492e-06,
      "loss": 0.2957,
      "step": 2207
    },
    {
      "epoch": 0.05779398677299457,
      "grad_norm": 48.42825698852539,
      "learning_rate": 5.068830149175609e-06,
      "loss": 0.6133,
      "step": 2208
    },
    {
      "epoch": 0.05782016158584466,
      "grad_norm": 22.758392333984375,
      "learning_rate": 5.065689610049726e-06,
      "loss": 0.3468,
      "step": 2209
    },
    {
      "epoch": 0.05784633639869475,
      "grad_norm": 22.312551498413086,
      "learning_rate": 5.062549070923842e-06,
      "loss": 0.4484,
      "step": 2210
    },
    {
      "epoch": 0.05787251121154484,
      "grad_norm": 19.736310958862305,
      "learning_rate": 5.059408531797958e-06,
      "loss": 0.5596,
      "step": 2211
    },
    {
      "epoch": 0.05789868602439493,
      "grad_norm": 11.75357437133789,
      "learning_rate": 5.056267992672075e-06,
      "loss": 0.2431,
      "step": 2212
    },
    {
      "epoch": 0.05792486083724501,
      "grad_norm": 16.70085906982422,
      "learning_rate": 5.053127453546193e-06,
      "loss": 0.3899,
      "step": 2213
    },
    {
      "epoch": 0.0579510356500951,
      "grad_norm": 19.541702270507812,
      "learning_rate": 5.049986914420309e-06,
      "loss": 0.5802,
      "step": 2214
    },
    {
      "epoch": 0.05797721046294519,
      "grad_norm": 22.267160415649414,
      "learning_rate": 5.046846375294426e-06,
      "loss": 0.4647,
      "step": 2215
    },
    {
      "epoch": 0.05800338527579528,
      "grad_norm": 12.967199325561523,
      "learning_rate": 5.043705836168543e-06,
      "loss": 0.3351,
      "step": 2216
    },
    {
      "epoch": 0.05802956008864536,
      "grad_norm": 41.50945281982422,
      "learning_rate": 5.040565297042659e-06,
      "loss": 0.4196,
      "step": 2217
    },
    {
      "epoch": 0.05805573490149545,
      "grad_norm": 20.85186767578125,
      "learning_rate": 5.037424757916775e-06,
      "loss": 0.4018,
      "step": 2218
    },
    {
      "epoch": 0.05808190971434554,
      "grad_norm": 17.239341735839844,
      "learning_rate": 5.034284218790893e-06,
      "loss": 0.2102,
      "step": 2219
    },
    {
      "epoch": 0.05810808452719563,
      "grad_norm": 24.02797508239746,
      "learning_rate": 5.0311436796650095e-06,
      "loss": 0.604,
      "step": 2220
    },
    {
      "epoch": 0.05813425934004572,
      "grad_norm": 20.107194900512695,
      "learning_rate": 5.028003140539126e-06,
      "loss": 0.4782,
      "step": 2221
    },
    {
      "epoch": 0.058160434152895804,
      "grad_norm": 18.98370361328125,
      "learning_rate": 5.024862601413243e-06,
      "loss": 0.2981,
      "step": 2222
    },
    {
      "epoch": 0.058186608965745894,
      "grad_norm": 16.613319396972656,
      "learning_rate": 5.02172206228736e-06,
      "loss": 0.3074,
      "step": 2223
    },
    {
      "epoch": 0.05821278377859598,
      "grad_norm": 28.188547134399414,
      "learning_rate": 5.018581523161476e-06,
      "loss": 0.4034,
      "step": 2224
    },
    {
      "epoch": 0.05823895859144607,
      "grad_norm": 15.769797325134277,
      "learning_rate": 5.015440984035592e-06,
      "loss": 0.3866,
      "step": 2225
    },
    {
      "epoch": 0.05826513340429616,
      "grad_norm": 20.702407836914062,
      "learning_rate": 5.01230044490971e-06,
      "loss": 0.5092,
      "step": 2226
    },
    {
      "epoch": 0.058291308217146245,
      "grad_norm": 20.320173263549805,
      "learning_rate": 5.0091599057838265e-06,
      "loss": 0.2185,
      "step": 2227
    },
    {
      "epoch": 0.058317483029996335,
      "grad_norm": 13.101259231567383,
      "learning_rate": 5.006019366657943e-06,
      "loss": 0.2705,
      "step": 2228
    },
    {
      "epoch": 0.058343657842846425,
      "grad_norm": 17.763822555541992,
      "learning_rate": 5.00287882753206e-06,
      "loss": 0.4093,
      "step": 2229
    },
    {
      "epoch": 0.058369832655696514,
      "grad_norm": 20.036407470703125,
      "learning_rate": 4.999738288406177e-06,
      "loss": 0.2879,
      "step": 2230
    },
    {
      "epoch": 0.0583960074685466,
      "grad_norm": 26.64327621459961,
      "learning_rate": 4.996597749280293e-06,
      "loss": 0.3354,
      "step": 2231
    },
    {
      "epoch": 0.05842218228139669,
      "grad_norm": 19.33296012878418,
      "learning_rate": 4.99345721015441e-06,
      "loss": 0.4497,
      "step": 2232
    },
    {
      "epoch": 0.058448357094246776,
      "grad_norm": 17.099468231201172,
      "learning_rate": 4.990316671028527e-06,
      "loss": 0.3564,
      "step": 2233
    },
    {
      "epoch": 0.058474531907096866,
      "grad_norm": 19.095413208007812,
      "learning_rate": 4.987176131902643e-06,
      "loss": 0.4139,
      "step": 2234
    },
    {
      "epoch": 0.058500706719946956,
      "grad_norm": 14.700515747070312,
      "learning_rate": 4.98403559277676e-06,
      "loss": 0.3637,
      "step": 2235
    },
    {
      "epoch": 0.05852688153279704,
      "grad_norm": 13.880035400390625,
      "learning_rate": 4.980895053650877e-06,
      "loss": 0.3004,
      "step": 2236
    },
    {
      "epoch": 0.05855305634564713,
      "grad_norm": 39.31035614013672,
      "learning_rate": 4.9777545145249935e-06,
      "loss": 0.4483,
      "step": 2237
    },
    {
      "epoch": 0.05857923115849722,
      "grad_norm": 19.329133987426758,
      "learning_rate": 4.97461397539911e-06,
      "loss": 0.3703,
      "step": 2238
    },
    {
      "epoch": 0.05860540597134731,
      "grad_norm": 22.907939910888672,
      "learning_rate": 4.971473436273227e-06,
      "loss": 0.5002,
      "step": 2239
    },
    {
      "epoch": 0.05863158078419739,
      "grad_norm": 21.53961181640625,
      "learning_rate": 4.968332897147344e-06,
      "loss": 0.5427,
      "step": 2240
    },
    {
      "epoch": 0.05865775559704748,
      "grad_norm": 27.999670028686523,
      "learning_rate": 4.96519235802146e-06,
      "loss": 0.5557,
      "step": 2241
    },
    {
      "epoch": 0.05868393040989757,
      "grad_norm": 22.802005767822266,
      "learning_rate": 4.962051818895577e-06,
      "loss": 0.4043,
      "step": 2242
    },
    {
      "epoch": 0.05871010522274766,
      "grad_norm": 22.962631225585938,
      "learning_rate": 4.958911279769694e-06,
      "loss": 0.3877,
      "step": 2243
    },
    {
      "epoch": 0.05873628003559775,
      "grad_norm": 23.54732322692871,
      "learning_rate": 4.9557707406438105e-06,
      "loss": 0.4476,
      "step": 2244
    },
    {
      "epoch": 0.05876245484844783,
      "grad_norm": 18.165769577026367,
      "learning_rate": 4.952630201517927e-06,
      "loss": 0.2783,
      "step": 2245
    },
    {
      "epoch": 0.05878862966129792,
      "grad_norm": 22.484464645385742,
      "learning_rate": 4.949489662392044e-06,
      "loss": 0.5113,
      "step": 2246
    },
    {
      "epoch": 0.05881480447414801,
      "grad_norm": 23.983482360839844,
      "learning_rate": 4.9463491232661614e-06,
      "loss": 0.5992,
      "step": 2247
    },
    {
      "epoch": 0.0588409792869981,
      "grad_norm": 28.92018699645996,
      "learning_rate": 4.943208584140277e-06,
      "loss": 0.5835,
      "step": 2248
    },
    {
      "epoch": 0.05886715409984818,
      "grad_norm": 21.488967895507812,
      "learning_rate": 4.940068045014394e-06,
      "loss": 0.3212,
      "step": 2249
    },
    {
      "epoch": 0.05889332891269827,
      "grad_norm": 17.249807357788086,
      "learning_rate": 4.936927505888511e-06,
      "loss": 0.3961,
      "step": 2250
    },
    {
      "epoch": 0.05891950372554836,
      "grad_norm": 26.02885627746582,
      "learning_rate": 4.933786966762627e-06,
      "loss": 0.4976,
      "step": 2251
    },
    {
      "epoch": 0.05894567853839845,
      "grad_norm": 30.58405876159668,
      "learning_rate": 4.930646427636744e-06,
      "loss": 0.7642,
      "step": 2252
    },
    {
      "epoch": 0.05897185335124854,
      "grad_norm": 22.37689971923828,
      "learning_rate": 4.927505888510862e-06,
      "loss": 0.4396,
      "step": 2253
    },
    {
      "epoch": 0.058998028164098625,
      "grad_norm": 17.074554443359375,
      "learning_rate": 4.924365349384978e-06,
      "loss": 0.3551,
      "step": 2254
    },
    {
      "epoch": 0.059024202976948714,
      "grad_norm": 20.867202758789062,
      "learning_rate": 4.921224810259095e-06,
      "loss": 0.2883,
      "step": 2255
    },
    {
      "epoch": 0.059050377789798804,
      "grad_norm": 16.282190322875977,
      "learning_rate": 4.918084271133211e-06,
      "loss": 0.45,
      "step": 2256
    },
    {
      "epoch": 0.059076552602648894,
      "grad_norm": 19.279281616210938,
      "learning_rate": 4.914943732007328e-06,
      "loss": 0.4318,
      "step": 2257
    },
    {
      "epoch": 0.059102727415498976,
      "grad_norm": 16.8918399810791,
      "learning_rate": 4.911803192881444e-06,
      "loss": 0.31,
      "step": 2258
    },
    {
      "epoch": 0.059128902228349066,
      "grad_norm": 27.33017921447754,
      "learning_rate": 4.908662653755561e-06,
      "loss": 0.2894,
      "step": 2259
    },
    {
      "epoch": 0.059155077041199156,
      "grad_norm": 14.184236526489258,
      "learning_rate": 4.905522114629679e-06,
      "loss": 0.329,
      "step": 2260
    },
    {
      "epoch": 0.059181251854049245,
      "grad_norm": 30.423824310302734,
      "learning_rate": 4.902381575503795e-06,
      "loss": 0.8389,
      "step": 2261
    },
    {
      "epoch": 0.059207426666899335,
      "grad_norm": 17.21653938293457,
      "learning_rate": 4.899241036377912e-06,
      "loss": 0.3467,
      "step": 2262
    },
    {
      "epoch": 0.05923360147974942,
      "grad_norm": 14.85358715057373,
      "learning_rate": 4.896100497252029e-06,
      "loss": 0.3053,
      "step": 2263
    },
    {
      "epoch": 0.05925977629259951,
      "grad_norm": 16.879989624023438,
      "learning_rate": 4.892959958126145e-06,
      "loss": 0.4257,
      "step": 2264
    },
    {
      "epoch": 0.0592859511054496,
      "grad_norm": 22.85228729248047,
      "learning_rate": 4.889819419000261e-06,
      "loss": 0.6377,
      "step": 2265
    },
    {
      "epoch": 0.05931212591829969,
      "grad_norm": 16.203758239746094,
      "learning_rate": 4.886678879874379e-06,
      "loss": 0.3783,
      "step": 2266
    },
    {
      "epoch": 0.05933830073114977,
      "grad_norm": 20.971885681152344,
      "learning_rate": 4.8835383407484956e-06,
      "loss": 0.4814,
      "step": 2267
    },
    {
      "epoch": 0.05936447554399986,
      "grad_norm": 17.760112762451172,
      "learning_rate": 4.880397801622612e-06,
      "loss": 0.2962,
      "step": 2268
    },
    {
      "epoch": 0.05939065035684995,
      "grad_norm": 39.19157409667969,
      "learning_rate": 4.877257262496729e-06,
      "loss": 0.6008,
      "step": 2269
    },
    {
      "epoch": 0.05941682516970004,
      "grad_norm": 16.625267028808594,
      "learning_rate": 4.874116723370846e-06,
      "loss": 0.2921,
      "step": 2270
    },
    {
      "epoch": 0.05944299998255013,
      "grad_norm": 12.032122611999512,
      "learning_rate": 4.870976184244962e-06,
      "loss": 0.3123,
      "step": 2271
    },
    {
      "epoch": 0.05946917479540021,
      "grad_norm": 17.285192489624023,
      "learning_rate": 4.867835645119078e-06,
      "loss": 0.2721,
      "step": 2272
    },
    {
      "epoch": 0.0594953496082503,
      "grad_norm": 18.02934455871582,
      "learning_rate": 4.864695105993196e-06,
      "loss": 0.3677,
      "step": 2273
    },
    {
      "epoch": 0.05952152442110039,
      "grad_norm": 19.944690704345703,
      "learning_rate": 4.8615545668673125e-06,
      "loss": 0.4532,
      "step": 2274
    },
    {
      "epoch": 0.05954769923395048,
      "grad_norm": 15.194948196411133,
      "learning_rate": 4.858414027741429e-06,
      "loss": 0.2185,
      "step": 2275
    },
    {
      "epoch": 0.05957387404680056,
      "grad_norm": 22.04694366455078,
      "learning_rate": 4.855273488615546e-06,
      "loss": 0.3953,
      "step": 2276
    },
    {
      "epoch": 0.05960004885965065,
      "grad_norm": 22.552505493164062,
      "learning_rate": 4.852132949489663e-06,
      "loss": 0.5633,
      "step": 2277
    },
    {
      "epoch": 0.05962622367250074,
      "grad_norm": 27.655763626098633,
      "learning_rate": 4.848992410363779e-06,
      "loss": 0.4923,
      "step": 2278
    },
    {
      "epoch": 0.05965239848535083,
      "grad_norm": 22.040605545043945,
      "learning_rate": 4.845851871237896e-06,
      "loss": 0.4334,
      "step": 2279
    },
    {
      "epoch": 0.05967857329820092,
      "grad_norm": 27.63294219970703,
      "learning_rate": 4.842711332112013e-06,
      "loss": 0.5975,
      "step": 2280
    },
    {
      "epoch": 0.059704748111051004,
      "grad_norm": 18.70258331298828,
      "learning_rate": 4.8395707929861295e-06,
      "loss": 0.4726,
      "step": 2281
    },
    {
      "epoch": 0.05973092292390109,
      "grad_norm": 21.40654182434082,
      "learning_rate": 4.836430253860246e-06,
      "loss": 0.4933,
      "step": 2282
    },
    {
      "epoch": 0.05975709773675118,
      "grad_norm": 18.055370330810547,
      "learning_rate": 4.833289714734363e-06,
      "loss": 0.4516,
      "step": 2283
    },
    {
      "epoch": 0.05978327254960127,
      "grad_norm": 31.077096939086914,
      "learning_rate": 4.8301491756084796e-06,
      "loss": 0.6538,
      "step": 2284
    },
    {
      "epoch": 0.059809447362451355,
      "grad_norm": 18.513185501098633,
      "learning_rate": 4.827008636482596e-06,
      "loss": 0.3138,
      "step": 2285
    },
    {
      "epoch": 0.059835622175301445,
      "grad_norm": 23.1224308013916,
      "learning_rate": 4.823868097356713e-06,
      "loss": 0.3812,
      "step": 2286
    },
    {
      "epoch": 0.059861796988151535,
      "grad_norm": 14.453357696533203,
      "learning_rate": 4.82072755823083e-06,
      "loss": 0.3294,
      "step": 2287
    },
    {
      "epoch": 0.059887971801001624,
      "grad_norm": 7.509631156921387,
      "learning_rate": 4.817587019104946e-06,
      "loss": 0.2194,
      "step": 2288
    },
    {
      "epoch": 0.059914146613851714,
      "grad_norm": 21.047657012939453,
      "learning_rate": 4.814446479979063e-06,
      "loss": 0.5727,
      "step": 2289
    },
    {
      "epoch": 0.0599403214267018,
      "grad_norm": 25.61353874206543,
      "learning_rate": 4.81130594085318e-06,
      "loss": 0.5435,
      "step": 2290
    },
    {
      "epoch": 0.059966496239551886,
      "grad_norm": 13.316302299499512,
      "learning_rate": 4.8081654017272965e-06,
      "loss": 0.3775,
      "step": 2291
    },
    {
      "epoch": 0.059992671052401976,
      "grad_norm": 22.657146453857422,
      "learning_rate": 4.805024862601413e-06,
      "loss": 0.4489,
      "step": 2292
    },
    {
      "epoch": 0.060018845865252066,
      "grad_norm": 13.978827476501465,
      "learning_rate": 4.80188432347553e-06,
      "loss": 0.2755,
      "step": 2293
    },
    {
      "epoch": 0.06004502067810215,
      "grad_norm": 24.893810272216797,
      "learning_rate": 4.7987437843496475e-06,
      "loss": 0.3307,
      "step": 2294
    },
    {
      "epoch": 0.06007119549095224,
      "grad_norm": 23.275020599365234,
      "learning_rate": 4.795603245223763e-06,
      "loss": 0.5089,
      "step": 2295
    },
    {
      "epoch": 0.06009737030380233,
      "grad_norm": 14.908562660217285,
      "learning_rate": 4.79246270609788e-06,
      "loss": 0.2476,
      "step": 2296
    },
    {
      "epoch": 0.06012354511665242,
      "grad_norm": 16.14305877685547,
      "learning_rate": 4.789322166971997e-06,
      "loss": 0.427,
      "step": 2297
    },
    {
      "epoch": 0.06014971992950251,
      "grad_norm": 15.329513549804688,
      "learning_rate": 4.7861816278461135e-06,
      "loss": 0.3598,
      "step": 2298
    },
    {
      "epoch": 0.06017589474235259,
      "grad_norm": 12.259956359863281,
      "learning_rate": 4.78304108872023e-06,
      "loss": 0.1261,
      "step": 2299
    },
    {
      "epoch": 0.06020206955520268,
      "grad_norm": 13.370216369628906,
      "learning_rate": 4.779900549594348e-06,
      "loss": 0.2771,
      "step": 2300
    },
    {
      "epoch": 0.06022824436805277,
      "grad_norm": 22.284299850463867,
      "learning_rate": 4.7767600104684644e-06,
      "loss": 0.3212,
      "step": 2301
    },
    {
      "epoch": 0.06025441918090286,
      "grad_norm": 38.61958694458008,
      "learning_rate": 4.773619471342581e-06,
      "loss": 0.4736,
      "step": 2302
    },
    {
      "epoch": 0.06028059399375294,
      "grad_norm": 21.037540435791016,
      "learning_rate": 4.770478932216697e-06,
      "loss": 0.2713,
      "step": 2303
    },
    {
      "epoch": 0.06030676880660303,
      "grad_norm": 26.51718521118164,
      "learning_rate": 4.767338393090814e-06,
      "loss": 0.6908,
      "step": 2304
    },
    {
      "epoch": 0.06033294361945312,
      "grad_norm": 17.58550262451172,
      "learning_rate": 4.76419785396493e-06,
      "loss": 0.1902,
      "step": 2305
    },
    {
      "epoch": 0.06035911843230321,
      "grad_norm": 21.423290252685547,
      "learning_rate": 4.761057314839047e-06,
      "loss": 0.5143,
      "step": 2306
    },
    {
      "epoch": 0.0603852932451533,
      "grad_norm": 18.851224899291992,
      "learning_rate": 4.757916775713165e-06,
      "loss": 0.4084,
      "step": 2307
    },
    {
      "epoch": 0.06041146805800338,
      "grad_norm": 16.31719398498535,
      "learning_rate": 4.754776236587281e-06,
      "loss": 0.6126,
      "step": 2308
    },
    {
      "epoch": 0.06043764287085347,
      "grad_norm": 13.999140739440918,
      "learning_rate": 4.751635697461398e-06,
      "loss": 0.3313,
      "step": 2309
    },
    {
      "epoch": 0.06046381768370356,
      "grad_norm": 24.683277130126953,
      "learning_rate": 4.748495158335514e-06,
      "loss": 0.4201,
      "step": 2310
    },
    {
      "epoch": 0.06048999249655365,
      "grad_norm": 28.348047256469727,
      "learning_rate": 4.745354619209631e-06,
      "loss": 0.3818,
      "step": 2311
    },
    {
      "epoch": 0.060516167309403734,
      "grad_norm": 20.209360122680664,
      "learning_rate": 4.742214080083747e-06,
      "loss": 0.4574,
      "step": 2312
    },
    {
      "epoch": 0.060542342122253824,
      "grad_norm": 11.452337265014648,
      "learning_rate": 4.739073540957865e-06,
      "loss": 0.179,
      "step": 2313
    },
    {
      "epoch": 0.060568516935103914,
      "grad_norm": 16.067829132080078,
      "learning_rate": 4.735933001831982e-06,
      "loss": 0.2583,
      "step": 2314
    },
    {
      "epoch": 0.060594691747954,
      "grad_norm": 9.141051292419434,
      "learning_rate": 4.732792462706098e-06,
      "loss": 0.1701,
      "step": 2315
    },
    {
      "epoch": 0.06062086656080409,
      "grad_norm": 20.76168441772461,
      "learning_rate": 4.729651923580215e-06,
      "loss": 0.4146,
      "step": 2316
    },
    {
      "epoch": 0.060647041373654176,
      "grad_norm": 16.55156135559082,
      "learning_rate": 4.726511384454332e-06,
      "loss": 0.3982,
      "step": 2317
    },
    {
      "epoch": 0.060673216186504265,
      "grad_norm": 21.600128173828125,
      "learning_rate": 4.723370845328448e-06,
      "loss": 0.453,
      "step": 2318
    },
    {
      "epoch": 0.060699390999354355,
      "grad_norm": 11.63177490234375,
      "learning_rate": 4.720230306202564e-06,
      "loss": 0.3803,
      "step": 2319
    },
    {
      "epoch": 0.060725565812204445,
      "grad_norm": 24.580791473388672,
      "learning_rate": 4.717089767076682e-06,
      "loss": 0.5848,
      "step": 2320
    },
    {
      "epoch": 0.06075174062505453,
      "grad_norm": 22.21952247619629,
      "learning_rate": 4.7139492279507985e-06,
      "loss": 0.504,
      "step": 2321
    },
    {
      "epoch": 0.06077791543790462,
      "grad_norm": 14.48661994934082,
      "learning_rate": 4.710808688824915e-06,
      "loss": 0.304,
      "step": 2322
    },
    {
      "epoch": 0.06080409025075471,
      "grad_norm": 27.815811157226562,
      "learning_rate": 4.707668149699032e-06,
      "loss": 0.4357,
      "step": 2323
    },
    {
      "epoch": 0.060830265063604796,
      "grad_norm": 14.089852333068848,
      "learning_rate": 4.704527610573149e-06,
      "loss": 0.4785,
      "step": 2324
    },
    {
      "epoch": 0.060856439876454886,
      "grad_norm": 14.844074249267578,
      "learning_rate": 4.701387071447265e-06,
      "loss": 0.2847,
      "step": 2325
    },
    {
      "epoch": 0.06088261468930497,
      "grad_norm": 17.788761138916016,
      "learning_rate": 4.698246532321382e-06,
      "loss": 0.3281,
      "step": 2326
    },
    {
      "epoch": 0.06090878950215506,
      "grad_norm": 16.691295623779297,
      "learning_rate": 4.695105993195499e-06,
      "loss": 0.3742,
      "step": 2327
    },
    {
      "epoch": 0.06093496431500515,
      "grad_norm": 15.27404499053955,
      "learning_rate": 4.6919654540696155e-06,
      "loss": 0.2854,
      "step": 2328
    },
    {
      "epoch": 0.06096113912785524,
      "grad_norm": 20.9449405670166,
      "learning_rate": 4.688824914943732e-06,
      "loss": 0.382,
      "step": 2329
    },
    {
      "epoch": 0.06098731394070532,
      "grad_norm": 27.538047790527344,
      "learning_rate": 4.685684375817849e-06,
      "loss": 0.4659,
      "step": 2330
    },
    {
      "epoch": 0.06101348875355541,
      "grad_norm": 17.909889221191406,
      "learning_rate": 4.682543836691966e-06,
      "loss": 0.2803,
      "step": 2331
    },
    {
      "epoch": 0.0610396635664055,
      "grad_norm": 24.168352127075195,
      "learning_rate": 4.679403297566082e-06,
      "loss": 0.4153,
      "step": 2332
    },
    {
      "epoch": 0.06106583837925559,
      "grad_norm": 23.18195152282715,
      "learning_rate": 4.676262758440199e-06,
      "loss": 0.3391,
      "step": 2333
    },
    {
      "epoch": 0.06109201319210568,
      "grad_norm": 14.340374946594238,
      "learning_rate": 4.673122219314316e-06,
      "loss": 0.3577,
      "step": 2334
    },
    {
      "epoch": 0.06111818800495576,
      "grad_norm": 17.001585006713867,
      "learning_rate": 4.6699816801884324e-06,
      "loss": 0.312,
      "step": 2335
    },
    {
      "epoch": 0.06114436281780585,
      "grad_norm": 23.867055892944336,
      "learning_rate": 4.666841141062549e-06,
      "loss": 0.579,
      "step": 2336
    },
    {
      "epoch": 0.06117053763065594,
      "grad_norm": 28.446298599243164,
      "learning_rate": 4.663700601936666e-06,
      "loss": 0.5449,
      "step": 2337
    },
    {
      "epoch": 0.06119671244350603,
      "grad_norm": 19.61847496032715,
      "learning_rate": 4.6605600628107826e-06,
      "loss": 0.51,
      "step": 2338
    },
    {
      "epoch": 0.06122288725635612,
      "grad_norm": 29.29342269897461,
      "learning_rate": 4.657419523684899e-06,
      "loss": 0.506,
      "step": 2339
    },
    {
      "epoch": 0.0612490620692062,
      "grad_norm": 23.75014305114746,
      "learning_rate": 4.654278984559016e-06,
      "loss": 0.4071,
      "step": 2340
    },
    {
      "epoch": 0.06127523688205629,
      "grad_norm": 18.72879981994629,
      "learning_rate": 4.651138445433133e-06,
      "loss": 0.3697,
      "step": 2341
    },
    {
      "epoch": 0.06130141169490638,
      "grad_norm": 16.650407791137695,
      "learning_rate": 4.647997906307249e-06,
      "loss": 0.4004,
      "step": 2342
    },
    {
      "epoch": 0.06132758650775647,
      "grad_norm": 22.892385482788086,
      "learning_rate": 4.644857367181366e-06,
      "loss": 0.3998,
      "step": 2343
    },
    {
      "epoch": 0.061353761320606555,
      "grad_norm": 31.594030380249023,
      "learning_rate": 4.641716828055483e-06,
      "loss": 0.5389,
      "step": 2344
    },
    {
      "epoch": 0.061379936133456645,
      "grad_norm": 12.668462753295898,
      "learning_rate": 4.6385762889295995e-06,
      "loss": 0.2396,
      "step": 2345
    },
    {
      "epoch": 0.061406110946306734,
      "grad_norm": 18.031919479370117,
      "learning_rate": 4.635435749803716e-06,
      "loss": 0.2894,
      "step": 2346
    },
    {
      "epoch": 0.061432285759156824,
      "grad_norm": 18.81981658935547,
      "learning_rate": 4.632295210677834e-06,
      "loss": 0.2816,
      "step": 2347
    },
    {
      "epoch": 0.061458460572006914,
      "grad_norm": 24.10572624206543,
      "learning_rate": 4.6291546715519505e-06,
      "loss": 0.42,
      "step": 2348
    },
    {
      "epoch": 0.061484635384856996,
      "grad_norm": 25.993654251098633,
      "learning_rate": 4.626014132426066e-06,
      "loss": 0.4017,
      "step": 2349
    },
    {
      "epoch": 0.061510810197707086,
      "grad_norm": 16.27717399597168,
      "learning_rate": 4.622873593300183e-06,
      "loss": 0.3358,
      "step": 2350
    },
    {
      "epoch": 0.061536985010557176,
      "grad_norm": 25.73367691040039,
      "learning_rate": 4.6197330541743e-06,
      "loss": 0.5143,
      "step": 2351
    },
    {
      "epoch": 0.061563159823407265,
      "grad_norm": 17.06876564025879,
      "learning_rate": 4.6165925150484164e-06,
      "loss": 0.4254,
      "step": 2352
    },
    {
      "epoch": 0.06158933463625735,
      "grad_norm": 14.134662628173828,
      "learning_rate": 4.613451975922533e-06,
      "loss": 0.265,
      "step": 2353
    },
    {
      "epoch": 0.06161550944910744,
      "grad_norm": 12.83560562133789,
      "learning_rate": 4.610311436796651e-06,
      "loss": 0.3125,
      "step": 2354
    },
    {
      "epoch": 0.06164168426195753,
      "grad_norm": 19.544689178466797,
      "learning_rate": 4.607170897670767e-06,
      "loss": 0.2462,
      "step": 2355
    },
    {
      "epoch": 0.06166785907480762,
      "grad_norm": 17.261463165283203,
      "learning_rate": 4.604030358544884e-06,
      "loss": 0.3947,
      "step": 2356
    },
    {
      "epoch": 0.06169403388765771,
      "grad_norm": 26.88421058654785,
      "learning_rate": 4.600889819419e-06,
      "loss": 0.3374,
      "step": 2357
    },
    {
      "epoch": 0.06172020870050779,
      "grad_norm": 26.412242889404297,
      "learning_rate": 4.597749280293117e-06,
      "loss": 0.4859,
      "step": 2358
    },
    {
      "epoch": 0.06174638351335788,
      "grad_norm": 30.48992919921875,
      "learning_rate": 4.594608741167233e-06,
      "loss": 0.5987,
      "step": 2359
    },
    {
      "epoch": 0.06177255832620797,
      "grad_norm": 16.058446884155273,
      "learning_rate": 4.591468202041351e-06,
      "loss": 0.3701,
      "step": 2360
    },
    {
      "epoch": 0.06179873313905806,
      "grad_norm": 21.365224838256836,
      "learning_rate": 4.588327662915468e-06,
      "loss": 0.3475,
      "step": 2361
    },
    {
      "epoch": 0.06182490795190814,
      "grad_norm": 17.07310676574707,
      "learning_rate": 4.585187123789584e-06,
      "loss": 0.4445,
      "step": 2362
    },
    {
      "epoch": 0.06185108276475823,
      "grad_norm": 15.873931884765625,
      "learning_rate": 4.582046584663701e-06,
      "loss": 0.3115,
      "step": 2363
    },
    {
      "epoch": 0.06187725757760832,
      "grad_norm": 27.413110733032227,
      "learning_rate": 4.578906045537817e-06,
      "loss": 0.4197,
      "step": 2364
    },
    {
      "epoch": 0.06190343239045841,
      "grad_norm": 21.008066177368164,
      "learning_rate": 4.575765506411934e-06,
      "loss": 0.2338,
      "step": 2365
    },
    {
      "epoch": 0.0619296072033085,
      "grad_norm": 13.712997436523438,
      "learning_rate": 4.57262496728605e-06,
      "loss": 0.4852,
      "step": 2366
    },
    {
      "epoch": 0.06195578201615858,
      "grad_norm": 16.328746795654297,
      "learning_rate": 4.569484428160168e-06,
      "loss": 0.344,
      "step": 2367
    },
    {
      "epoch": 0.06198195682900867,
      "grad_norm": 18.21456527709961,
      "learning_rate": 4.566343889034285e-06,
      "loss": 0.4342,
      "step": 2368
    },
    {
      "epoch": 0.06200813164185876,
      "grad_norm": 19.34034538269043,
      "learning_rate": 4.563203349908401e-06,
      "loss": 0.2058,
      "step": 2369
    },
    {
      "epoch": 0.06203430645470885,
      "grad_norm": 16.35610008239746,
      "learning_rate": 4.560062810782518e-06,
      "loss": 0.3521,
      "step": 2370
    },
    {
      "epoch": 0.062060481267558934,
      "grad_norm": 25.045406341552734,
      "learning_rate": 4.556922271656635e-06,
      "loss": 0.3947,
      "step": 2371
    },
    {
      "epoch": 0.062086656080409024,
      "grad_norm": 21.87718963623047,
      "learning_rate": 4.5537817325307506e-06,
      "loss": 0.2958,
      "step": 2372
    },
    {
      "epoch": 0.06211283089325911,
      "grad_norm": 19.68260955810547,
      "learning_rate": 4.550641193404868e-06,
      "loss": 0.3777,
      "step": 2373
    },
    {
      "epoch": 0.0621390057061092,
      "grad_norm": 29.77998924255371,
      "learning_rate": 4.547500654278985e-06,
      "loss": 0.6253,
      "step": 2374
    },
    {
      "epoch": 0.06216518051895929,
      "grad_norm": 22.839588165283203,
      "learning_rate": 4.5443601151531015e-06,
      "loss": 0.414,
      "step": 2375
    },
    {
      "epoch": 0.062191355331809375,
      "grad_norm": 20.35675048828125,
      "learning_rate": 4.541219576027218e-06,
      "loss": 0.4843,
      "step": 2376
    },
    {
      "epoch": 0.062217530144659465,
      "grad_norm": 22.32573127746582,
      "learning_rate": 4.538079036901335e-06,
      "loss": 0.5352,
      "step": 2377
    },
    {
      "epoch": 0.062243704957509555,
      "grad_norm": 24.884401321411133,
      "learning_rate": 4.534938497775452e-06,
      "loss": 0.3475,
      "step": 2378
    },
    {
      "epoch": 0.062269879770359644,
      "grad_norm": 10.011404037475586,
      "learning_rate": 4.531797958649568e-06,
      "loss": 0.2293,
      "step": 2379
    },
    {
      "epoch": 0.06229605458320973,
      "grad_norm": 12.522443771362305,
      "learning_rate": 4.528657419523685e-06,
      "loss": 0.2302,
      "step": 2380
    },
    {
      "epoch": 0.06232222939605982,
      "grad_norm": 27.711313247680664,
      "learning_rate": 4.525516880397802e-06,
      "loss": 0.5533,
      "step": 2381
    },
    {
      "epoch": 0.062348404208909906,
      "grad_norm": 24.685752868652344,
      "learning_rate": 4.5223763412719185e-06,
      "loss": 0.3971,
      "step": 2382
    },
    {
      "epoch": 0.062374579021759996,
      "grad_norm": 19.96586799621582,
      "learning_rate": 4.519235802146035e-06,
      "loss": 0.3877,
      "step": 2383
    },
    {
      "epoch": 0.062400753834610086,
      "grad_norm": 14.3150634765625,
      "learning_rate": 4.516095263020152e-06,
      "loss": 0.3135,
      "step": 2384
    },
    {
      "epoch": 0.06242692864746017,
      "grad_norm": 13.001672744750977,
      "learning_rate": 4.512954723894269e-06,
      "loss": 0.1605,
      "step": 2385
    },
    {
      "epoch": 0.06245310346031026,
      "grad_norm": 24.023637771606445,
      "learning_rate": 4.509814184768385e-06,
      "loss": 0.5247,
      "step": 2386
    },
    {
      "epoch": 0.06247927827316035,
      "grad_norm": 26.73920249938965,
      "learning_rate": 4.506673645642502e-06,
      "loss": 0.6013,
      "step": 2387
    },
    {
      "epoch": 0.06250545308601044,
      "grad_norm": 16.518295288085938,
      "learning_rate": 4.503533106516619e-06,
      "loss": 0.3591,
      "step": 2388
    },
    {
      "epoch": 0.06253162789886052,
      "grad_norm": 32.3651123046875,
      "learning_rate": 4.500392567390735e-06,
      "loss": 0.4237,
      "step": 2389
    },
    {
      "epoch": 0.06255780271171062,
      "grad_norm": 24.047199249267578,
      "learning_rate": 4.497252028264852e-06,
      "loss": 0.5737,
      "step": 2390
    },
    {
      "epoch": 0.0625839775245607,
      "grad_norm": 30.263317108154297,
      "learning_rate": 4.494111489138969e-06,
      "loss": 0.5852,
      "step": 2391
    },
    {
      "epoch": 0.06261015233741078,
      "grad_norm": 29.953575134277344,
      "learning_rate": 4.4909709500130855e-06,
      "loss": 0.3091,
      "step": 2392
    },
    {
      "epoch": 0.06263632715026088,
      "grad_norm": 21.214265823364258,
      "learning_rate": 4.487830410887202e-06,
      "loss": 0.2463,
      "step": 2393
    },
    {
      "epoch": 0.06266250196311096,
      "grad_norm": 22.788541793823242,
      "learning_rate": 4.48468987176132e-06,
      "loss": 0.5071,
      "step": 2394
    },
    {
      "epoch": 0.06268867677596106,
      "grad_norm": 12.896437644958496,
      "learning_rate": 4.481549332635436e-06,
      "loss": 0.2409,
      "step": 2395
    },
    {
      "epoch": 0.06271485158881114,
      "grad_norm": 17.01666259765625,
      "learning_rate": 4.478408793509552e-06,
      "loss": 0.3886,
      "step": 2396
    },
    {
      "epoch": 0.06274102640166122,
      "grad_norm": 21.009815216064453,
      "learning_rate": 4.475268254383669e-06,
      "loss": 0.2196,
      "step": 2397
    },
    {
      "epoch": 0.06276720121451132,
      "grad_norm": 18.242151260375977,
      "learning_rate": 4.472127715257786e-06,
      "loss": 0.3977,
      "step": 2398
    },
    {
      "epoch": 0.0627933760273614,
      "grad_norm": 23.34393310546875,
      "learning_rate": 4.4689871761319025e-06,
      "loss": 0.3987,
      "step": 2399
    },
    {
      "epoch": 0.0628195508402115,
      "grad_norm": 20.49034881591797,
      "learning_rate": 4.465846637006019e-06,
      "loss": 0.3303,
      "step": 2400
    },
    {
      "epoch": 0.06284572565306158,
      "grad_norm": 21.165782928466797,
      "learning_rate": 4.462706097880137e-06,
      "loss": 0.3225,
      "step": 2401
    },
    {
      "epoch": 0.06287190046591166,
      "grad_norm": 20.565454483032227,
      "learning_rate": 4.4595655587542534e-06,
      "loss": 0.3847,
      "step": 2402
    },
    {
      "epoch": 0.06289807527876176,
      "grad_norm": 29.866153717041016,
      "learning_rate": 4.456425019628369e-06,
      "loss": 0.4584,
      "step": 2403
    },
    {
      "epoch": 0.06292425009161184,
      "grad_norm": 30.66123390197754,
      "learning_rate": 4.453284480502486e-06,
      "loss": 0.3807,
      "step": 2404
    },
    {
      "epoch": 0.06295042490446193,
      "grad_norm": 12.59261417388916,
      "learning_rate": 4.450143941376603e-06,
      "loss": 0.2359,
      "step": 2405
    },
    {
      "epoch": 0.06297659971731202,
      "grad_norm": 13.850491523742676,
      "learning_rate": 4.4470034022507194e-06,
      "loss": 0.2558,
      "step": 2406
    },
    {
      "epoch": 0.0630027745301621,
      "grad_norm": 20.527366638183594,
      "learning_rate": 4.443862863124837e-06,
      "loss": 0.3347,
      "step": 2407
    },
    {
      "epoch": 0.0630289493430122,
      "grad_norm": 10.965956687927246,
      "learning_rate": 4.440722323998954e-06,
      "loss": 0.2765,
      "step": 2408
    },
    {
      "epoch": 0.06305512415586229,
      "grad_norm": 14.632637023925781,
      "learning_rate": 4.43758178487307e-06,
      "loss": 0.3273,
      "step": 2409
    },
    {
      "epoch": 0.06308129896871237,
      "grad_norm": 13.659649848937988,
      "learning_rate": 4.434441245747187e-06,
      "loss": 0.2485,
      "step": 2410
    },
    {
      "epoch": 0.06310747378156246,
      "grad_norm": 27.351715087890625,
      "learning_rate": 4.431300706621303e-06,
      "loss": 0.158,
      "step": 2411
    },
    {
      "epoch": 0.06313364859441255,
      "grad_norm": 15.344744682312012,
      "learning_rate": 4.42816016749542e-06,
      "loss": 0.4154,
      "step": 2412
    },
    {
      "epoch": 0.06315982340726264,
      "grad_norm": 12.355428695678711,
      "learning_rate": 4.425019628369536e-06,
      "loss": 0.156,
      "step": 2413
    },
    {
      "epoch": 0.06318599822011273,
      "grad_norm": 16.041303634643555,
      "learning_rate": 4.421879089243654e-06,
      "loss": 0.4161,
      "step": 2414
    },
    {
      "epoch": 0.06321217303296281,
      "grad_norm": 14.571477890014648,
      "learning_rate": 4.418738550117771e-06,
      "loss": 0.3593,
      "step": 2415
    },
    {
      "epoch": 0.0632383478458129,
      "grad_norm": 24.627117156982422,
      "learning_rate": 4.415598010991887e-06,
      "loss": 0.2397,
      "step": 2416
    },
    {
      "epoch": 0.06326452265866299,
      "grad_norm": 27.115978240966797,
      "learning_rate": 4.412457471866004e-06,
      "loss": 0.5421,
      "step": 2417
    },
    {
      "epoch": 0.06329069747151309,
      "grad_norm": 23.969375610351562,
      "learning_rate": 4.409316932740121e-06,
      "loss": 0.4114,
      "step": 2418
    },
    {
      "epoch": 0.06331687228436317,
      "grad_norm": 27.61382293701172,
      "learning_rate": 4.406176393614237e-06,
      "loss": 0.4472,
      "step": 2419
    },
    {
      "epoch": 0.06334304709721325,
      "grad_norm": 16.86248779296875,
      "learning_rate": 4.403035854488354e-06,
      "loss": 0.278,
      "step": 2420
    },
    {
      "epoch": 0.06336922191006335,
      "grad_norm": 15.093748092651367,
      "learning_rate": 4.399895315362471e-06,
      "loss": 0.334,
      "step": 2421
    },
    {
      "epoch": 0.06339539672291343,
      "grad_norm": 19.63490867614746,
      "learning_rate": 4.3967547762365876e-06,
      "loss": 0.329,
      "step": 2422
    },
    {
      "epoch": 0.06342157153576351,
      "grad_norm": 21.7840518951416,
      "learning_rate": 4.393614237110704e-06,
      "loss": 0.4248,
      "step": 2423
    },
    {
      "epoch": 0.06344774634861361,
      "grad_norm": 21.9702091217041,
      "learning_rate": 4.390473697984821e-06,
      "loss": 0.4787,
      "step": 2424
    },
    {
      "epoch": 0.06347392116146369,
      "grad_norm": 11.289648056030273,
      "learning_rate": 4.387333158858938e-06,
      "loss": 0.3568,
      "step": 2425
    },
    {
      "epoch": 0.06350009597431379,
      "grad_norm": 22.55670738220215,
      "learning_rate": 4.3841926197330536e-06,
      "loss": 0.4416,
      "step": 2426
    },
    {
      "epoch": 0.06352627078716387,
      "grad_norm": 24.93673324584961,
      "learning_rate": 4.381052080607171e-06,
      "loss": 0.4537,
      "step": 2427
    },
    {
      "epoch": 0.06355244560001395,
      "grad_norm": 28.74759864807129,
      "learning_rate": 4.377911541481288e-06,
      "loss": 0.6408,
      "step": 2428
    },
    {
      "epoch": 0.06357862041286405,
      "grad_norm": 23.262611389160156,
      "learning_rate": 4.3747710023554045e-06,
      "loss": 0.5367,
      "step": 2429
    },
    {
      "epoch": 0.06360479522571413,
      "grad_norm": 28.095678329467773,
      "learning_rate": 4.371630463229521e-06,
      "loss": 0.4506,
      "step": 2430
    },
    {
      "epoch": 0.06363097003856423,
      "grad_norm": 16.498729705810547,
      "learning_rate": 4.368489924103638e-06,
      "loss": 0.2951,
      "step": 2431
    },
    {
      "epoch": 0.06365714485141431,
      "grad_norm": 22.101585388183594,
      "learning_rate": 4.365349384977755e-06,
      "loss": 0.2357,
      "step": 2432
    },
    {
      "epoch": 0.0636833196642644,
      "grad_norm": 24.781208038330078,
      "learning_rate": 4.362208845851871e-06,
      "loss": 0.5816,
      "step": 2433
    },
    {
      "epoch": 0.06370949447711449,
      "grad_norm": 35.2663688659668,
      "learning_rate": 4.359068306725988e-06,
      "loss": 0.3348,
      "step": 2434
    },
    {
      "epoch": 0.06373566928996457,
      "grad_norm": 26.866697311401367,
      "learning_rate": 4.355927767600105e-06,
      "loss": 0.2888,
      "step": 2435
    },
    {
      "epoch": 0.06376184410281467,
      "grad_norm": 19.26416015625,
      "learning_rate": 4.3527872284742215e-06,
      "loss": 0.512,
      "step": 2436
    },
    {
      "epoch": 0.06378801891566475,
      "grad_norm": 16.22312355041504,
      "learning_rate": 4.349646689348338e-06,
      "loss": 0.497,
      "step": 2437
    },
    {
      "epoch": 0.06381419372851484,
      "grad_norm": 15.472418785095215,
      "learning_rate": 4.346506150222455e-06,
      "loss": 0.3112,
      "step": 2438
    },
    {
      "epoch": 0.06384036854136493,
      "grad_norm": 15.83409309387207,
      "learning_rate": 4.343365611096572e-06,
      "loss": 0.2934,
      "step": 2439
    },
    {
      "epoch": 0.06386654335421502,
      "grad_norm": 19.250831604003906,
      "learning_rate": 4.340225071970688e-06,
      "loss": 0.2534,
      "step": 2440
    },
    {
      "epoch": 0.0638927181670651,
      "grad_norm": 18.725080490112305,
      "learning_rate": 4.337084532844806e-06,
      "loss": 0.2251,
      "step": 2441
    },
    {
      "epoch": 0.0639188929799152,
      "grad_norm": 14.896206855773926,
      "learning_rate": 4.333943993718922e-06,
      "loss": 0.2415,
      "step": 2442
    },
    {
      "epoch": 0.06394506779276528,
      "grad_norm": 20.81050682067871,
      "learning_rate": 4.330803454593038e-06,
      "loss": 0.3398,
      "step": 2443
    },
    {
      "epoch": 0.06397124260561537,
      "grad_norm": 18.765029907226562,
      "learning_rate": 4.327662915467155e-06,
      "loss": 0.3747,
      "step": 2444
    },
    {
      "epoch": 0.06399741741846546,
      "grad_norm": 11.777022361755371,
      "learning_rate": 4.324522376341272e-06,
      "loss": 0.3094,
      "step": 2445
    },
    {
      "epoch": 0.06402359223131554,
      "grad_norm": 14.574971199035645,
      "learning_rate": 4.3213818372153885e-06,
      "loss": 0.2686,
      "step": 2446
    },
    {
      "epoch": 0.06404976704416564,
      "grad_norm": 19.4022216796875,
      "learning_rate": 4.318241298089505e-06,
      "loss": 0.5402,
      "step": 2447
    },
    {
      "epoch": 0.06407594185701572,
      "grad_norm": 19.04385757446289,
      "learning_rate": 4.315100758963623e-06,
      "loss": 0.2929,
      "step": 2448
    },
    {
      "epoch": 0.06410211666986582,
      "grad_norm": 22.414033889770508,
      "learning_rate": 4.3119602198377395e-06,
      "loss": 0.1442,
      "step": 2449
    },
    {
      "epoch": 0.0641282914827159,
      "grad_norm": 35.14970779418945,
      "learning_rate": 4.308819680711855e-06,
      "loss": 0.5101,
      "step": 2450
    },
    {
      "epoch": 0.06415446629556598,
      "grad_norm": 47.21257781982422,
      "learning_rate": 4.305679141585972e-06,
      "loss": 0.5905,
      "step": 2451
    },
    {
      "epoch": 0.06418064110841608,
      "grad_norm": 16.597000122070312,
      "learning_rate": 4.302538602460089e-06,
      "loss": 0.525,
      "step": 2452
    },
    {
      "epoch": 0.06420681592126616,
      "grad_norm": 25.31477165222168,
      "learning_rate": 4.2993980633342055e-06,
      "loss": 0.2256,
      "step": 2453
    },
    {
      "epoch": 0.06423299073411626,
      "grad_norm": 11.24415397644043,
      "learning_rate": 4.296257524208323e-06,
      "loss": 0.2537,
      "step": 2454
    },
    {
      "epoch": 0.06425916554696634,
      "grad_norm": 20.382827758789062,
      "learning_rate": 4.29311698508244e-06,
      "loss": 0.4708,
      "step": 2455
    },
    {
      "epoch": 0.06428534035981642,
      "grad_norm": 21.56345558166504,
      "learning_rate": 4.2899764459565564e-06,
      "loss": 0.3708,
      "step": 2456
    },
    {
      "epoch": 0.06431151517266652,
      "grad_norm": 23.19330596923828,
      "learning_rate": 4.286835906830672e-06,
      "loss": 0.4341,
      "step": 2457
    },
    {
      "epoch": 0.0643376899855166,
      "grad_norm": 18.960294723510742,
      "learning_rate": 4.283695367704789e-06,
      "loss": 0.3337,
      "step": 2458
    },
    {
      "epoch": 0.06436386479836669,
      "grad_norm": 10.910029411315918,
      "learning_rate": 4.280554828578906e-06,
      "loss": 0.2973,
      "step": 2459
    },
    {
      "epoch": 0.06439003961121678,
      "grad_norm": 15.047123908996582,
      "learning_rate": 4.277414289453022e-06,
      "loss": 0.254,
      "step": 2460
    },
    {
      "epoch": 0.06441621442406686,
      "grad_norm": 16.969934463500977,
      "learning_rate": 4.27427375032714e-06,
      "loss": 0.4274,
      "step": 2461
    },
    {
      "epoch": 0.06444238923691696,
      "grad_norm": 23.64444351196289,
      "learning_rate": 4.271133211201257e-06,
      "loss": 0.3175,
      "step": 2462
    },
    {
      "epoch": 0.06446856404976704,
      "grad_norm": 28.56719398498535,
      "learning_rate": 4.267992672075373e-06,
      "loss": 0.5479,
      "step": 2463
    },
    {
      "epoch": 0.06449473886261713,
      "grad_norm": 27.086820602416992,
      "learning_rate": 4.26485213294949e-06,
      "loss": 0.4673,
      "step": 2464
    },
    {
      "epoch": 0.06452091367546722,
      "grad_norm": 26.838363647460938,
      "learning_rate": 4.261711593823606e-06,
      "loss": 0.4541,
      "step": 2465
    },
    {
      "epoch": 0.0645470884883173,
      "grad_norm": 22.352745056152344,
      "learning_rate": 4.258571054697723e-06,
      "loss": 0.2588,
      "step": 2466
    },
    {
      "epoch": 0.0645732633011674,
      "grad_norm": 12.641462326049805,
      "learning_rate": 4.25543051557184e-06,
      "loss": 0.2348,
      "step": 2467
    },
    {
      "epoch": 0.06459943811401749,
      "grad_norm": 15.794898986816406,
      "learning_rate": 4.252289976445957e-06,
      "loss": 0.4104,
      "step": 2468
    },
    {
      "epoch": 0.06462561292686757,
      "grad_norm": 17.17122459411621,
      "learning_rate": 4.249149437320074e-06,
      "loss": 0.2779,
      "step": 2469
    },
    {
      "epoch": 0.06465178773971766,
      "grad_norm": 30.723665237426758,
      "learning_rate": 4.24600889819419e-06,
      "loss": 0.6026,
      "step": 2470
    },
    {
      "epoch": 0.06467796255256775,
      "grad_norm": 13.563246726989746,
      "learning_rate": 4.242868359068307e-06,
      "loss": 0.2391,
      "step": 2471
    },
    {
      "epoch": 0.06470413736541784,
      "grad_norm": 17.589622497558594,
      "learning_rate": 4.239727819942424e-06,
      "loss": 0.3639,
      "step": 2472
    },
    {
      "epoch": 0.06473031217826793,
      "grad_norm": 20.676166534423828,
      "learning_rate": 4.23658728081654e-06,
      "loss": 0.4155,
      "step": 2473
    },
    {
      "epoch": 0.06475648699111801,
      "grad_norm": 20.956045150756836,
      "learning_rate": 4.233446741690657e-06,
      "loss": 0.4167,
      "step": 2474
    },
    {
      "epoch": 0.0647826618039681,
      "grad_norm": 17.791677474975586,
      "learning_rate": 4.230306202564774e-06,
      "loss": 0.4523,
      "step": 2475
    },
    {
      "epoch": 0.06480883661681819,
      "grad_norm": 18.796266555786133,
      "learning_rate": 4.2271656634388906e-06,
      "loss": 0.3117,
      "step": 2476
    },
    {
      "epoch": 0.06483501142966827,
      "grad_norm": 16.30542755126953,
      "learning_rate": 4.224025124313007e-06,
      "loss": 0.2813,
      "step": 2477
    },
    {
      "epoch": 0.06486118624251837,
      "grad_norm": 23.418224334716797,
      "learning_rate": 4.220884585187124e-06,
      "loss": 0.5684,
      "step": 2478
    },
    {
      "epoch": 0.06488736105536845,
      "grad_norm": 23.167388916015625,
      "learning_rate": 4.217744046061241e-06,
      "loss": 0.4614,
      "step": 2479
    },
    {
      "epoch": 0.06491353586821855,
      "grad_norm": 18.705957412719727,
      "learning_rate": 4.214603506935357e-06,
      "loss": 0.4568,
      "step": 2480
    },
    {
      "epoch": 0.06493971068106863,
      "grad_norm": 14.162557601928711,
      "learning_rate": 4.211462967809474e-06,
      "loss": 0.2853,
      "step": 2481
    },
    {
      "epoch": 0.06496588549391871,
      "grad_norm": 18.300077438354492,
      "learning_rate": 4.208322428683591e-06,
      "loss": 0.27,
      "step": 2482
    },
    {
      "epoch": 0.06499206030676881,
      "grad_norm": 10.767967224121094,
      "learning_rate": 4.2051818895577075e-06,
      "loss": 0.1966,
      "step": 2483
    },
    {
      "epoch": 0.06501823511961889,
      "grad_norm": 18.66852378845215,
      "learning_rate": 4.202041350431824e-06,
      "loss": 0.4459,
      "step": 2484
    },
    {
      "epoch": 0.06504440993246899,
      "grad_norm": 13.092920303344727,
      "learning_rate": 4.198900811305941e-06,
      "loss": 0.1747,
      "step": 2485
    },
    {
      "epoch": 0.06507058474531907,
      "grad_norm": 23.19249725341797,
      "learning_rate": 4.195760272180058e-06,
      "loss": 0.4278,
      "step": 2486
    },
    {
      "epoch": 0.06509675955816915,
      "grad_norm": 16.603666305541992,
      "learning_rate": 4.192619733054174e-06,
      "loss": 0.3865,
      "step": 2487
    },
    {
      "epoch": 0.06512293437101925,
      "grad_norm": 16.754323959350586,
      "learning_rate": 4.189479193928291e-06,
      "loss": 0.2675,
      "step": 2488
    },
    {
      "epoch": 0.06514910918386933,
      "grad_norm": 25.52260971069336,
      "learning_rate": 4.186338654802408e-06,
      "loss": 0.557,
      "step": 2489
    },
    {
      "epoch": 0.06517528399671943,
      "grad_norm": 21.458866119384766,
      "learning_rate": 4.1831981156765244e-06,
      "loss": 0.3695,
      "step": 2490
    },
    {
      "epoch": 0.06520145880956951,
      "grad_norm": 15.21574592590332,
      "learning_rate": 4.180057576550641e-06,
      "loss": 0.2772,
      "step": 2491
    },
    {
      "epoch": 0.0652276336224196,
      "grad_norm": 17.416839599609375,
      "learning_rate": 4.176917037424758e-06,
      "loss": 0.3186,
      "step": 2492
    },
    {
      "epoch": 0.06525380843526969,
      "grad_norm": 23.180387496948242,
      "learning_rate": 4.1737764982988746e-06,
      "loss": 0.4351,
      "step": 2493
    },
    {
      "epoch": 0.06527998324811977,
      "grad_norm": 22.108278274536133,
      "learning_rate": 4.170635959172991e-06,
      "loss": 0.4559,
      "step": 2494
    },
    {
      "epoch": 0.06530615806096986,
      "grad_norm": 12.670075416564941,
      "learning_rate": 4.167495420047109e-06,
      "loss": 0.3302,
      "step": 2495
    },
    {
      "epoch": 0.06533233287381995,
      "grad_norm": 16.795265197753906,
      "learning_rate": 4.164354880921225e-06,
      "loss": 0.3873,
      "step": 2496
    },
    {
      "epoch": 0.06535850768667004,
      "grad_norm": 19.969806671142578,
      "learning_rate": 4.161214341795341e-06,
      "loss": 0.396,
      "step": 2497
    },
    {
      "epoch": 0.06538468249952013,
      "grad_norm": 16.817283630371094,
      "learning_rate": 4.158073802669458e-06,
      "loss": 0.1873,
      "step": 2498
    },
    {
      "epoch": 0.06541085731237022,
      "grad_norm": 16.58475685119629,
      "learning_rate": 4.154933263543575e-06,
      "loss": 0.2564,
      "step": 2499
    },
    {
      "epoch": 0.0654370321252203,
      "grad_norm": 24.641481399536133,
      "learning_rate": 4.1517927244176915e-06,
      "loss": 0.3594,
      "step": 2500
    },
    {
      "epoch": 0.0654632069380704,
      "grad_norm": 22.19991111755371,
      "learning_rate": 4.148652185291809e-06,
      "loss": 0.4206,
      "step": 2501
    },
    {
      "epoch": 0.06548938175092048,
      "grad_norm": 14.184181213378906,
      "learning_rate": 4.145511646165926e-06,
      "loss": 0.4219,
      "step": 2502
    },
    {
      "epoch": 0.06551555656377057,
      "grad_norm": 24.23337173461914,
      "learning_rate": 4.1423711070400425e-06,
      "loss": 0.4478,
      "step": 2503
    },
    {
      "epoch": 0.06554173137662066,
      "grad_norm": 19.853485107421875,
      "learning_rate": 4.139230567914158e-06,
      "loss": 0.3866,
      "step": 2504
    },
    {
      "epoch": 0.06556790618947074,
      "grad_norm": 23.780502319335938,
      "learning_rate": 4.136090028788275e-06,
      "loss": 0.3735,
      "step": 2505
    },
    {
      "epoch": 0.06559408100232084,
      "grad_norm": 19.861770629882812,
      "learning_rate": 4.132949489662392e-06,
      "loss": 0.3576,
      "step": 2506
    },
    {
      "epoch": 0.06562025581517092,
      "grad_norm": 17.984142303466797,
      "learning_rate": 4.1298089505365084e-06,
      "loss": 0.3809,
      "step": 2507
    },
    {
      "epoch": 0.06564643062802102,
      "grad_norm": 23.85924530029297,
      "learning_rate": 4.126668411410626e-06,
      "loss": 0.5182,
      "step": 2508
    },
    {
      "epoch": 0.0656726054408711,
      "grad_norm": 26.554073333740234,
      "learning_rate": 4.123527872284743e-06,
      "loss": 0.6488,
      "step": 2509
    },
    {
      "epoch": 0.06569878025372118,
      "grad_norm": 18.74307632446289,
      "learning_rate": 4.120387333158859e-06,
      "loss": 0.3531,
      "step": 2510
    },
    {
      "epoch": 0.06572495506657128,
      "grad_norm": 20.87367820739746,
      "learning_rate": 4.117246794032975e-06,
      "loss": 0.3876,
      "step": 2511
    },
    {
      "epoch": 0.06575112987942136,
      "grad_norm": 25.211669921875,
      "learning_rate": 4.114106254907092e-06,
      "loss": 0.4376,
      "step": 2512
    },
    {
      "epoch": 0.06577730469227146,
      "grad_norm": 13.995809555053711,
      "learning_rate": 4.110965715781209e-06,
      "loss": 0.2785,
      "step": 2513
    },
    {
      "epoch": 0.06580347950512154,
      "grad_norm": 13.259817123413086,
      "learning_rate": 4.107825176655326e-06,
      "loss": 0.1792,
      "step": 2514
    },
    {
      "epoch": 0.06582965431797162,
      "grad_norm": 22.891937255859375,
      "learning_rate": 4.104684637529443e-06,
      "loss": 0.4609,
      "step": 2515
    },
    {
      "epoch": 0.06585582913082172,
      "grad_norm": 20.263927459716797,
      "learning_rate": 4.10154409840356e-06,
      "loss": 0.3414,
      "step": 2516
    },
    {
      "epoch": 0.0658820039436718,
      "grad_norm": 21.39095115661621,
      "learning_rate": 4.098403559277676e-06,
      "loss": 0.529,
      "step": 2517
    },
    {
      "epoch": 0.06590817875652188,
      "grad_norm": 26.12457275390625,
      "learning_rate": 4.095263020151793e-06,
      "loss": 0.6845,
      "step": 2518
    },
    {
      "epoch": 0.06593435356937198,
      "grad_norm": 10.917986869812012,
      "learning_rate": 4.092122481025909e-06,
      "loss": 0.1651,
      "step": 2519
    },
    {
      "epoch": 0.06596052838222206,
      "grad_norm": 22.372861862182617,
      "learning_rate": 4.0889819419000265e-06,
      "loss": 0.4912,
      "step": 2520
    },
    {
      "epoch": 0.06598670319507216,
      "grad_norm": 17.727258682250977,
      "learning_rate": 4.085841402774143e-06,
      "loss": 0.2989,
      "step": 2521
    },
    {
      "epoch": 0.06601287800792224,
      "grad_norm": 16.127235412597656,
      "learning_rate": 4.08270086364826e-06,
      "loss": 0.3471,
      "step": 2522
    },
    {
      "epoch": 0.06603905282077233,
      "grad_norm": 19.394044876098633,
      "learning_rate": 4.079560324522377e-06,
      "loss": 0.3154,
      "step": 2523
    },
    {
      "epoch": 0.06606522763362242,
      "grad_norm": 21.77772331237793,
      "learning_rate": 4.076419785396493e-06,
      "loss": 0.3203,
      "step": 2524
    },
    {
      "epoch": 0.0660914024464725,
      "grad_norm": 18.735275268554688,
      "learning_rate": 4.07327924627061e-06,
      "loss": 0.2933,
      "step": 2525
    },
    {
      "epoch": 0.0661175772593226,
      "grad_norm": 15.684094429016113,
      "learning_rate": 4.070138707144727e-06,
      "loss": 0.1846,
      "step": 2526
    },
    {
      "epoch": 0.06614375207217268,
      "grad_norm": 23.017576217651367,
      "learning_rate": 4.066998168018843e-06,
      "loss": 0.5157,
      "step": 2527
    },
    {
      "epoch": 0.06616992688502277,
      "grad_norm": 22.031789779663086,
      "learning_rate": 4.06385762889296e-06,
      "loss": 0.4383,
      "step": 2528
    },
    {
      "epoch": 0.06619610169787286,
      "grad_norm": 18.604005813598633,
      "learning_rate": 4.060717089767077e-06,
      "loss": 0.2968,
      "step": 2529
    },
    {
      "epoch": 0.06622227651072295,
      "grad_norm": 23.28251075744629,
      "learning_rate": 4.0575765506411935e-06,
      "loss": 0.3673,
      "step": 2530
    },
    {
      "epoch": 0.06624845132357304,
      "grad_norm": 11.070050239562988,
      "learning_rate": 4.05443601151531e-06,
      "loss": 0.1107,
      "step": 2531
    },
    {
      "epoch": 0.06627462613642313,
      "grad_norm": 21.15941047668457,
      "learning_rate": 4.051295472389427e-06,
      "loss": 0.3724,
      "step": 2532
    },
    {
      "epoch": 0.06630080094927321,
      "grad_norm": 18.53551483154297,
      "learning_rate": 4.048154933263544e-06,
      "loss": 0.3671,
      "step": 2533
    },
    {
      "epoch": 0.0663269757621233,
      "grad_norm": 42.658843994140625,
      "learning_rate": 4.04501439413766e-06,
      "loss": 0.538,
      "step": 2534
    },
    {
      "epoch": 0.06635315057497339,
      "grad_norm": 17.991500854492188,
      "learning_rate": 4.041873855011777e-06,
      "loss": 0.2026,
      "step": 2535
    },
    {
      "epoch": 0.06637932538782347,
      "grad_norm": 18.283824920654297,
      "learning_rate": 4.038733315885894e-06,
      "loss": 0.2193,
      "step": 2536
    },
    {
      "epoch": 0.06640550020067357,
      "grad_norm": 24.300832748413086,
      "learning_rate": 4.0355927767600105e-06,
      "loss": 0.4264,
      "step": 2537
    },
    {
      "epoch": 0.06643167501352365,
      "grad_norm": 23.524263381958008,
      "learning_rate": 4.032452237634127e-06,
      "loss": 0.5109,
      "step": 2538
    },
    {
      "epoch": 0.06645784982637375,
      "grad_norm": 28.021907806396484,
      "learning_rate": 4.029311698508244e-06,
      "loss": 0.6811,
      "step": 2539
    },
    {
      "epoch": 0.06648402463922383,
      "grad_norm": 17.641860961914062,
      "learning_rate": 4.026171159382361e-06,
      "loss": 0.2978,
      "step": 2540
    },
    {
      "epoch": 0.06651019945207391,
      "grad_norm": 24.225433349609375,
      "learning_rate": 4.023030620256477e-06,
      "loss": 0.4661,
      "step": 2541
    },
    {
      "epoch": 0.06653637426492401,
      "grad_norm": 33.61189651489258,
      "learning_rate": 4.019890081130594e-06,
      "loss": 0.6852,
      "step": 2542
    },
    {
      "epoch": 0.06656254907777409,
      "grad_norm": 25.810564041137695,
      "learning_rate": 4.016749542004711e-06,
      "loss": 0.3911,
      "step": 2543
    },
    {
      "epoch": 0.06658872389062419,
      "grad_norm": 19.51397132873535,
      "learning_rate": 4.0136090028788274e-06,
      "loss": 0.419,
      "step": 2544
    },
    {
      "epoch": 0.06661489870347427,
      "grad_norm": 23.904821395874023,
      "learning_rate": 4.010468463752944e-06,
      "loss": 0.3523,
      "step": 2545
    },
    {
      "epoch": 0.06664107351632435,
      "grad_norm": 18.1219425201416,
      "learning_rate": 4.007327924627061e-06,
      "loss": 0.3206,
      "step": 2546
    },
    {
      "epoch": 0.06666724832917445,
      "grad_norm": 14.725926399230957,
      "learning_rate": 4.0041873855011775e-06,
      "loss": 0.3725,
      "step": 2547
    },
    {
      "epoch": 0.06669342314202453,
      "grad_norm": 19.058259963989258,
      "learning_rate": 4.001046846375295e-06,
      "loss": 0.3616,
      "step": 2548
    },
    {
      "epoch": 0.06671959795487463,
      "grad_norm": 25.43714714050293,
      "learning_rate": 3.997906307249412e-06,
      "loss": 0.4878,
      "step": 2549
    },
    {
      "epoch": 0.06674577276772471,
      "grad_norm": 20.22097396850586,
      "learning_rate": 3.994765768123528e-06,
      "loss": 0.3051,
      "step": 2550
    },
    {
      "epoch": 0.0667719475805748,
      "grad_norm": 19.46816062927246,
      "learning_rate": 3.991625228997644e-06,
      "loss": 0.5828,
      "step": 2551
    },
    {
      "epoch": 0.06679812239342489,
      "grad_norm": 20.610328674316406,
      "learning_rate": 3.988484689871761e-06,
      "loss": 0.1753,
      "step": 2552
    },
    {
      "epoch": 0.06682429720627497,
      "grad_norm": 16.637975692749023,
      "learning_rate": 3.985344150745878e-06,
      "loss": 0.3566,
      "step": 2553
    },
    {
      "epoch": 0.06685047201912506,
      "grad_norm": 22.151309967041016,
      "learning_rate": 3.9822036116199945e-06,
      "loss": 0.3949,
      "step": 2554
    },
    {
      "epoch": 0.06687664683197515,
      "grad_norm": 20.6536865234375,
      "learning_rate": 3.979063072494112e-06,
      "loss": 0.2333,
      "step": 2555
    },
    {
      "epoch": 0.06690282164482524,
      "grad_norm": 19.43416976928711,
      "learning_rate": 3.975922533368229e-06,
      "loss": 0.2768,
      "step": 2556
    },
    {
      "epoch": 0.06692899645767533,
      "grad_norm": 19.752254486083984,
      "learning_rate": 3.9727819942423455e-06,
      "loss": 0.089,
      "step": 2557
    },
    {
      "epoch": 0.06695517127052542,
      "grad_norm": 16.851482391357422,
      "learning_rate": 3.969641455116461e-06,
      "loss": 0.3185,
      "step": 2558
    },
    {
      "epoch": 0.0669813460833755,
      "grad_norm": 11.949830055236816,
      "learning_rate": 3.966500915990578e-06,
      "loss": 0.1721,
      "step": 2559
    },
    {
      "epoch": 0.0670075208962256,
      "grad_norm": 28.994056701660156,
      "learning_rate": 3.963360376864695e-06,
      "loss": 0.454,
      "step": 2560
    },
    {
      "epoch": 0.06703369570907568,
      "grad_norm": 12.581809997558594,
      "learning_rate": 3.960219837738812e-06,
      "loss": 0.41,
      "step": 2561
    },
    {
      "epoch": 0.06705987052192577,
      "grad_norm": 27.954696655273438,
      "learning_rate": 3.957079298612929e-06,
      "loss": 0.36,
      "step": 2562
    },
    {
      "epoch": 0.06708604533477586,
      "grad_norm": 23.060617446899414,
      "learning_rate": 3.953938759487046e-06,
      "loss": 0.3618,
      "step": 2563
    },
    {
      "epoch": 0.06711222014762594,
      "grad_norm": 19.444555282592773,
      "learning_rate": 3.950798220361162e-06,
      "loss": 0.3243,
      "step": 2564
    },
    {
      "epoch": 0.06713839496047604,
      "grad_norm": 12.88818073272705,
      "learning_rate": 3.947657681235279e-06,
      "loss": 0.2097,
      "step": 2565
    },
    {
      "epoch": 0.06716456977332612,
      "grad_norm": 14.954519271850586,
      "learning_rate": 3.944517142109395e-06,
      "loss": 0.4944,
      "step": 2566
    },
    {
      "epoch": 0.06719074458617622,
      "grad_norm": 21.515827178955078,
      "learning_rate": 3.9413766029835125e-06,
      "loss": 0.4497,
      "step": 2567
    },
    {
      "epoch": 0.0672169193990263,
      "grad_norm": 22.796417236328125,
      "learning_rate": 3.938236063857629e-06,
      "loss": 0.3162,
      "step": 2568
    },
    {
      "epoch": 0.06724309421187638,
      "grad_norm": 22.069242477416992,
      "learning_rate": 3.935095524731746e-06,
      "loss": 0.456,
      "step": 2569
    },
    {
      "epoch": 0.06726926902472648,
      "grad_norm": 11.572783470153809,
      "learning_rate": 3.931954985605863e-06,
      "loss": 0.126,
      "step": 2570
    },
    {
      "epoch": 0.06729544383757656,
      "grad_norm": 19.39166259765625,
      "learning_rate": 3.928814446479979e-06,
      "loss": 0.4345,
      "step": 2571
    },
    {
      "epoch": 0.06732161865042664,
      "grad_norm": 24.145668029785156,
      "learning_rate": 3.925673907354096e-06,
      "loss": 0.3591,
      "step": 2572
    },
    {
      "epoch": 0.06734779346327674,
      "grad_norm": 10.486777305603027,
      "learning_rate": 3.922533368228212e-06,
      "loss": 0.3034,
      "step": 2573
    },
    {
      "epoch": 0.06737396827612682,
      "grad_norm": 18.088388442993164,
      "learning_rate": 3.9193928291023295e-06,
      "loss": 0.3346,
      "step": 2574
    },
    {
      "epoch": 0.06740014308897692,
      "grad_norm": 19.71380615234375,
      "learning_rate": 3.916252289976446e-06,
      "loss": 0.2801,
      "step": 2575
    },
    {
      "epoch": 0.067426317901827,
      "grad_norm": 16.216400146484375,
      "learning_rate": 3.913111750850563e-06,
      "loss": 0.3492,
      "step": 2576
    },
    {
      "epoch": 0.06745249271467708,
      "grad_norm": 21.55854606628418,
      "learning_rate": 3.90997121172468e-06,
      "loss": 0.4095,
      "step": 2577
    },
    {
      "epoch": 0.06747866752752718,
      "grad_norm": 22.217077255249023,
      "learning_rate": 3.906830672598796e-06,
      "loss": 0.2949,
      "step": 2578
    },
    {
      "epoch": 0.06750484234037726,
      "grad_norm": 12.250509262084961,
      "learning_rate": 3.903690133472913e-06,
      "loss": 0.1539,
      "step": 2579
    },
    {
      "epoch": 0.06753101715322736,
      "grad_norm": 15.294159889221191,
      "learning_rate": 3.90054959434703e-06,
      "loss": 0.2138,
      "step": 2580
    },
    {
      "epoch": 0.06755719196607744,
      "grad_norm": 20.53630256652832,
      "learning_rate": 3.897409055221146e-06,
      "loss": 0.3667,
      "step": 2581
    },
    {
      "epoch": 0.06758336677892753,
      "grad_norm": 21.466798782348633,
      "learning_rate": 3.894268516095263e-06,
      "loss": 0.3872,
      "step": 2582
    },
    {
      "epoch": 0.06760954159177762,
      "grad_norm": 16.513011932373047,
      "learning_rate": 3.89112797696938e-06,
      "loss": 0.3006,
      "step": 2583
    },
    {
      "epoch": 0.0676357164046277,
      "grad_norm": 18.997650146484375,
      "learning_rate": 3.8879874378434965e-06,
      "loss": 0.1533,
      "step": 2584
    },
    {
      "epoch": 0.0676618912174778,
      "grad_norm": 14.66732120513916,
      "learning_rate": 3.884846898717613e-06,
      "loss": 0.1652,
      "step": 2585
    },
    {
      "epoch": 0.06768806603032788,
      "grad_norm": 17.4103946685791,
      "learning_rate": 3.88170635959173e-06,
      "loss": 0.482,
      "step": 2586
    },
    {
      "epoch": 0.06771424084317797,
      "grad_norm": 27.51597785949707,
      "learning_rate": 3.878565820465847e-06,
      "loss": 0.5798,
      "step": 2587
    },
    {
      "epoch": 0.06774041565602806,
      "grad_norm": 15.01077651977539,
      "learning_rate": 3.875425281339963e-06,
      "loss": 0.2296,
      "step": 2588
    },
    {
      "epoch": 0.06776659046887815,
      "grad_norm": 21.151628494262695,
      "learning_rate": 3.87228474221408e-06,
      "loss": 0.4002,
      "step": 2589
    },
    {
      "epoch": 0.06779276528172823,
      "grad_norm": 19.001432418823242,
      "learning_rate": 3.869144203088197e-06,
      "loss": 0.4082,
      "step": 2590
    },
    {
      "epoch": 0.06781894009457833,
      "grad_norm": 18.56776237487793,
      "learning_rate": 3.8660036639623135e-06,
      "loss": 0.3856,
      "step": 2591
    },
    {
      "epoch": 0.06784511490742841,
      "grad_norm": 51.37945556640625,
      "learning_rate": 3.86286312483643e-06,
      "loss": 0.8448,
      "step": 2592
    },
    {
      "epoch": 0.0678712897202785,
      "grad_norm": 16.877880096435547,
      "learning_rate": 3.859722585710547e-06,
      "loss": 0.2199,
      "step": 2593
    },
    {
      "epoch": 0.06789746453312859,
      "grad_norm": 32.99179458618164,
      "learning_rate": 3.856582046584664e-06,
      "loss": 0.8445,
      "step": 2594
    },
    {
      "epoch": 0.06792363934597867,
      "grad_norm": 19.36643409729004,
      "learning_rate": 3.853441507458781e-06,
      "loss": 0.3462,
      "step": 2595
    },
    {
      "epoch": 0.06794981415882877,
      "grad_norm": 22.192415237426758,
      "learning_rate": 3.850300968332898e-06,
      "loss": 0.3477,
      "step": 2596
    },
    {
      "epoch": 0.06797598897167885,
      "grad_norm": 17.087060928344727,
      "learning_rate": 3.847160429207014e-06,
      "loss": 0.483,
      "step": 2597
    },
    {
      "epoch": 0.06800216378452895,
      "grad_norm": 25.76961898803711,
      "learning_rate": 3.84401989008113e-06,
      "loss": 0.4639,
      "step": 2598
    },
    {
      "epoch": 0.06802833859737903,
      "grad_norm": 18.463417053222656,
      "learning_rate": 3.840879350955247e-06,
      "loss": 0.426,
      "step": 2599
    },
    {
      "epoch": 0.06805451341022911,
      "grad_norm": 20.196359634399414,
      "learning_rate": 3.837738811829364e-06,
      "loss": 0.3909,
      "step": 2600
    },
    {
      "epoch": 0.06808068822307921,
      "grad_norm": 19.73770523071289,
      "learning_rate": 3.834598272703481e-06,
      "loss": 0.5058,
      "step": 2601
    },
    {
      "epoch": 0.06810686303592929,
      "grad_norm": 21.07646369934082,
      "learning_rate": 3.831457733577598e-06,
      "loss": 0.4096,
      "step": 2602
    },
    {
      "epoch": 0.06813303784877939,
      "grad_norm": 11.637979507446289,
      "learning_rate": 3.828317194451715e-06,
      "loss": 0.3085,
      "step": 2603
    },
    {
      "epoch": 0.06815921266162947,
      "grad_norm": 14.75130844116211,
      "learning_rate": 3.825176655325831e-06,
      "loss": 0.2114,
      "step": 2604
    },
    {
      "epoch": 0.06818538747447955,
      "grad_norm": 31.928266525268555,
      "learning_rate": 3.822036116199947e-06,
      "loss": 0.3521,
      "step": 2605
    },
    {
      "epoch": 0.06821156228732965,
      "grad_norm": 23.52499771118164,
      "learning_rate": 3.818895577074064e-06,
      "loss": 0.5231,
      "step": 2606
    },
    {
      "epoch": 0.06823773710017973,
      "grad_norm": 17.607891082763672,
      "learning_rate": 3.815755037948181e-06,
      "loss": 0.2298,
      "step": 2607
    },
    {
      "epoch": 0.06826391191302981,
      "grad_norm": 9.79287052154541,
      "learning_rate": 3.812614498822298e-06,
      "loss": 0.1788,
      "step": 2608
    },
    {
      "epoch": 0.06829008672587991,
      "grad_norm": 14.651420593261719,
      "learning_rate": 3.8094739596964146e-06,
      "loss": 0.3453,
      "step": 2609
    },
    {
      "epoch": 0.06831626153873,
      "grad_norm": 16.060991287231445,
      "learning_rate": 3.8063334205705317e-06,
      "loss": 0.3128,
      "step": 2610
    },
    {
      "epoch": 0.06834243635158009,
      "grad_norm": 18.298376083374023,
      "learning_rate": 3.8031928814446484e-06,
      "loss": 0.3359,
      "step": 2611
    },
    {
      "epoch": 0.06836861116443017,
      "grad_norm": 20.211578369140625,
      "learning_rate": 3.8000523423187647e-06,
      "loss": 0.5501,
      "step": 2612
    },
    {
      "epoch": 0.06839478597728026,
      "grad_norm": 23.594213485717773,
      "learning_rate": 3.7969118031928814e-06,
      "loss": 0.453,
      "step": 2613
    },
    {
      "epoch": 0.06842096079013035,
      "grad_norm": 21.545669555664062,
      "learning_rate": 3.793771264066998e-06,
      "loss": 0.4284,
      "step": 2614
    },
    {
      "epoch": 0.06844713560298044,
      "grad_norm": 25.24675750732422,
      "learning_rate": 3.790630724941115e-06,
      "loss": 0.5357,
      "step": 2615
    },
    {
      "epoch": 0.06847331041583053,
      "grad_norm": 22.89546775817871,
      "learning_rate": 3.787490185815232e-06,
      "loss": 0.3228,
      "step": 2616
    },
    {
      "epoch": 0.06849948522868061,
      "grad_norm": 25.836071014404297,
      "learning_rate": 3.7843496466893487e-06,
      "loss": 0.6515,
      "step": 2617
    },
    {
      "epoch": 0.0685256600415307,
      "grad_norm": 21.974502563476562,
      "learning_rate": 3.7812091075634654e-06,
      "loss": 0.5549,
      "step": 2618
    },
    {
      "epoch": 0.0685518348543808,
      "grad_norm": 22.461660385131836,
      "learning_rate": 3.778068568437582e-06,
      "loss": 0.5236,
      "step": 2619
    },
    {
      "epoch": 0.06857800966723088,
      "grad_norm": 17.10297393798828,
      "learning_rate": 3.7749280293116984e-06,
      "loss": 0.4904,
      "step": 2620
    },
    {
      "epoch": 0.06860418448008097,
      "grad_norm": 18.33449363708496,
      "learning_rate": 3.771787490185815e-06,
      "loss": 0.3985,
      "step": 2621
    },
    {
      "epoch": 0.06863035929293106,
      "grad_norm": 25.011356353759766,
      "learning_rate": 3.7686469510599318e-06,
      "loss": 0.4653,
      "step": 2622
    },
    {
      "epoch": 0.06865653410578114,
      "grad_norm": 25.703414916992188,
      "learning_rate": 3.765506411934049e-06,
      "loss": 0.3393,
      "step": 2623
    },
    {
      "epoch": 0.06868270891863124,
      "grad_norm": 9.999871253967285,
      "learning_rate": 3.7623658728081656e-06,
      "loss": 0.2169,
      "step": 2624
    },
    {
      "epoch": 0.06870888373148132,
      "grad_norm": 14.913398742675781,
      "learning_rate": 3.7592253336822823e-06,
      "loss": 0.2498,
      "step": 2625
    },
    {
      "epoch": 0.06873505854433141,
      "grad_norm": 23.794435501098633,
      "learning_rate": 3.756084794556399e-06,
      "loss": 0.4706,
      "step": 2626
    },
    {
      "epoch": 0.0687612333571815,
      "grad_norm": 17.50168228149414,
      "learning_rate": 3.7529442554305153e-06,
      "loss": 0.3847,
      "step": 2627
    },
    {
      "epoch": 0.06878740817003158,
      "grad_norm": 28.50861167907715,
      "learning_rate": 3.749803716304632e-06,
      "loss": 0.483,
      "step": 2628
    },
    {
      "epoch": 0.06881358298288168,
      "grad_norm": 15.692161560058594,
      "learning_rate": 3.746663177178749e-06,
      "loss": 0.1569,
      "step": 2629
    },
    {
      "epoch": 0.06883975779573176,
      "grad_norm": 14.595085144042969,
      "learning_rate": 3.743522638052866e-06,
      "loss": 0.3428,
      "step": 2630
    },
    {
      "epoch": 0.06886593260858184,
      "grad_norm": 16.843564987182617,
      "learning_rate": 3.7403820989269826e-06,
      "loss": 0.447,
      "step": 2631
    },
    {
      "epoch": 0.06889210742143194,
      "grad_norm": 13.11552619934082,
      "learning_rate": 3.7372415598010993e-06,
      "loss": 0.2741,
      "step": 2632
    },
    {
      "epoch": 0.06891828223428202,
      "grad_norm": 32.72548294067383,
      "learning_rate": 3.7341010206752164e-06,
      "loss": 0.3438,
      "step": 2633
    },
    {
      "epoch": 0.06894445704713212,
      "grad_norm": 17.77821159362793,
      "learning_rate": 3.730960481549333e-06,
      "loss": 0.4371,
      "step": 2634
    },
    {
      "epoch": 0.0689706318599822,
      "grad_norm": 20.86141014099121,
      "learning_rate": 3.727819942423449e-06,
      "loss": 0.3486,
      "step": 2635
    },
    {
      "epoch": 0.06899680667283228,
      "grad_norm": 24.324405670166016,
      "learning_rate": 3.724679403297566e-06,
      "loss": 0.5427,
      "step": 2636
    },
    {
      "epoch": 0.06902298148568238,
      "grad_norm": 50.06397247314453,
      "learning_rate": 3.721538864171683e-06,
      "loss": 0.6466,
      "step": 2637
    },
    {
      "epoch": 0.06904915629853246,
      "grad_norm": 23.663251876831055,
      "learning_rate": 3.7183983250457995e-06,
      "loss": 0.5884,
      "step": 2638
    },
    {
      "epoch": 0.06907533111138256,
      "grad_norm": 17.463218688964844,
      "learning_rate": 3.715257785919916e-06,
      "loss": 0.3112,
      "step": 2639
    },
    {
      "epoch": 0.06910150592423264,
      "grad_norm": 19.607894897460938,
      "learning_rate": 3.7121172467940333e-06,
      "loss": 0.6145,
      "step": 2640
    },
    {
      "epoch": 0.06912768073708273,
      "grad_norm": 18.947681427001953,
      "learning_rate": 3.70897670766815e-06,
      "loss": 0.4095,
      "step": 2641
    },
    {
      "epoch": 0.06915385554993282,
      "grad_norm": 13.455037117004395,
      "learning_rate": 3.7058361685422668e-06,
      "loss": 0.1606,
      "step": 2642
    },
    {
      "epoch": 0.0691800303627829,
      "grad_norm": 14.693815231323242,
      "learning_rate": 3.702695629416383e-06,
      "loss": 0.3227,
      "step": 2643
    },
    {
      "epoch": 0.069206205175633,
      "grad_norm": 15.797130584716797,
      "learning_rate": 3.6995550902904997e-06,
      "loss": 0.4049,
      "step": 2644
    },
    {
      "epoch": 0.06923237998848308,
      "grad_norm": 15.588329315185547,
      "learning_rate": 3.6964145511646164e-06,
      "loss": 0.3169,
      "step": 2645
    },
    {
      "epoch": 0.06925855480133317,
      "grad_norm": 14.292417526245117,
      "learning_rate": 3.6932740120387336e-06,
      "loss": 0.3939,
      "step": 2646
    },
    {
      "epoch": 0.06928472961418326,
      "grad_norm": 13.829115867614746,
      "learning_rate": 3.6901334729128503e-06,
      "loss": 0.2143,
      "step": 2647
    },
    {
      "epoch": 0.06931090442703335,
      "grad_norm": 20.179216384887695,
      "learning_rate": 3.686992933786967e-06,
      "loss": 0.364,
      "step": 2648
    },
    {
      "epoch": 0.06933707923988343,
      "grad_norm": 22.759544372558594,
      "learning_rate": 3.6838523946610837e-06,
      "loss": 0.4051,
      "step": 2649
    },
    {
      "epoch": 0.06936325405273353,
      "grad_norm": 26.19148826599121,
      "learning_rate": 3.680711855535201e-06,
      "loss": 0.5467,
      "step": 2650
    },
    {
      "epoch": 0.06938942886558361,
      "grad_norm": 19.623050689697266,
      "learning_rate": 3.6775713164093167e-06,
      "loss": 0.3597,
      "step": 2651
    },
    {
      "epoch": 0.0694156036784337,
      "grad_norm": 15.902603149414062,
      "learning_rate": 3.6744307772834334e-06,
      "loss": 0.3874,
      "step": 2652
    },
    {
      "epoch": 0.06944177849128379,
      "grad_norm": 15.160813331604004,
      "learning_rate": 3.6712902381575505e-06,
      "loss": 0.2392,
      "step": 2653
    },
    {
      "epoch": 0.06946795330413387,
      "grad_norm": 21.44749641418457,
      "learning_rate": 3.6681496990316672e-06,
      "loss": 0.3102,
      "step": 2654
    },
    {
      "epoch": 0.06949412811698397,
      "grad_norm": 19.93824577331543,
      "learning_rate": 3.665009159905784e-06,
      "loss": 0.3809,
      "step": 2655
    },
    {
      "epoch": 0.06952030292983405,
      "grad_norm": 25.371244430541992,
      "learning_rate": 3.6618686207799006e-06,
      "loss": 0.5995,
      "step": 2656
    },
    {
      "epoch": 0.06954647774268415,
      "grad_norm": 22.15095329284668,
      "learning_rate": 3.6587280816540178e-06,
      "loss": 0.5308,
      "step": 2657
    },
    {
      "epoch": 0.06957265255553423,
      "grad_norm": 21.878768920898438,
      "learning_rate": 3.6555875425281336e-06,
      "loss": 0.26,
      "step": 2658
    },
    {
      "epoch": 0.06959882736838431,
      "grad_norm": 21.240739822387695,
      "learning_rate": 3.6524470034022508e-06,
      "loss": 0.4369,
      "step": 2659
    },
    {
      "epoch": 0.06962500218123441,
      "grad_norm": 15.06022834777832,
      "learning_rate": 3.6493064642763675e-06,
      "loss": 0.2453,
      "step": 2660
    },
    {
      "epoch": 0.06965117699408449,
      "grad_norm": 20.04867935180664,
      "learning_rate": 3.646165925150484e-06,
      "loss": 0.4997,
      "step": 2661
    },
    {
      "epoch": 0.06967735180693459,
      "grad_norm": 26.88216209411621,
      "learning_rate": 3.643025386024601e-06,
      "loss": 0.2796,
      "step": 2662
    },
    {
      "epoch": 0.06970352661978467,
      "grad_norm": 19.57303810119629,
      "learning_rate": 3.639884846898718e-06,
      "loss": 0.2387,
      "step": 2663
    },
    {
      "epoch": 0.06972970143263475,
      "grad_norm": 20.684711456298828,
      "learning_rate": 3.6367443077728347e-06,
      "loss": 0.214,
      "step": 2664
    },
    {
      "epoch": 0.06975587624548485,
      "grad_norm": 24.06061553955078,
      "learning_rate": 3.6336037686469514e-06,
      "loss": 0.4305,
      "step": 2665
    },
    {
      "epoch": 0.06978205105833493,
      "grad_norm": 26.646364212036133,
      "learning_rate": 3.6304632295210677e-06,
      "loss": 0.6705,
      "step": 2666
    },
    {
      "epoch": 0.06980822587118501,
      "grad_norm": 22.568532943725586,
      "learning_rate": 3.6273226903951844e-06,
      "loss": 0.4792,
      "step": 2667
    },
    {
      "epoch": 0.06983440068403511,
      "grad_norm": 13.169037818908691,
      "learning_rate": 3.624182151269301e-06,
      "loss": 0.3247,
      "step": 2668
    },
    {
      "epoch": 0.0698605754968852,
      "grad_norm": 28.030330657958984,
      "learning_rate": 3.621041612143418e-06,
      "loss": 0.4144,
      "step": 2669
    },
    {
      "epoch": 0.06988675030973529,
      "grad_norm": 12.446420669555664,
      "learning_rate": 3.617901073017535e-06,
      "loss": 0.2619,
      "step": 2670
    },
    {
      "epoch": 0.06991292512258537,
      "grad_norm": 33.30252456665039,
      "learning_rate": 3.6147605338916517e-06,
      "loss": 0.4205,
      "step": 2671
    },
    {
      "epoch": 0.06993909993543546,
      "grad_norm": 20.756881713867188,
      "learning_rate": 3.6116199947657684e-06,
      "loss": 0.3282,
      "step": 2672
    },
    {
      "epoch": 0.06996527474828555,
      "grad_norm": 18.783349990844727,
      "learning_rate": 3.608479455639885e-06,
      "loss": 0.2714,
      "step": 2673
    },
    {
      "epoch": 0.06999144956113564,
      "grad_norm": 24.229045867919922,
      "learning_rate": 3.6053389165140014e-06,
      "loss": 0.4456,
      "step": 2674
    },
    {
      "epoch": 0.07001762437398573,
      "grad_norm": 29.077184677124023,
      "learning_rate": 3.602198377388118e-06,
      "loss": 0.4181,
      "step": 2675
    },
    {
      "epoch": 0.07004379918683581,
      "grad_norm": 24.20526885986328,
      "learning_rate": 3.599057838262235e-06,
      "loss": 0.4513,
      "step": 2676
    },
    {
      "epoch": 0.0700699739996859,
      "grad_norm": 21.420583724975586,
      "learning_rate": 3.595917299136352e-06,
      "loss": 0.4248,
      "step": 2677
    },
    {
      "epoch": 0.070096148812536,
      "grad_norm": 15.547658920288086,
      "learning_rate": 3.5927767600104686e-06,
      "loss": 0.3831,
      "step": 2678
    },
    {
      "epoch": 0.07012232362538608,
      "grad_norm": 18.439599990844727,
      "learning_rate": 3.5896362208845853e-06,
      "loss": 0.3385,
      "step": 2679
    },
    {
      "epoch": 0.07014849843823617,
      "grad_norm": 17.0686092376709,
      "learning_rate": 3.5864956817587024e-06,
      "loss": 0.4319,
      "step": 2680
    },
    {
      "epoch": 0.07017467325108626,
      "grad_norm": 16.39766502380371,
      "learning_rate": 3.583355142632819e-06,
      "loss": 0.466,
      "step": 2681
    },
    {
      "epoch": 0.07020084806393634,
      "grad_norm": 19.37130355834961,
      "learning_rate": 3.580214603506935e-06,
      "loss": 0.4711,
      "step": 2682
    },
    {
      "epoch": 0.07022702287678644,
      "grad_norm": 22.729236602783203,
      "learning_rate": 3.577074064381052e-06,
      "loss": 0.6088,
      "step": 2683
    },
    {
      "epoch": 0.07025319768963652,
      "grad_norm": 16.263338088989258,
      "learning_rate": 3.573933525255169e-06,
      "loss": 0.3202,
      "step": 2684
    },
    {
      "epoch": 0.0702793725024866,
      "grad_norm": 22.025360107421875,
      "learning_rate": 3.5707929861292855e-06,
      "loss": 0.4398,
      "step": 2685
    },
    {
      "epoch": 0.0703055473153367,
      "grad_norm": 18.764429092407227,
      "learning_rate": 3.5676524470034022e-06,
      "loss": 0.3413,
      "step": 2686
    },
    {
      "epoch": 0.07033172212818678,
      "grad_norm": 18.44981575012207,
      "learning_rate": 3.5645119078775194e-06,
      "loss": 0.3714,
      "step": 2687
    },
    {
      "epoch": 0.07035789694103688,
      "grad_norm": 17.64421272277832,
      "learning_rate": 3.561371368751636e-06,
      "loss": 0.373,
      "step": 2688
    },
    {
      "epoch": 0.07038407175388696,
      "grad_norm": 21.55701446533203,
      "learning_rate": 3.5582308296257524e-06,
      "loss": 0.3863,
      "step": 2689
    },
    {
      "epoch": 0.07041024656673704,
      "grad_norm": 31.031980514526367,
      "learning_rate": 3.555090290499869e-06,
      "loss": 0.4088,
      "step": 2690
    },
    {
      "epoch": 0.07043642137958714,
      "grad_norm": 21.49720001220703,
      "learning_rate": 3.5519497513739858e-06,
      "loss": 0.4025,
      "step": 2691
    },
    {
      "epoch": 0.07046259619243722,
      "grad_norm": 15.181379318237305,
      "learning_rate": 3.5488092122481025e-06,
      "loss": 0.2605,
      "step": 2692
    },
    {
      "epoch": 0.07048877100528732,
      "grad_norm": 18.289592742919922,
      "learning_rate": 3.5456686731222196e-06,
      "loss": 0.3664,
      "step": 2693
    },
    {
      "epoch": 0.0705149458181374,
      "grad_norm": 10.599650382995605,
      "learning_rate": 3.5425281339963363e-06,
      "loss": 0.3157,
      "step": 2694
    },
    {
      "epoch": 0.07054112063098748,
      "grad_norm": 25.961042404174805,
      "learning_rate": 3.539387594870453e-06,
      "loss": 0.4755,
      "step": 2695
    },
    {
      "epoch": 0.07056729544383758,
      "grad_norm": 23.253721237182617,
      "learning_rate": 3.5362470557445697e-06,
      "loss": 0.4136,
      "step": 2696
    },
    {
      "epoch": 0.07059347025668766,
      "grad_norm": 20.992530822753906,
      "learning_rate": 3.533106516618686e-06,
      "loss": 0.3727,
      "step": 2697
    },
    {
      "epoch": 0.07061964506953776,
      "grad_norm": 27.84260368347168,
      "learning_rate": 3.5299659774928027e-06,
      "loss": 0.567,
      "step": 2698
    },
    {
      "epoch": 0.07064581988238784,
      "grad_norm": 16.68890953063965,
      "learning_rate": 3.5268254383669194e-06,
      "loss": 0.3337,
      "step": 2699
    },
    {
      "epoch": 0.07067199469523792,
      "grad_norm": 13.358253479003906,
      "learning_rate": 3.5236848992410366e-06,
      "loss": 0.3176,
      "step": 2700
    },
    {
      "epoch": 0.07069816950808802,
      "grad_norm": 16.95918083190918,
      "learning_rate": 3.5205443601151533e-06,
      "loss": 0.2776,
      "step": 2701
    },
    {
      "epoch": 0.0707243443209381,
      "grad_norm": 18.362712860107422,
      "learning_rate": 3.51740382098927e-06,
      "loss": 0.3523,
      "step": 2702
    },
    {
      "epoch": 0.07075051913378819,
      "grad_norm": 25.85883140563965,
      "learning_rate": 3.5142632818633867e-06,
      "loss": 0.3344,
      "step": 2703
    },
    {
      "epoch": 0.07077669394663828,
      "grad_norm": 25.63673973083496,
      "learning_rate": 3.511122742737504e-06,
      "loss": 0.4794,
      "step": 2704
    },
    {
      "epoch": 0.07080286875948837,
      "grad_norm": 15.621490478515625,
      "learning_rate": 3.5079822036116197e-06,
      "loss": 0.2261,
      "step": 2705
    },
    {
      "epoch": 0.07082904357233846,
      "grad_norm": 19.5607967376709,
      "learning_rate": 3.504841664485737e-06,
      "loss": 0.4465,
      "step": 2706
    },
    {
      "epoch": 0.07085521838518855,
      "grad_norm": 29.535280227661133,
      "learning_rate": 3.5017011253598535e-06,
      "loss": 0.5701,
      "step": 2707
    },
    {
      "epoch": 0.07088139319803863,
      "grad_norm": 17.28217124938965,
      "learning_rate": 3.49856058623397e-06,
      "loss": 0.202,
      "step": 2708
    },
    {
      "epoch": 0.07090756801088872,
      "grad_norm": 20.3538818359375,
      "learning_rate": 3.495420047108087e-06,
      "loss": 0.4821,
      "step": 2709
    },
    {
      "epoch": 0.07093374282373881,
      "grad_norm": 25.787084579467773,
      "learning_rate": 3.492279507982204e-06,
      "loss": 0.3251,
      "step": 2710
    },
    {
      "epoch": 0.0709599176365889,
      "grad_norm": 16.315053939819336,
      "learning_rate": 3.4891389688563208e-06,
      "loss": 0.306,
      "step": 2711
    },
    {
      "epoch": 0.07098609244943899,
      "grad_norm": 37.14488983154297,
      "learning_rate": 3.4859984297304375e-06,
      "loss": 0.6845,
      "step": 2712
    },
    {
      "epoch": 0.07101226726228907,
      "grad_norm": 18.53975486755371,
      "learning_rate": 3.4828578906045537e-06,
      "loss": 0.2638,
      "step": 2713
    },
    {
      "epoch": 0.07103844207513917,
      "grad_norm": 24.308395385742188,
      "learning_rate": 3.4797173514786704e-06,
      "loss": 0.4309,
      "step": 2714
    },
    {
      "epoch": 0.07106461688798925,
      "grad_norm": 22.37741470336914,
      "learning_rate": 3.476576812352787e-06,
      "loss": 0.4033,
      "step": 2715
    },
    {
      "epoch": 0.07109079170083935,
      "grad_norm": 34.719818115234375,
      "learning_rate": 3.473436273226904e-06,
      "loss": 0.6255,
      "step": 2716
    },
    {
      "epoch": 0.07111696651368943,
      "grad_norm": 28.154767990112305,
      "learning_rate": 3.470295734101021e-06,
      "loss": 0.3731,
      "step": 2717
    },
    {
      "epoch": 0.07114314132653951,
      "grad_norm": 18.383190155029297,
      "learning_rate": 3.4671551949751377e-06,
      "loss": 0.3314,
      "step": 2718
    },
    {
      "epoch": 0.07116931613938961,
      "grad_norm": 15.13977336883545,
      "learning_rate": 3.4640146558492544e-06,
      "loss": 0.269,
      "step": 2719
    },
    {
      "epoch": 0.07119549095223969,
      "grad_norm": 38.44101333618164,
      "learning_rate": 3.4608741167233707e-06,
      "loss": 0.6711,
      "step": 2720
    },
    {
      "epoch": 0.07122166576508977,
      "grad_norm": 18.035511016845703,
      "learning_rate": 3.4577335775974874e-06,
      "loss": 0.4062,
      "step": 2721
    },
    {
      "epoch": 0.07124784057793987,
      "grad_norm": 17.583538055419922,
      "learning_rate": 3.454593038471604e-06,
      "loss": 0.3903,
      "step": 2722
    },
    {
      "epoch": 0.07127401539078995,
      "grad_norm": 20.724817276000977,
      "learning_rate": 3.4514524993457212e-06,
      "loss": 0.3615,
      "step": 2723
    },
    {
      "epoch": 0.07130019020364005,
      "grad_norm": 21.850608825683594,
      "learning_rate": 3.448311960219838e-06,
      "loss": 0.6239,
      "step": 2724
    },
    {
      "epoch": 0.07132636501649013,
      "grad_norm": 18.579547882080078,
      "learning_rate": 3.4451714210939546e-06,
      "loss": 0.3852,
      "step": 2725
    },
    {
      "epoch": 0.07135253982934021,
      "grad_norm": 17.21211814880371,
      "learning_rate": 3.4420308819680713e-06,
      "loss": 0.3504,
      "step": 2726
    },
    {
      "epoch": 0.07137871464219031,
      "grad_norm": 22.7604923248291,
      "learning_rate": 3.4388903428421885e-06,
      "loss": 0.4815,
      "step": 2727
    },
    {
      "epoch": 0.0714048894550404,
      "grad_norm": 30.498716354370117,
      "learning_rate": 3.4357498037163043e-06,
      "loss": 0.3947,
      "step": 2728
    },
    {
      "epoch": 0.07143106426789049,
      "grad_norm": 23.175025939941406,
      "learning_rate": 3.432609264590421e-06,
      "loss": 0.5748,
      "step": 2729
    },
    {
      "epoch": 0.07145723908074057,
      "grad_norm": 19.260847091674805,
      "learning_rate": 3.429468725464538e-06,
      "loss": 0.3009,
      "step": 2730
    },
    {
      "epoch": 0.07148341389359066,
      "grad_norm": 24.82628059387207,
      "learning_rate": 3.426328186338655e-06,
      "loss": 0.672,
      "step": 2731
    },
    {
      "epoch": 0.07150958870644075,
      "grad_norm": 12.489027976989746,
      "learning_rate": 3.4231876472127716e-06,
      "loss": 0.315,
      "step": 2732
    },
    {
      "epoch": 0.07153576351929083,
      "grad_norm": 16.01370620727539,
      "learning_rate": 3.4200471080868883e-06,
      "loss": 0.3559,
      "step": 2733
    },
    {
      "epoch": 0.07156193833214093,
      "grad_norm": 24.32720375061035,
      "learning_rate": 3.4169065689610054e-06,
      "loss": 0.488,
      "step": 2734
    },
    {
      "epoch": 0.07158811314499101,
      "grad_norm": 12.231523513793945,
      "learning_rate": 3.413766029835122e-06,
      "loss": 0.1937,
      "step": 2735
    },
    {
      "epoch": 0.0716142879578411,
      "grad_norm": 20.446441650390625,
      "learning_rate": 3.4106254907092384e-06,
      "loss": 0.4903,
      "step": 2736
    },
    {
      "epoch": 0.0716404627706912,
      "grad_norm": 16.030311584472656,
      "learning_rate": 3.407484951583355e-06,
      "loss": 0.2983,
      "step": 2737
    },
    {
      "epoch": 0.07166663758354128,
      "grad_norm": 14.814799308776855,
      "learning_rate": 3.404344412457472e-06,
      "loss": 0.3071,
      "step": 2738
    },
    {
      "epoch": 0.07169281239639137,
      "grad_norm": 30.271289825439453,
      "learning_rate": 3.4012038733315885e-06,
      "loss": 0.3948,
      "step": 2739
    },
    {
      "epoch": 0.07171898720924146,
      "grad_norm": 15.342944145202637,
      "learning_rate": 3.3980633342057057e-06,
      "loss": 0.2353,
      "step": 2740
    },
    {
      "epoch": 0.07174516202209154,
      "grad_norm": 16.96076202392578,
      "learning_rate": 3.3949227950798224e-06,
      "loss": 0.3002,
      "step": 2741
    },
    {
      "epoch": 0.07177133683494163,
      "grad_norm": 15.255982398986816,
      "learning_rate": 3.391782255953939e-06,
      "loss": 0.2377,
      "step": 2742
    },
    {
      "epoch": 0.07179751164779172,
      "grad_norm": 14.883891105651855,
      "learning_rate": 3.3886417168280554e-06,
      "loss": 0.2865,
      "step": 2743
    },
    {
      "epoch": 0.0718236864606418,
      "grad_norm": 20.455413818359375,
      "learning_rate": 3.385501177702172e-06,
      "loss": 0.2004,
      "step": 2744
    },
    {
      "epoch": 0.0718498612734919,
      "grad_norm": 20.985872268676758,
      "learning_rate": 3.3823606385762888e-06,
      "loss": 0.3909,
      "step": 2745
    },
    {
      "epoch": 0.07187603608634198,
      "grad_norm": 18.23155975341797,
      "learning_rate": 3.3792200994504055e-06,
      "loss": 0.3733,
      "step": 2746
    },
    {
      "epoch": 0.07190221089919208,
      "grad_norm": 18.480392456054688,
      "learning_rate": 3.3760795603245226e-06,
      "loss": 0.3513,
      "step": 2747
    },
    {
      "epoch": 0.07192838571204216,
      "grad_norm": 16.74898910522461,
      "learning_rate": 3.3729390211986393e-06,
      "loss": 0.3051,
      "step": 2748
    },
    {
      "epoch": 0.07195456052489224,
      "grad_norm": 18.94245147705078,
      "learning_rate": 3.369798482072756e-06,
      "loss": 0.399,
      "step": 2749
    },
    {
      "epoch": 0.07198073533774234,
      "grad_norm": 14.54839038848877,
      "learning_rate": 3.3666579429468727e-06,
      "loss": 0.2734,
      "step": 2750
    },
    {
      "epoch": 0.07200691015059242,
      "grad_norm": 18.758581161499023,
      "learning_rate": 3.363517403820989e-06,
      "loss": 0.3561,
      "step": 2751
    },
    {
      "epoch": 0.07203308496344252,
      "grad_norm": 18.59882926940918,
      "learning_rate": 3.3603768646951057e-06,
      "loss": 0.3512,
      "step": 2752
    },
    {
      "epoch": 0.0720592597762926,
      "grad_norm": 17.31528091430664,
      "learning_rate": 3.357236325569223e-06,
      "loss": 0.3591,
      "step": 2753
    },
    {
      "epoch": 0.07208543458914268,
      "grad_norm": 25.03875732421875,
      "learning_rate": 3.3540957864433395e-06,
      "loss": 0.4159,
      "step": 2754
    },
    {
      "epoch": 0.07211160940199278,
      "grad_norm": 31.541297912597656,
      "learning_rate": 3.3509552473174562e-06,
      "loss": 0.4002,
      "step": 2755
    },
    {
      "epoch": 0.07213778421484286,
      "grad_norm": 25.37648582458496,
      "learning_rate": 3.347814708191573e-06,
      "loss": 0.327,
      "step": 2756
    },
    {
      "epoch": 0.07216395902769296,
      "grad_norm": 15.994768142700195,
      "learning_rate": 3.34467416906569e-06,
      "loss": 0.2436,
      "step": 2757
    },
    {
      "epoch": 0.07219013384054304,
      "grad_norm": 20.546113967895508,
      "learning_rate": 3.341533629939807e-06,
      "loss": 0.4355,
      "step": 2758
    },
    {
      "epoch": 0.07221630865339312,
      "grad_norm": 17.221900939941406,
      "learning_rate": 3.3383930908139227e-06,
      "loss": 0.283,
      "step": 2759
    },
    {
      "epoch": 0.07224248346624322,
      "grad_norm": 18.173137664794922,
      "learning_rate": 3.3352525516880398e-06,
      "loss": 0.1608,
      "step": 2760
    },
    {
      "epoch": 0.0722686582790933,
      "grad_norm": 23.591529846191406,
      "learning_rate": 3.3321120125621565e-06,
      "loss": 0.3644,
      "step": 2761
    },
    {
      "epoch": 0.07229483309194339,
      "grad_norm": 16.27047348022461,
      "learning_rate": 3.328971473436273e-06,
      "loss": 0.4074,
      "step": 2762
    },
    {
      "epoch": 0.07232100790479348,
      "grad_norm": 18.64449691772461,
      "learning_rate": 3.32583093431039e-06,
      "loss": 0.3333,
      "step": 2763
    },
    {
      "epoch": 0.07234718271764357,
      "grad_norm": 35.08289337158203,
      "learning_rate": 3.322690395184507e-06,
      "loss": 0.2559,
      "step": 2764
    },
    {
      "epoch": 0.07237335753049366,
      "grad_norm": 21.817520141601562,
      "learning_rate": 3.3195498560586237e-06,
      "loss": 0.6147,
      "step": 2765
    },
    {
      "epoch": 0.07239953234334374,
      "grad_norm": 16.69463348388672,
      "learning_rate": 3.3164093169327404e-06,
      "loss": 0.3071,
      "step": 2766
    },
    {
      "epoch": 0.07242570715619383,
      "grad_norm": 21.444316864013672,
      "learning_rate": 3.3132687778068567e-06,
      "loss": 0.443,
      "step": 2767
    },
    {
      "epoch": 0.07245188196904392,
      "grad_norm": 23.2562313079834,
      "learning_rate": 3.3101282386809734e-06,
      "loss": 0.2303,
      "step": 2768
    },
    {
      "epoch": 0.072478056781894,
      "grad_norm": 18.63339614868164,
      "learning_rate": 3.30698769955509e-06,
      "loss": 0.4496,
      "step": 2769
    },
    {
      "epoch": 0.0725042315947441,
      "grad_norm": 17.329307556152344,
      "learning_rate": 3.3038471604292073e-06,
      "loss": 0.2534,
      "step": 2770
    },
    {
      "epoch": 0.07253040640759419,
      "grad_norm": 13.793363571166992,
      "learning_rate": 3.300706621303324e-06,
      "loss": 0.2097,
      "step": 2771
    },
    {
      "epoch": 0.07255658122044427,
      "grad_norm": 14.506278038024902,
      "learning_rate": 3.2975660821774407e-06,
      "loss": 0.4095,
      "step": 2772
    },
    {
      "epoch": 0.07258275603329437,
      "grad_norm": 12.774569511413574,
      "learning_rate": 3.2944255430515574e-06,
      "loss": 0.2403,
      "step": 2773
    },
    {
      "epoch": 0.07260893084614445,
      "grad_norm": 16.125751495361328,
      "learning_rate": 3.2912850039256737e-06,
      "loss": 0.1321,
      "step": 2774
    },
    {
      "epoch": 0.07263510565899454,
      "grad_norm": 11.471235275268555,
      "learning_rate": 3.2881444647997904e-06,
      "loss": 0.2954,
      "step": 2775
    },
    {
      "epoch": 0.07266128047184463,
      "grad_norm": 13.574540138244629,
      "learning_rate": 3.285003925673907e-06,
      "loss": 0.2862,
      "step": 2776
    },
    {
      "epoch": 0.07268745528469471,
      "grad_norm": 15.49380111694336,
      "learning_rate": 3.281863386548024e-06,
      "loss": 0.2195,
      "step": 2777
    },
    {
      "epoch": 0.0727136300975448,
      "grad_norm": 19.085716247558594,
      "learning_rate": 3.278722847422141e-06,
      "loss": 0.5013,
      "step": 2778
    },
    {
      "epoch": 0.07273980491039489,
      "grad_norm": 14.574397087097168,
      "learning_rate": 3.2755823082962576e-06,
      "loss": 0.245,
      "step": 2779
    },
    {
      "epoch": 0.07276597972324497,
      "grad_norm": 17.949316024780273,
      "learning_rate": 3.2724417691703743e-06,
      "loss": 0.3591,
      "step": 2780
    },
    {
      "epoch": 0.07279215453609507,
      "grad_norm": 11.461202621459961,
      "learning_rate": 3.2693012300444915e-06,
      "loss": 0.1578,
      "step": 2781
    },
    {
      "epoch": 0.07281832934894515,
      "grad_norm": 28.277366638183594,
      "learning_rate": 3.2661606909186073e-06,
      "loss": 0.3744,
      "step": 2782
    },
    {
      "epoch": 0.07284450416179525,
      "grad_norm": 10.438882827758789,
      "learning_rate": 3.2630201517927244e-06,
      "loss": 0.1831,
      "step": 2783
    },
    {
      "epoch": 0.07287067897464533,
      "grad_norm": 19.832054138183594,
      "learning_rate": 3.259879612666841e-06,
      "loss": 0.3512,
      "step": 2784
    },
    {
      "epoch": 0.07289685378749541,
      "grad_norm": 15.098703384399414,
      "learning_rate": 3.256739073540958e-06,
      "loss": 0.2006,
      "step": 2785
    },
    {
      "epoch": 0.07292302860034551,
      "grad_norm": 21.097068786621094,
      "learning_rate": 3.2535985344150746e-06,
      "loss": 0.5858,
      "step": 2786
    },
    {
      "epoch": 0.07294920341319559,
      "grad_norm": 22.39211082458496,
      "learning_rate": 3.2504579952891917e-06,
      "loss": 0.425,
      "step": 2787
    },
    {
      "epoch": 0.07297537822604569,
      "grad_norm": 23.282060623168945,
      "learning_rate": 3.2473174561633084e-06,
      "loss": 0.5025,
      "step": 2788
    },
    {
      "epoch": 0.07300155303889577,
      "grad_norm": 17.216506958007812,
      "learning_rate": 3.244176917037425e-06,
      "loss": 0.3676,
      "step": 2789
    },
    {
      "epoch": 0.07302772785174586,
      "grad_norm": 23.838998794555664,
      "learning_rate": 3.2410363779115414e-06,
      "loss": 0.5075,
      "step": 2790
    },
    {
      "epoch": 0.07305390266459595,
      "grad_norm": 18.534591674804688,
      "learning_rate": 3.237895838785658e-06,
      "loss": 0.3384,
      "step": 2791
    },
    {
      "epoch": 0.07308007747744603,
      "grad_norm": 17.44755744934082,
      "learning_rate": 3.234755299659775e-06,
      "loss": 0.3109,
      "step": 2792
    },
    {
      "epoch": 0.07310625229029613,
      "grad_norm": 23.237014770507812,
      "learning_rate": 3.2316147605338915e-06,
      "loss": 0.221,
      "step": 2793
    },
    {
      "epoch": 0.07313242710314621,
      "grad_norm": 20.394893646240234,
      "learning_rate": 3.2284742214080086e-06,
      "loss": 0.2693,
      "step": 2794
    },
    {
      "epoch": 0.0731586019159963,
      "grad_norm": 11.138130187988281,
      "learning_rate": 3.2253336822821253e-06,
      "loss": 0.2259,
      "step": 2795
    },
    {
      "epoch": 0.07318477672884639,
      "grad_norm": 21.41794776916504,
      "learning_rate": 3.222193143156242e-06,
      "loss": 0.6365,
      "step": 2796
    },
    {
      "epoch": 0.07321095154169648,
      "grad_norm": 15.960946083068848,
      "learning_rate": 3.2190526040303588e-06,
      "loss": 0.2467,
      "step": 2797
    },
    {
      "epoch": 0.07323712635454656,
      "grad_norm": 12.966826438903809,
      "learning_rate": 3.215912064904475e-06,
      "loss": 0.2153,
      "step": 2798
    },
    {
      "epoch": 0.07326330116739665,
      "grad_norm": 18.677631378173828,
      "learning_rate": 3.2127715257785917e-06,
      "loss": 0.4272,
      "step": 2799
    },
    {
      "epoch": 0.07328947598024674,
      "grad_norm": 22.266311645507812,
      "learning_rate": 3.209630986652709e-06,
      "loss": 0.338,
      "step": 2800
    },
    {
      "epoch": 0.07331565079309683,
      "grad_norm": 11.138181686401367,
      "learning_rate": 3.2064904475268256e-06,
      "loss": 0.1736,
      "step": 2801
    },
    {
      "epoch": 0.07334182560594692,
      "grad_norm": 19.03462791442871,
      "learning_rate": 3.2033499084009423e-06,
      "loss": 0.351,
      "step": 2802
    },
    {
      "epoch": 0.073368000418797,
      "grad_norm": 19.748287200927734,
      "learning_rate": 3.200209369275059e-06,
      "loss": 0.2536,
      "step": 2803
    },
    {
      "epoch": 0.0733941752316471,
      "grad_norm": 9.115032196044922,
      "learning_rate": 3.197068830149176e-06,
      "loss": 0.2489,
      "step": 2804
    },
    {
      "epoch": 0.07342035004449718,
      "grad_norm": 15.434274673461914,
      "learning_rate": 3.193928291023292e-06,
      "loss": 0.2201,
      "step": 2805
    },
    {
      "epoch": 0.07344652485734728,
      "grad_norm": 26.902250289916992,
      "learning_rate": 3.1907877518974087e-06,
      "loss": 0.3759,
      "step": 2806
    },
    {
      "epoch": 0.07347269967019736,
      "grad_norm": 19.61575698852539,
      "learning_rate": 3.187647212771526e-06,
      "loss": 0.2478,
      "step": 2807
    },
    {
      "epoch": 0.07349887448304744,
      "grad_norm": 23.294639587402344,
      "learning_rate": 3.1845066736456425e-06,
      "loss": 0.4891,
      "step": 2808
    },
    {
      "epoch": 0.07352504929589754,
      "grad_norm": 12.213580131530762,
      "learning_rate": 3.1813661345197592e-06,
      "loss": 0.1055,
      "step": 2809
    },
    {
      "epoch": 0.07355122410874762,
      "grad_norm": 15.241900444030762,
      "learning_rate": 3.178225595393876e-06,
      "loss": 0.2646,
      "step": 2810
    },
    {
      "epoch": 0.07357739892159772,
      "grad_norm": 24.43379783630371,
      "learning_rate": 3.175085056267993e-06,
      "loss": 0.4283,
      "step": 2811
    },
    {
      "epoch": 0.0736035737344478,
      "grad_norm": 30.20269012451172,
      "learning_rate": 3.1719445171421098e-06,
      "loss": 0.4626,
      "step": 2812
    },
    {
      "epoch": 0.07362974854729788,
      "grad_norm": 21.535104751586914,
      "learning_rate": 3.168803978016226e-06,
      "loss": 0.395,
      "step": 2813
    },
    {
      "epoch": 0.07365592336014798,
      "grad_norm": 24.610313415527344,
      "learning_rate": 3.1656634388903428e-06,
      "loss": 0.3679,
      "step": 2814
    },
    {
      "epoch": 0.07368209817299806,
      "grad_norm": 16.338043212890625,
      "learning_rate": 3.1625228997644595e-06,
      "loss": 0.3743,
      "step": 2815
    },
    {
      "epoch": 0.07370827298584814,
      "grad_norm": 14.770686149597168,
      "learning_rate": 3.159382360638576e-06,
      "loss": 0.332,
      "step": 2816
    },
    {
      "epoch": 0.07373444779869824,
      "grad_norm": 22.021345138549805,
      "learning_rate": 3.1562418215126933e-06,
      "loss": 0.3669,
      "step": 2817
    },
    {
      "epoch": 0.07376062261154832,
      "grad_norm": 29.036422729492188,
      "learning_rate": 3.15310128238681e-06,
      "loss": 0.4276,
      "step": 2818
    },
    {
      "epoch": 0.07378679742439842,
      "grad_norm": 31.92242431640625,
      "learning_rate": 3.1499607432609267e-06,
      "loss": 0.4869,
      "step": 2819
    },
    {
      "epoch": 0.0738129722372485,
      "grad_norm": 33.730228424072266,
      "learning_rate": 3.1468202041350434e-06,
      "loss": 0.3844,
      "step": 2820
    },
    {
      "epoch": 0.07383914705009859,
      "grad_norm": 20.99494743347168,
      "learning_rate": 3.1436796650091597e-06,
      "loss": 0.2829,
      "step": 2821
    },
    {
      "epoch": 0.07386532186294868,
      "grad_norm": 21.91173553466797,
      "learning_rate": 3.1405391258832764e-06,
      "loss": 0.3486,
      "step": 2822
    },
    {
      "epoch": 0.07389149667579877,
      "grad_norm": 16.77596664428711,
      "learning_rate": 3.137398586757393e-06,
      "loss": 0.3554,
      "step": 2823
    },
    {
      "epoch": 0.07391767148864886,
      "grad_norm": 22.468645095825195,
      "learning_rate": 3.1342580476315102e-06,
      "loss": 0.3835,
      "step": 2824
    },
    {
      "epoch": 0.07394384630149894,
      "grad_norm": 19.697620391845703,
      "learning_rate": 3.131117508505627e-06,
      "loss": 0.4162,
      "step": 2825
    },
    {
      "epoch": 0.07397002111434903,
      "grad_norm": 20.79418182373047,
      "learning_rate": 3.1279769693797437e-06,
      "loss": 0.4263,
      "step": 2826
    },
    {
      "epoch": 0.07399619592719912,
      "grad_norm": 17.694311141967773,
      "learning_rate": 3.1248364302538604e-06,
      "loss": 0.3355,
      "step": 2827
    },
    {
      "epoch": 0.0740223707400492,
      "grad_norm": 18.469974517822266,
      "learning_rate": 3.1216958911279775e-06,
      "loss": 0.3685,
      "step": 2828
    },
    {
      "epoch": 0.0740485455528993,
      "grad_norm": 26.029985427856445,
      "learning_rate": 3.1185553520020934e-06,
      "loss": 0.5235,
      "step": 2829
    },
    {
      "epoch": 0.07407472036574939,
      "grad_norm": 39.300270080566406,
      "learning_rate": 3.1154148128762105e-06,
      "loss": 0.6007,
      "step": 2830
    },
    {
      "epoch": 0.07410089517859947,
      "grad_norm": 15.759007453918457,
      "learning_rate": 3.112274273750327e-06,
      "loss": 0.3603,
      "step": 2831
    },
    {
      "epoch": 0.07412706999144957,
      "grad_norm": 15.429612159729004,
      "learning_rate": 3.109133734624444e-06,
      "loss": 0.1529,
      "step": 2832
    },
    {
      "epoch": 0.07415324480429965,
      "grad_norm": 18.249040603637695,
      "learning_rate": 3.1059931954985606e-06,
      "loss": 0.3609,
      "step": 2833
    },
    {
      "epoch": 0.07417941961714973,
      "grad_norm": 30.544830322265625,
      "learning_rate": 3.1028526563726777e-06,
      "loss": 0.3966,
      "step": 2834
    },
    {
      "epoch": 0.07420559442999983,
      "grad_norm": 23.470958709716797,
      "learning_rate": 3.0997121172467944e-06,
      "loss": 0.3837,
      "step": 2835
    },
    {
      "epoch": 0.07423176924284991,
      "grad_norm": 31.387596130371094,
      "learning_rate": 3.0965715781209107e-06,
      "loss": 0.6489,
      "step": 2836
    },
    {
      "epoch": 0.0742579440557,
      "grad_norm": 16.411489486694336,
      "learning_rate": 3.0934310389950274e-06,
      "loss": 0.3094,
      "step": 2837
    },
    {
      "epoch": 0.07428411886855009,
      "grad_norm": 27.08842658996582,
      "learning_rate": 3.090290499869144e-06,
      "loss": 0.6561,
      "step": 2838
    },
    {
      "epoch": 0.07431029368140017,
      "grad_norm": 20.337085723876953,
      "learning_rate": 3.087149960743261e-06,
      "loss": 0.5613,
      "step": 2839
    },
    {
      "epoch": 0.07433646849425027,
      "grad_norm": 28.619417190551758,
      "learning_rate": 3.0840094216173775e-06,
      "loss": 0.3101,
      "step": 2840
    },
    {
      "epoch": 0.07436264330710035,
      "grad_norm": 15.829922676086426,
      "learning_rate": 3.0808688824914947e-06,
      "loss": 0.3302,
      "step": 2841
    },
    {
      "epoch": 0.07438881811995045,
      "grad_norm": 18.721229553222656,
      "learning_rate": 3.0777283433656114e-06,
      "loss": 0.4856,
      "step": 2842
    },
    {
      "epoch": 0.07441499293280053,
      "grad_norm": 17.841693878173828,
      "learning_rate": 3.074587804239728e-06,
      "loss": 0.4237,
      "step": 2843
    },
    {
      "epoch": 0.07444116774565061,
      "grad_norm": 21.399890899658203,
      "learning_rate": 3.0714472651138444e-06,
      "loss": 0.4466,
      "step": 2844
    },
    {
      "epoch": 0.07446734255850071,
      "grad_norm": 31.248584747314453,
      "learning_rate": 3.068306725987961e-06,
      "loss": 0.6241,
      "step": 2845
    },
    {
      "epoch": 0.07449351737135079,
      "grad_norm": 15.650325775146484,
      "learning_rate": 3.0651661868620778e-06,
      "loss": 0.298,
      "step": 2846
    },
    {
      "epoch": 0.07451969218420089,
      "grad_norm": 20.468168258666992,
      "learning_rate": 3.062025647736195e-06,
      "loss": 0.4473,
      "step": 2847
    },
    {
      "epoch": 0.07454586699705097,
      "grad_norm": 19.04978370666504,
      "learning_rate": 3.0588851086103116e-06,
      "loss": 0.4322,
      "step": 2848
    },
    {
      "epoch": 0.07457204180990105,
      "grad_norm": 20.025739669799805,
      "learning_rate": 3.0557445694844283e-06,
      "loss": 0.4168,
      "step": 2849
    },
    {
      "epoch": 0.07459821662275115,
      "grad_norm": 13.251590728759766,
      "learning_rate": 3.052604030358545e-06,
      "loss": 0.2394,
      "step": 2850
    },
    {
      "epoch": 0.07462439143560123,
      "grad_norm": 21.17861557006836,
      "learning_rate": 3.049463491232662e-06,
      "loss": 0.4487,
      "step": 2851
    },
    {
      "epoch": 0.07465056624845133,
      "grad_norm": 25.94441795349121,
      "learning_rate": 3.046322952106778e-06,
      "loss": 0.5302,
      "step": 2852
    },
    {
      "epoch": 0.07467674106130141,
      "grad_norm": 19.953125,
      "learning_rate": 3.043182412980895e-06,
      "loss": 0.5024,
      "step": 2853
    },
    {
      "epoch": 0.0747029158741515,
      "grad_norm": 20.585838317871094,
      "learning_rate": 3.040041873855012e-06,
      "loss": 0.2427,
      "step": 2854
    },
    {
      "epoch": 0.07472909068700159,
      "grad_norm": 19.714174270629883,
      "learning_rate": 3.0369013347291286e-06,
      "loss": 0.3754,
      "step": 2855
    },
    {
      "epoch": 0.07475526549985168,
      "grad_norm": 18.245635986328125,
      "learning_rate": 3.0337607956032453e-06,
      "loss": 0.3015,
      "step": 2856
    },
    {
      "epoch": 0.07478144031270176,
      "grad_norm": 11.272880554199219,
      "learning_rate": 3.030620256477362e-06,
      "loss": 0.1662,
      "step": 2857
    },
    {
      "epoch": 0.07480761512555185,
      "grad_norm": 19.843505859375,
      "learning_rate": 3.027479717351479e-06,
      "loss": 0.3228,
      "step": 2858
    },
    {
      "epoch": 0.07483378993840194,
      "grad_norm": 28.536684036254883,
      "learning_rate": 3.024339178225596e-06,
      "loss": 0.4255,
      "step": 2859
    },
    {
      "epoch": 0.07485996475125203,
      "grad_norm": 25.610933303833008,
      "learning_rate": 3.021198639099712e-06,
      "loss": 0.5351,
      "step": 2860
    },
    {
      "epoch": 0.07488613956410212,
      "grad_norm": 12.386569023132324,
      "learning_rate": 3.018058099973829e-06,
      "loss": 0.2328,
      "step": 2861
    },
    {
      "epoch": 0.0749123143769522,
      "grad_norm": 15.962974548339844,
      "learning_rate": 3.0149175608479455e-06,
      "loss": 0.1043,
      "step": 2862
    },
    {
      "epoch": 0.0749384891898023,
      "grad_norm": 27.216703414916992,
      "learning_rate": 3.0117770217220622e-06,
      "loss": 0.5116,
      "step": 2863
    },
    {
      "epoch": 0.07496466400265238,
      "grad_norm": 14.298276901245117,
      "learning_rate": 3.0086364825961793e-06,
      "loss": 0.2059,
      "step": 2864
    },
    {
      "epoch": 0.07499083881550248,
      "grad_norm": 17.79351043701172,
      "learning_rate": 3.005495943470296e-06,
      "loss": 0.2309,
      "step": 2865
    },
    {
      "epoch": 0.07501701362835256,
      "grad_norm": 13.698631286621094,
      "learning_rate": 3.0023554043444128e-06,
      "loss": 0.2193,
      "step": 2866
    },
    {
      "epoch": 0.07504318844120264,
      "grad_norm": 18.636550903320312,
      "learning_rate": 2.9992148652185295e-06,
      "loss": 0.2373,
      "step": 2867
    },
    {
      "epoch": 0.07506936325405274,
      "grad_norm": 29.264135360717773,
      "learning_rate": 2.996074326092646e-06,
      "loss": 0.3454,
      "step": 2868
    },
    {
      "epoch": 0.07509553806690282,
      "grad_norm": 22.890634536743164,
      "learning_rate": 2.9929337869667625e-06,
      "loss": 0.4032,
      "step": 2869
    },
    {
      "epoch": 0.07512171287975292,
      "grad_norm": 21.24633026123047,
      "learning_rate": 2.9897932478408796e-06,
      "loss": 0.4855,
      "step": 2870
    },
    {
      "epoch": 0.075147887692603,
      "grad_norm": 23.853363037109375,
      "learning_rate": 2.9866527087149963e-06,
      "loss": 0.4358,
      "step": 2871
    },
    {
      "epoch": 0.07517406250545308,
      "grad_norm": 22.586929321289062,
      "learning_rate": 2.983512169589113e-06,
      "loss": 0.3437,
      "step": 2872
    },
    {
      "epoch": 0.07520023731830318,
      "grad_norm": 20.077714920043945,
      "learning_rate": 2.9803716304632293e-06,
      "loss": 0.3463,
      "step": 2873
    },
    {
      "epoch": 0.07522641213115326,
      "grad_norm": 34.57415008544922,
      "learning_rate": 2.9772310913373464e-06,
      "loss": 0.7929,
      "step": 2874
    },
    {
      "epoch": 0.07525258694400334,
      "grad_norm": 23.749805450439453,
      "learning_rate": 2.974090552211463e-06,
      "loss": 0.3313,
      "step": 2875
    },
    {
      "epoch": 0.07527876175685344,
      "grad_norm": 20.77327537536621,
      "learning_rate": 2.97095001308558e-06,
      "loss": 0.4135,
      "step": 2876
    },
    {
      "epoch": 0.07530493656970352,
      "grad_norm": 18.69814109802246,
      "learning_rate": 2.9678094739596965e-06,
      "loss": 0.5993,
      "step": 2877
    },
    {
      "epoch": 0.07533111138255362,
      "grad_norm": 14.143827438354492,
      "learning_rate": 2.9646689348338132e-06,
      "loss": 0.159,
      "step": 2878
    },
    {
      "epoch": 0.0753572861954037,
      "grad_norm": 23.930570602416992,
      "learning_rate": 2.96152839570793e-06,
      "loss": 0.4647,
      "step": 2879
    },
    {
      "epoch": 0.07538346100825379,
      "grad_norm": 11.378724098205566,
      "learning_rate": 2.9583878565820466e-06,
      "loss": 0.1452,
      "step": 2880
    },
    {
      "epoch": 0.07540963582110388,
      "grad_norm": 25.72671127319336,
      "learning_rate": 2.9552473174561634e-06,
      "loss": 0.5986,
      "step": 2881
    },
    {
      "epoch": 0.07543581063395396,
      "grad_norm": 15.801834106445312,
      "learning_rate": 2.95210677833028e-06,
      "loss": 0.3089,
      "step": 2882
    },
    {
      "epoch": 0.07546198544680406,
      "grad_norm": 22.302017211914062,
      "learning_rate": 2.9489662392043968e-06,
      "loss": 0.4217,
      "step": 2883
    },
    {
      "epoch": 0.07548816025965414,
      "grad_norm": 20.25213050842285,
      "learning_rate": 2.945825700078514e-06,
      "loss": 0.3585,
      "step": 2884
    },
    {
      "epoch": 0.07551433507250423,
      "grad_norm": 17.772241592407227,
      "learning_rate": 2.94268516095263e-06,
      "loss": 0.2852,
      "step": 2885
    },
    {
      "epoch": 0.07554050988535432,
      "grad_norm": 17.377058029174805,
      "learning_rate": 2.939544621826747e-06,
      "loss": 0.3452,
      "step": 2886
    },
    {
      "epoch": 0.0755666846982044,
      "grad_norm": 25.344697952270508,
      "learning_rate": 2.936404082700864e-06,
      "loss": 0.506,
      "step": 2887
    },
    {
      "epoch": 0.0755928595110545,
      "grad_norm": 24.775243759155273,
      "learning_rate": 2.9332635435749807e-06,
      "loss": 0.4365,
      "step": 2888
    },
    {
      "epoch": 0.07561903432390459,
      "grad_norm": 22.669485092163086,
      "learning_rate": 2.930123004449097e-06,
      "loss": 0.3535,
      "step": 2889
    },
    {
      "epoch": 0.07564520913675467,
      "grad_norm": 15.515217781066895,
      "learning_rate": 2.9269824653232137e-06,
      "loss": 0.3603,
      "step": 2890
    },
    {
      "epoch": 0.07567138394960476,
      "grad_norm": 24.036090850830078,
      "learning_rate": 2.923841926197331e-06,
      "loss": 0.3824,
      "step": 2891
    },
    {
      "epoch": 0.07569755876245485,
      "grad_norm": 20.2916259765625,
      "learning_rate": 2.920701387071447e-06,
      "loss": 0.3996,
      "step": 2892
    },
    {
      "epoch": 0.07572373357530493,
      "grad_norm": 25.415224075317383,
      "learning_rate": 2.917560847945564e-06,
      "loss": 0.5278,
      "step": 2893
    },
    {
      "epoch": 0.07574990838815503,
      "grad_norm": 29.91558265686035,
      "learning_rate": 2.914420308819681e-06,
      "loss": 0.4251,
      "step": 2894
    },
    {
      "epoch": 0.07577608320100511,
      "grad_norm": 20.05961036682129,
      "learning_rate": 2.9112797696937977e-06,
      "loss": 0.451,
      "step": 2895
    },
    {
      "epoch": 0.0758022580138552,
      "grad_norm": 25.13962745666504,
      "learning_rate": 2.908139230567914e-06,
      "loss": 0.467,
      "step": 2896
    },
    {
      "epoch": 0.07582843282670529,
      "grad_norm": 19.305763244628906,
      "learning_rate": 2.904998691442031e-06,
      "loss": 0.5871,
      "step": 2897
    },
    {
      "epoch": 0.07585460763955537,
      "grad_norm": 12.476944923400879,
      "learning_rate": 2.9018581523161478e-06,
      "loss": 0.1449,
      "step": 2898
    },
    {
      "epoch": 0.07588078245240547,
      "grad_norm": 18.180028915405273,
      "learning_rate": 2.8987176131902645e-06,
      "loss": 0.323,
      "step": 2899
    },
    {
      "epoch": 0.07590695726525555,
      "grad_norm": 21.424427032470703,
      "learning_rate": 2.895577074064381e-06,
      "loss": 0.432,
      "step": 2900
    },
    {
      "epoch": 0.07593313207810565,
      "grad_norm": 37.24107360839844,
      "learning_rate": 2.892436534938498e-06,
      "loss": 0.6594,
      "step": 2901
    },
    {
      "epoch": 0.07595930689095573,
      "grad_norm": 13.032051086425781,
      "learning_rate": 2.8892959958126146e-06,
      "loss": 0.3603,
      "step": 2902
    },
    {
      "epoch": 0.07598548170380581,
      "grad_norm": 17.275371551513672,
      "learning_rate": 2.8861554566867313e-06,
      "loss": 0.4036,
      "step": 2903
    },
    {
      "epoch": 0.07601165651665591,
      "grad_norm": 30.528247833251953,
      "learning_rate": 2.883014917560848e-06,
      "loss": 0.6879,
      "step": 2904
    },
    {
      "epoch": 0.07603783132950599,
      "grad_norm": 29.13702392578125,
      "learning_rate": 2.8798743784349647e-06,
      "loss": 0.2325,
      "step": 2905
    },
    {
      "epoch": 0.07606400614235609,
      "grad_norm": 17.71526527404785,
      "learning_rate": 2.8767338393090814e-06,
      "loss": 0.3686,
      "step": 2906
    },
    {
      "epoch": 0.07609018095520617,
      "grad_norm": 12.71264362335205,
      "learning_rate": 2.873593300183198e-06,
      "loss": 0.2978,
      "step": 2907
    },
    {
      "epoch": 0.07611635576805625,
      "grad_norm": 21.38494873046875,
      "learning_rate": 2.870452761057315e-06,
      "loss": 0.4612,
      "step": 2908
    },
    {
      "epoch": 0.07614253058090635,
      "grad_norm": 15.742820739746094,
      "learning_rate": 2.8673122219314315e-06,
      "loss": 0.3845,
      "step": 2909
    },
    {
      "epoch": 0.07616870539375643,
      "grad_norm": 13.733641624450684,
      "learning_rate": 2.8641716828055483e-06,
      "loss": 0.1123,
      "step": 2910
    },
    {
      "epoch": 0.07619488020660652,
      "grad_norm": 11.877307891845703,
      "learning_rate": 2.8610311436796654e-06,
      "loss": 0.263,
      "step": 2911
    },
    {
      "epoch": 0.07622105501945661,
      "grad_norm": 20.697595596313477,
      "learning_rate": 2.8578906045537817e-06,
      "loss": 0.3099,
      "step": 2912
    },
    {
      "epoch": 0.0762472298323067,
      "grad_norm": 31.9465389251709,
      "learning_rate": 2.8547500654278984e-06,
      "loss": 0.5105,
      "step": 2913
    },
    {
      "epoch": 0.07627340464515679,
      "grad_norm": 13.19759750366211,
      "learning_rate": 2.8516095263020155e-06,
      "loss": 0.1834,
      "step": 2914
    },
    {
      "epoch": 0.07629957945800687,
      "grad_norm": 20.072389602661133,
      "learning_rate": 2.848468987176132e-06,
      "loss": 0.2524,
      "step": 2915
    },
    {
      "epoch": 0.07632575427085696,
      "grad_norm": 21.116790771484375,
      "learning_rate": 2.8453284480502485e-06,
      "loss": 0.4537,
      "step": 2916
    },
    {
      "epoch": 0.07635192908370705,
      "grad_norm": 25.966493606567383,
      "learning_rate": 2.8421879089243656e-06,
      "loss": 0.4697,
      "step": 2917
    },
    {
      "epoch": 0.07637810389655714,
      "grad_norm": 17.374223709106445,
      "learning_rate": 2.8390473697984823e-06,
      "loss": 0.3488,
      "step": 2918
    },
    {
      "epoch": 0.07640427870940723,
      "grad_norm": 32.42593765258789,
      "learning_rate": 2.8359068306725986e-06,
      "loss": 0.4656,
      "step": 2919
    },
    {
      "epoch": 0.07643045352225732,
      "grad_norm": 22.087158203125,
      "learning_rate": 2.8327662915467153e-06,
      "loss": 0.2993,
      "step": 2920
    },
    {
      "epoch": 0.0764566283351074,
      "grad_norm": 25.318471908569336,
      "learning_rate": 2.8296257524208324e-06,
      "loss": 0.4197,
      "step": 2921
    },
    {
      "epoch": 0.0764828031479575,
      "grad_norm": 26.744712829589844,
      "learning_rate": 2.826485213294949e-06,
      "loss": 0.4499,
      "step": 2922
    },
    {
      "epoch": 0.07650897796080758,
      "grad_norm": 17.269983291625977,
      "learning_rate": 2.8233446741690654e-06,
      "loss": 0.2883,
      "step": 2923
    },
    {
      "epoch": 0.07653515277365767,
      "grad_norm": 15.77410888671875,
      "learning_rate": 2.8202041350431826e-06,
      "loss": 0.2513,
      "step": 2924
    },
    {
      "epoch": 0.07656132758650776,
      "grad_norm": 24.823280334472656,
      "learning_rate": 2.8170635959172993e-06,
      "loss": 0.3773,
      "step": 2925
    },
    {
      "epoch": 0.07658750239935784,
      "grad_norm": 26.085773468017578,
      "learning_rate": 2.813923056791416e-06,
      "loss": 0.4662,
      "step": 2926
    },
    {
      "epoch": 0.07661367721220794,
      "grad_norm": 24.928037643432617,
      "learning_rate": 2.8107825176655327e-06,
      "loss": 0.4351,
      "step": 2927
    },
    {
      "epoch": 0.07663985202505802,
      "grad_norm": 30.059467315673828,
      "learning_rate": 2.8076419785396494e-06,
      "loss": 0.4753,
      "step": 2928
    },
    {
      "epoch": 0.0766660268379081,
      "grad_norm": 20.241975784301758,
      "learning_rate": 2.804501439413766e-06,
      "loss": 0.2648,
      "step": 2929
    },
    {
      "epoch": 0.0766922016507582,
      "grad_norm": 21.199071884155273,
      "learning_rate": 2.801360900287883e-06,
      "loss": 0.4564,
      "step": 2930
    },
    {
      "epoch": 0.07671837646360828,
      "grad_norm": 18.158205032348633,
      "learning_rate": 2.7982203611619995e-06,
      "loss": 0.2409,
      "step": 2931
    },
    {
      "epoch": 0.07674455127645838,
      "grad_norm": 25.0279598236084,
      "learning_rate": 2.7950798220361162e-06,
      "loss": 0.322,
      "step": 2932
    },
    {
      "epoch": 0.07677072608930846,
      "grad_norm": 15.991374015808105,
      "learning_rate": 2.791939282910233e-06,
      "loss": 0.4481,
      "step": 2933
    },
    {
      "epoch": 0.07679690090215854,
      "grad_norm": 26.096126556396484,
      "learning_rate": 2.78879874378435e-06,
      "loss": 0.616,
      "step": 2934
    },
    {
      "epoch": 0.07682307571500864,
      "grad_norm": 15.148693084716797,
      "learning_rate": 2.7856582046584663e-06,
      "loss": 0.2958,
      "step": 2935
    },
    {
      "epoch": 0.07684925052785872,
      "grad_norm": 20.5550537109375,
      "learning_rate": 2.782517665532583e-06,
      "loss": 0.3832,
      "step": 2936
    },
    {
      "epoch": 0.07687542534070882,
      "grad_norm": 26.548091888427734,
      "learning_rate": 2.7793771264066997e-06,
      "loss": 0.4139,
      "step": 2937
    },
    {
      "epoch": 0.0769016001535589,
      "grad_norm": 29.646163940429688,
      "learning_rate": 2.776236587280817e-06,
      "loss": 0.3953,
      "step": 2938
    },
    {
      "epoch": 0.07692777496640898,
      "grad_norm": 14.702056884765625,
      "learning_rate": 2.773096048154933e-06,
      "loss": 0.3328,
      "step": 2939
    },
    {
      "epoch": 0.07695394977925908,
      "grad_norm": 21.0512752532959,
      "learning_rate": 2.76995550902905e-06,
      "loss": 0.3884,
      "step": 2940
    },
    {
      "epoch": 0.07698012459210916,
      "grad_norm": 18.211952209472656,
      "learning_rate": 2.766814969903167e-06,
      "loss": 0.3388,
      "step": 2941
    },
    {
      "epoch": 0.07700629940495926,
      "grad_norm": 20.011526107788086,
      "learning_rate": 2.7636744307772837e-06,
      "loss": 0.488,
      "step": 2942
    },
    {
      "epoch": 0.07703247421780934,
      "grad_norm": 23.025043487548828,
      "learning_rate": 2.7605338916514e-06,
      "loss": 0.5728,
      "step": 2943
    },
    {
      "epoch": 0.07705864903065943,
      "grad_norm": 30.220226287841797,
      "learning_rate": 2.757393352525517e-06,
      "loss": 0.4664,
      "step": 2944
    },
    {
      "epoch": 0.07708482384350952,
      "grad_norm": 13.955244064331055,
      "learning_rate": 2.754252813399634e-06,
      "loss": 0.294,
      "step": 2945
    },
    {
      "epoch": 0.0771109986563596,
      "grad_norm": 25.482667922973633,
      "learning_rate": 2.7511122742737505e-06,
      "loss": 0.305,
      "step": 2946
    },
    {
      "epoch": 0.07713717346920969,
      "grad_norm": 22.9782657623291,
      "learning_rate": 2.7479717351478672e-06,
      "loss": 0.3527,
      "step": 2947
    },
    {
      "epoch": 0.07716334828205978,
      "grad_norm": 18.730234146118164,
      "learning_rate": 2.744831196021984e-06,
      "loss": 0.3373,
      "step": 2948
    },
    {
      "epoch": 0.07718952309490987,
      "grad_norm": 12.819247245788574,
      "learning_rate": 2.7416906568961006e-06,
      "loss": 0.2606,
      "step": 2949
    },
    {
      "epoch": 0.07721569790775996,
      "grad_norm": 14.656922340393066,
      "learning_rate": 2.738550117770217e-06,
      "loss": 0.2674,
      "step": 2950
    },
    {
      "epoch": 0.07724187272061005,
      "grad_norm": 30.591552734375,
      "learning_rate": 2.735409578644334e-06,
      "loss": 0.3659,
      "step": 2951
    },
    {
      "epoch": 0.07726804753346013,
      "grad_norm": 28.5106143951416,
      "learning_rate": 2.7322690395184508e-06,
      "loss": 0.3492,
      "step": 2952
    },
    {
      "epoch": 0.07729422234631023,
      "grad_norm": 17.654144287109375,
      "learning_rate": 2.7291285003925675e-06,
      "loss": 0.3971,
      "step": 2953
    },
    {
      "epoch": 0.07732039715916031,
      "grad_norm": 21.648117065429688,
      "learning_rate": 2.725987961266684e-06,
      "loss": 0.3843,
      "step": 2954
    },
    {
      "epoch": 0.0773465719720104,
      "grad_norm": 17.22783088684082,
      "learning_rate": 2.722847422140801e-06,
      "loss": 0.3076,
      "step": 2955
    },
    {
      "epoch": 0.07737274678486049,
      "grad_norm": 22.731773376464844,
      "learning_rate": 2.7197068830149176e-06,
      "loss": 0.3556,
      "step": 2956
    },
    {
      "epoch": 0.07739892159771057,
      "grad_norm": 16.633075714111328,
      "learning_rate": 2.7165663438890343e-06,
      "loss": 0.2854,
      "step": 2957
    },
    {
      "epoch": 0.07742509641056067,
      "grad_norm": 27.294044494628906,
      "learning_rate": 2.713425804763151e-06,
      "loss": 0.4579,
      "step": 2958
    },
    {
      "epoch": 0.07745127122341075,
      "grad_norm": 26.520706176757812,
      "learning_rate": 2.7102852656372677e-06,
      "loss": 0.5838,
      "step": 2959
    },
    {
      "epoch": 0.07747744603626085,
      "grad_norm": 16.87974739074707,
      "learning_rate": 2.7071447265113844e-06,
      "loss": 0.3527,
      "step": 2960
    },
    {
      "epoch": 0.07750362084911093,
      "grad_norm": 17.5876407623291,
      "learning_rate": 2.7040041873855015e-06,
      "loss": 0.3164,
      "step": 2961
    },
    {
      "epoch": 0.07752979566196101,
      "grad_norm": 13.178654670715332,
      "learning_rate": 2.700863648259618e-06,
      "loss": 0.2966,
      "step": 2962
    },
    {
      "epoch": 0.07755597047481111,
      "grad_norm": 16.753875732421875,
      "learning_rate": 2.6977231091337345e-06,
      "loss": 0.2405,
      "step": 2963
    },
    {
      "epoch": 0.07758214528766119,
      "grad_norm": 18.010269165039062,
      "learning_rate": 2.6945825700078517e-06,
      "loss": 0.3684,
      "step": 2964
    },
    {
      "epoch": 0.07760832010051129,
      "grad_norm": 18.091684341430664,
      "learning_rate": 2.6914420308819684e-06,
      "loss": 0.4112,
      "step": 2965
    },
    {
      "epoch": 0.07763449491336137,
      "grad_norm": 19.550731658935547,
      "learning_rate": 2.6883014917560847e-06,
      "loss": 0.3583,
      "step": 2966
    },
    {
      "epoch": 0.07766066972621145,
      "grad_norm": 25.304370880126953,
      "learning_rate": 2.6851609526302014e-06,
      "loss": 0.4431,
      "step": 2967
    },
    {
      "epoch": 0.07768684453906155,
      "grad_norm": 33.00871658325195,
      "learning_rate": 2.6820204135043185e-06,
      "loss": 0.5526,
      "step": 2968
    },
    {
      "epoch": 0.07771301935191163,
      "grad_norm": 28.872760772705078,
      "learning_rate": 2.678879874378435e-06,
      "loss": 0.5403,
      "step": 2969
    },
    {
      "epoch": 0.07773919416476172,
      "grad_norm": 17.96619415283203,
      "learning_rate": 2.6757393352525515e-06,
      "loss": 0.4643,
      "step": 2970
    },
    {
      "epoch": 0.07776536897761181,
      "grad_norm": 16.545515060424805,
      "learning_rate": 2.6725987961266686e-06,
      "loss": 0.2524,
      "step": 2971
    },
    {
      "epoch": 0.0777915437904619,
      "grad_norm": 18.607749938964844,
      "learning_rate": 2.6694582570007853e-06,
      "loss": 0.4166,
      "step": 2972
    },
    {
      "epoch": 0.07781771860331199,
      "grad_norm": 16.211938858032227,
      "learning_rate": 2.666317717874902e-06,
      "loss": 0.3078,
      "step": 2973
    },
    {
      "epoch": 0.07784389341616207,
      "grad_norm": 14.498132705688477,
      "learning_rate": 2.6631771787490187e-06,
      "loss": 0.2907,
      "step": 2974
    },
    {
      "epoch": 0.07787006822901216,
      "grad_norm": 16.385927200317383,
      "learning_rate": 2.6600366396231354e-06,
      "loss": 0.1909,
      "step": 2975
    },
    {
      "epoch": 0.07789624304186225,
      "grad_norm": 5.612741947174072,
      "learning_rate": 2.656896100497252e-06,
      "loss": 0.0303,
      "step": 2976
    },
    {
      "epoch": 0.07792241785471234,
      "grad_norm": 30.754724502563477,
      "learning_rate": 2.653755561371369e-06,
      "loss": 0.287,
      "step": 2977
    },
    {
      "epoch": 0.07794859266756243,
      "grad_norm": 21.052778244018555,
      "learning_rate": 2.6506150222454855e-06,
      "loss": 0.3271,
      "step": 2978
    },
    {
      "epoch": 0.07797476748041252,
      "grad_norm": 17.33724594116211,
      "learning_rate": 2.6474744831196023e-06,
      "loss": 0.3085,
      "step": 2979
    },
    {
      "epoch": 0.0780009422932626,
      "grad_norm": 17.139413833618164,
      "learning_rate": 2.644333943993719e-06,
      "loss": 0.3958,
      "step": 2980
    },
    {
      "epoch": 0.0780271171061127,
      "grad_norm": 11.553173065185547,
      "learning_rate": 2.6411934048678357e-06,
      "loss": 0.1809,
      "step": 2981
    },
    {
      "epoch": 0.07805329191896278,
      "grad_norm": 26.151063919067383,
      "learning_rate": 2.6380528657419524e-06,
      "loss": 0.4641,
      "step": 2982
    },
    {
      "epoch": 0.07807946673181287,
      "grad_norm": 29.818134307861328,
      "learning_rate": 2.634912326616069e-06,
      "loss": 0.7223,
      "step": 2983
    },
    {
      "epoch": 0.07810564154466296,
      "grad_norm": 20.297651290893555,
      "learning_rate": 2.6317717874901858e-06,
      "loss": 0.2103,
      "step": 2984
    },
    {
      "epoch": 0.07813181635751304,
      "grad_norm": 18.716514587402344,
      "learning_rate": 2.6286312483643025e-06,
      "loss": 0.428,
      "step": 2985
    },
    {
      "epoch": 0.07815799117036314,
      "grad_norm": 18.821561813354492,
      "learning_rate": 2.625490709238419e-06,
      "loss": 0.3743,
      "step": 2986
    },
    {
      "epoch": 0.07818416598321322,
      "grad_norm": 27.643096923828125,
      "learning_rate": 2.622350170112536e-06,
      "loss": 0.3074,
      "step": 2987
    },
    {
      "epoch": 0.0782103407960633,
      "grad_norm": 13.55276870727539,
      "learning_rate": 2.619209630986653e-06,
      "loss": 0.2339,
      "step": 2988
    },
    {
      "epoch": 0.0782365156089134,
      "grad_norm": 18.371063232421875,
      "learning_rate": 2.6160690918607693e-06,
      "loss": 0.4191,
      "step": 2989
    },
    {
      "epoch": 0.07826269042176348,
      "grad_norm": 18.077856063842773,
      "learning_rate": 2.612928552734886e-06,
      "loss": 0.3279,
      "step": 2990
    },
    {
      "epoch": 0.07828886523461358,
      "grad_norm": 26.049842834472656,
      "learning_rate": 2.609788013609003e-06,
      "loss": 0.4378,
      "step": 2991
    },
    {
      "epoch": 0.07831504004746366,
      "grad_norm": 29.336647033691406,
      "learning_rate": 2.60664747448312e-06,
      "loss": 0.5107,
      "step": 2992
    },
    {
      "epoch": 0.07834121486031374,
      "grad_norm": 30.980960845947266,
      "learning_rate": 2.603506935357236e-06,
      "loss": 0.4438,
      "step": 2993
    },
    {
      "epoch": 0.07836738967316384,
      "grad_norm": 18.288705825805664,
      "learning_rate": 2.6003663962313533e-06,
      "loss": 0.4533,
      "step": 2994
    },
    {
      "epoch": 0.07839356448601392,
      "grad_norm": 19.642663955688477,
      "learning_rate": 2.59722585710547e-06,
      "loss": 0.3245,
      "step": 2995
    },
    {
      "epoch": 0.07841973929886402,
      "grad_norm": 16.473905563354492,
      "learning_rate": 2.5940853179795867e-06,
      "loss": 0.2748,
      "step": 2996
    },
    {
      "epoch": 0.0784459141117141,
      "grad_norm": 32.19566345214844,
      "learning_rate": 2.590944778853703e-06,
      "loss": 0.5368,
      "step": 2997
    },
    {
      "epoch": 0.07847208892456418,
      "grad_norm": 24.193988800048828,
      "learning_rate": 2.58780423972782e-06,
      "loss": 0.5445,
      "step": 2998
    },
    {
      "epoch": 0.07849826373741428,
      "grad_norm": 23.308521270751953,
      "learning_rate": 2.584663700601937e-06,
      "loss": 0.4488,
      "step": 2999
    },
    {
      "epoch": 0.07852443855026436,
      "grad_norm": 17.966455459594727,
      "learning_rate": 2.5815231614760535e-06,
      "loss": 0.3546,
      "step": 3000
    },
    {
      "epoch": 0.07855061336311446,
      "grad_norm": 23.627716064453125,
      "learning_rate": 2.5783826223501702e-06,
      "loss": 0.4659,
      "step": 3001
    },
    {
      "epoch": 0.07857678817596454,
      "grad_norm": 26.70046615600586,
      "learning_rate": 2.575242083224287e-06,
      "loss": 0.5224,
      "step": 3002
    },
    {
      "epoch": 0.07860296298881463,
      "grad_norm": 18.91343879699707,
      "learning_rate": 2.5721015440984036e-06,
      "loss": 0.2767,
      "step": 3003
    },
    {
      "epoch": 0.07862913780166472,
      "grad_norm": 22.96126365661621,
      "learning_rate": 2.5689610049725203e-06,
      "loss": 0.3498,
      "step": 3004
    },
    {
      "epoch": 0.0786553126145148,
      "grad_norm": 24.551862716674805,
      "learning_rate": 2.565820465846637e-06,
      "loss": 0.3173,
      "step": 3005
    },
    {
      "epoch": 0.07868148742736489,
      "grad_norm": 22.458820343017578,
      "learning_rate": 2.5626799267207537e-06,
      "loss": 0.3927,
      "step": 3006
    },
    {
      "epoch": 0.07870766224021498,
      "grad_norm": 22.589046478271484,
      "learning_rate": 2.5595393875948705e-06,
      "loss": 0.441,
      "step": 3007
    },
    {
      "epoch": 0.07873383705306507,
      "grad_norm": 29.8669490814209,
      "learning_rate": 2.556398848468987e-06,
      "loss": 0.6244,
      "step": 3008
    },
    {
      "epoch": 0.07876001186591516,
      "grad_norm": 19.979564666748047,
      "learning_rate": 2.553258309343104e-06,
      "loss": 0.2,
      "step": 3009
    },
    {
      "epoch": 0.07878618667876525,
      "grad_norm": 15.671130180358887,
      "learning_rate": 2.5501177702172206e-06,
      "loss": 0.3658,
      "step": 3010
    },
    {
      "epoch": 0.07881236149161533,
      "grad_norm": 28.198936462402344,
      "learning_rate": 2.5469772310913377e-06,
      "loss": 0.6765,
      "step": 3011
    },
    {
      "epoch": 0.07883853630446543,
      "grad_norm": 15.966164588928223,
      "learning_rate": 2.543836691965454e-06,
      "loss": 0.3131,
      "step": 3012
    },
    {
      "epoch": 0.07886471111731551,
      "grad_norm": 20.741090774536133,
      "learning_rate": 2.5406961528395707e-06,
      "loss": 0.2633,
      "step": 3013
    },
    {
      "epoch": 0.0788908859301656,
      "grad_norm": 15.060288429260254,
      "learning_rate": 2.5375556137136874e-06,
      "loss": 0.3245,
      "step": 3014
    },
    {
      "epoch": 0.07891706074301569,
      "grad_norm": 21.077627182006836,
      "learning_rate": 2.5344150745878045e-06,
      "loss": 0.3355,
      "step": 3015
    },
    {
      "epoch": 0.07894323555586577,
      "grad_norm": 23.341434478759766,
      "learning_rate": 2.531274535461921e-06,
      "loss": 0.4417,
      "step": 3016
    },
    {
      "epoch": 0.07896941036871587,
      "grad_norm": 23.209678649902344,
      "learning_rate": 2.5281339963360375e-06,
      "loss": 0.238,
      "step": 3017
    },
    {
      "epoch": 0.07899558518156595,
      "grad_norm": 12.48785400390625,
      "learning_rate": 2.5249934572101546e-06,
      "loss": 0.3202,
      "step": 3018
    },
    {
      "epoch": 0.07902175999441605,
      "grad_norm": 11.815333366394043,
      "learning_rate": 2.5218529180842714e-06,
      "loss": 0.162,
      "step": 3019
    },
    {
      "epoch": 0.07904793480726613,
      "grad_norm": 28.110639572143555,
      "learning_rate": 2.5187123789583876e-06,
      "loss": 0.5591,
      "step": 3020
    },
    {
      "epoch": 0.07907410962011621,
      "grad_norm": 17.00056266784668,
      "learning_rate": 2.5155718398325048e-06,
      "loss": 0.3598,
      "step": 3021
    },
    {
      "epoch": 0.07910028443296631,
      "grad_norm": 15.494665145874023,
      "learning_rate": 2.5124313007066215e-06,
      "loss": 0.269,
      "step": 3022
    },
    {
      "epoch": 0.07912645924581639,
      "grad_norm": 17.112476348876953,
      "learning_rate": 2.509290761580738e-06,
      "loss": 0.2275,
      "step": 3023
    },
    {
      "epoch": 0.07915263405866647,
      "grad_norm": 24.945154190063477,
      "learning_rate": 2.506150222454855e-06,
      "loss": 0.39,
      "step": 3024
    },
    {
      "epoch": 0.07917880887151657,
      "grad_norm": 17.59297752380371,
      "learning_rate": 2.5030096833289716e-06,
      "loss": 0.3548,
      "step": 3025
    },
    {
      "epoch": 0.07920498368436665,
      "grad_norm": 15.036012649536133,
      "learning_rate": 2.4998691442030883e-06,
      "loss": 0.2565,
      "step": 3026
    },
    {
      "epoch": 0.07923115849721675,
      "grad_norm": 14.147059440612793,
      "learning_rate": 2.496728605077205e-06,
      "loss": 0.233,
      "step": 3027
    },
    {
      "epoch": 0.07925733331006683,
      "grad_norm": 26.501310348510742,
      "learning_rate": 2.4935880659513217e-06,
      "loss": 0.4797,
      "step": 3028
    },
    {
      "epoch": 0.07928350812291692,
      "grad_norm": 12.80854606628418,
      "learning_rate": 2.4904475268254384e-06,
      "loss": 0.3181,
      "step": 3029
    },
    {
      "epoch": 0.07930968293576701,
      "grad_norm": 17.21854591369629,
      "learning_rate": 2.487306987699555e-06,
      "loss": 0.1634,
      "step": 3030
    },
    {
      "epoch": 0.0793358577486171,
      "grad_norm": 23.7236385345459,
      "learning_rate": 2.484166448573672e-06,
      "loss": 0.3421,
      "step": 3031
    },
    {
      "epoch": 0.07936203256146719,
      "grad_norm": 21.40606689453125,
      "learning_rate": 2.4810259094477885e-06,
      "loss": 0.4833,
      "step": 3032
    },
    {
      "epoch": 0.07938820737431727,
      "grad_norm": 13.710057258605957,
      "learning_rate": 2.4778853703219052e-06,
      "loss": 0.229,
      "step": 3033
    },
    {
      "epoch": 0.07941438218716736,
      "grad_norm": 17.03337287902832,
      "learning_rate": 2.474744831196022e-06,
      "loss": 0.269,
      "step": 3034
    },
    {
      "epoch": 0.07944055700001745,
      "grad_norm": 22.37879180908203,
      "learning_rate": 2.4716042920701387e-06,
      "loss": 0.5032,
      "step": 3035
    },
    {
      "epoch": 0.07946673181286754,
      "grad_norm": 9.062419891357422,
      "learning_rate": 2.4684637529442554e-06,
      "loss": 0.1399,
      "step": 3036
    },
    {
      "epoch": 0.07949290662571763,
      "grad_norm": 25.725526809692383,
      "learning_rate": 2.465323213818372e-06,
      "loss": 0.4673,
      "step": 3037
    },
    {
      "epoch": 0.07951908143856772,
      "grad_norm": 18.205888748168945,
      "learning_rate": 2.462182674692489e-06,
      "loss": 0.2439,
      "step": 3038
    },
    {
      "epoch": 0.0795452562514178,
      "grad_norm": 12.348811149597168,
      "learning_rate": 2.4590421355666055e-06,
      "loss": 0.2983,
      "step": 3039
    },
    {
      "epoch": 0.0795714310642679,
      "grad_norm": 20.973163604736328,
      "learning_rate": 2.455901596440722e-06,
      "loss": 0.2002,
      "step": 3040
    },
    {
      "epoch": 0.07959760587711798,
      "grad_norm": 20.666706085205078,
      "learning_rate": 2.4527610573148393e-06,
      "loss": 0.246,
      "step": 3041
    },
    {
      "epoch": 0.07962378068996806,
      "grad_norm": 28.019392013549805,
      "learning_rate": 2.449620518188956e-06,
      "loss": 0.4673,
      "step": 3042
    },
    {
      "epoch": 0.07964995550281816,
      "grad_norm": 20.39847183227539,
      "learning_rate": 2.4464799790630723e-06,
      "loss": 0.4309,
      "step": 3043
    },
    {
      "epoch": 0.07967613031566824,
      "grad_norm": 18.848264694213867,
      "learning_rate": 2.4433394399371894e-06,
      "loss": 0.1706,
      "step": 3044
    },
    {
      "epoch": 0.07970230512851834,
      "grad_norm": 28.125823974609375,
      "learning_rate": 2.440198900811306e-06,
      "loss": 0.2657,
      "step": 3045
    },
    {
      "epoch": 0.07972847994136842,
      "grad_norm": 20.861360549926758,
      "learning_rate": 2.437058361685423e-06,
      "loss": 0.3587,
      "step": 3046
    },
    {
      "epoch": 0.0797546547542185,
      "grad_norm": 12.676016807556152,
      "learning_rate": 2.433917822559539e-06,
      "loss": 0.1964,
      "step": 3047
    },
    {
      "epoch": 0.0797808295670686,
      "grad_norm": 21.127527236938477,
      "learning_rate": 2.4307772834336563e-06,
      "loss": 0.2996,
      "step": 3048
    },
    {
      "epoch": 0.07980700437991868,
      "grad_norm": 13.224482536315918,
      "learning_rate": 2.427636744307773e-06,
      "loss": 0.2156,
      "step": 3049
    },
    {
      "epoch": 0.07983317919276878,
      "grad_norm": 13.835731506347656,
      "learning_rate": 2.4244962051818897e-06,
      "loss": 0.2955,
      "step": 3050
    },
    {
      "epoch": 0.07985935400561886,
      "grad_norm": 33.15402603149414,
      "learning_rate": 2.4213556660560064e-06,
      "loss": 0.4401,
      "step": 3051
    },
    {
      "epoch": 0.07988552881846894,
      "grad_norm": 21.201833724975586,
      "learning_rate": 2.418215126930123e-06,
      "loss": 0.4559,
      "step": 3052
    },
    {
      "epoch": 0.07991170363131904,
      "grad_norm": 10.327737808227539,
      "learning_rate": 2.4150745878042398e-06,
      "loss": 0.1439,
      "step": 3053
    },
    {
      "epoch": 0.07993787844416912,
      "grad_norm": 16.589189529418945,
      "learning_rate": 2.4119340486783565e-06,
      "loss": 0.4235,
      "step": 3054
    },
    {
      "epoch": 0.07996405325701922,
      "grad_norm": 44.190860748291016,
      "learning_rate": 2.408793509552473e-06,
      "loss": 0.3958,
      "step": 3055
    },
    {
      "epoch": 0.0799902280698693,
      "grad_norm": 17.51262092590332,
      "learning_rate": 2.40565297042659e-06,
      "loss": 0.2365,
      "step": 3056
    },
    {
      "epoch": 0.08001640288271938,
      "grad_norm": 25.602394104003906,
      "learning_rate": 2.4025124313007066e-06,
      "loss": 0.3272,
      "step": 3057
    },
    {
      "epoch": 0.08004257769556948,
      "grad_norm": 27.395044326782227,
      "learning_rate": 2.3993718921748237e-06,
      "loss": 0.4176,
      "step": 3058
    },
    {
      "epoch": 0.08006875250841956,
      "grad_norm": 25.66107177734375,
      "learning_rate": 2.39623135304894e-06,
      "loss": 0.5087,
      "step": 3059
    },
    {
      "epoch": 0.08009492732126965,
      "grad_norm": 18.79932975769043,
      "learning_rate": 2.3930908139230567e-06,
      "loss": 0.3019,
      "step": 3060
    },
    {
      "epoch": 0.08012110213411974,
      "grad_norm": 15.638559341430664,
      "learning_rate": 2.389950274797174e-06,
      "loss": 0.3131,
      "step": 3061
    },
    {
      "epoch": 0.08014727694696983,
      "grad_norm": 26.72919464111328,
      "learning_rate": 2.3868097356712906e-06,
      "loss": 0.4147,
      "step": 3062
    },
    {
      "epoch": 0.08017345175981992,
      "grad_norm": 14.06126880645752,
      "learning_rate": 2.383669196545407e-06,
      "loss": 0.1503,
      "step": 3063
    },
    {
      "epoch": 0.08019962657267,
      "grad_norm": 15.289724349975586,
      "learning_rate": 2.3805286574195236e-06,
      "loss": 0.2462,
      "step": 3064
    },
    {
      "epoch": 0.08022580138552009,
      "grad_norm": 20.346506118774414,
      "learning_rate": 2.3773881182936407e-06,
      "loss": 0.3513,
      "step": 3065
    },
    {
      "epoch": 0.08025197619837018,
      "grad_norm": 21.533592224121094,
      "learning_rate": 2.374247579167757e-06,
      "loss": 0.4015,
      "step": 3066
    },
    {
      "epoch": 0.08027815101122027,
      "grad_norm": 16.904115676879883,
      "learning_rate": 2.3711070400418737e-06,
      "loss": 0.3771,
      "step": 3067
    },
    {
      "epoch": 0.08030432582407036,
      "grad_norm": 10.38956356048584,
      "learning_rate": 2.367966500915991e-06,
      "loss": 0.2631,
      "step": 3068
    },
    {
      "epoch": 0.08033050063692045,
      "grad_norm": 16.5545654296875,
      "learning_rate": 2.3648259617901075e-06,
      "loss": 0.259,
      "step": 3069
    },
    {
      "epoch": 0.08035667544977053,
      "grad_norm": 15.364407539367676,
      "learning_rate": 2.361685422664224e-06,
      "loss": 0.1935,
      "step": 3070
    },
    {
      "epoch": 0.08038285026262063,
      "grad_norm": 31.87614631652832,
      "learning_rate": 2.358544883538341e-06,
      "loss": 0.4051,
      "step": 3071
    },
    {
      "epoch": 0.08040902507547071,
      "grad_norm": 19.649221420288086,
      "learning_rate": 2.3554043444124576e-06,
      "loss": 0.2031,
      "step": 3072
    },
    {
      "epoch": 0.0804351998883208,
      "grad_norm": 26.57266616821289,
      "learning_rate": 2.3522638052865743e-06,
      "loss": 0.4154,
      "step": 3073
    },
    {
      "epoch": 0.08046137470117089,
      "grad_norm": 18.66282844543457,
      "learning_rate": 2.349123266160691e-06,
      "loss": 0.2722,
      "step": 3074
    },
    {
      "epoch": 0.08048754951402097,
      "grad_norm": 12.980267524719238,
      "learning_rate": 2.3459827270348077e-06,
      "loss": 0.3508,
      "step": 3075
    },
    {
      "epoch": 0.08051372432687107,
      "grad_norm": 20.589778900146484,
      "learning_rate": 2.3428421879089245e-06,
      "loss": 0.3386,
      "step": 3076
    },
    {
      "epoch": 0.08053989913972115,
      "grad_norm": 19.740758895874023,
      "learning_rate": 2.339701648783041e-06,
      "loss": 0.3647,
      "step": 3077
    },
    {
      "epoch": 0.08056607395257125,
      "grad_norm": 16.269668579101562,
      "learning_rate": 2.336561109657158e-06,
      "loss": 0.2666,
      "step": 3078
    },
    {
      "epoch": 0.08059224876542133,
      "grad_norm": 13.007610321044922,
      "learning_rate": 2.3334205705312746e-06,
      "loss": 0.3137,
      "step": 3079
    },
    {
      "epoch": 0.08061842357827141,
      "grad_norm": 17.069597244262695,
      "learning_rate": 2.3302800314053913e-06,
      "loss": 0.2499,
      "step": 3080
    },
    {
      "epoch": 0.08064459839112151,
      "grad_norm": 23.322507858276367,
      "learning_rate": 2.327139492279508e-06,
      "loss": 0.2878,
      "step": 3081
    },
    {
      "epoch": 0.08067077320397159,
      "grad_norm": 25.614274978637695,
      "learning_rate": 2.3239989531536247e-06,
      "loss": 0.3341,
      "step": 3082
    },
    {
      "epoch": 0.08069694801682167,
      "grad_norm": 11.538199424743652,
      "learning_rate": 2.3208584140277414e-06,
      "loss": 0.2002,
      "step": 3083
    },
    {
      "epoch": 0.08072312282967177,
      "grad_norm": 25.203067779541016,
      "learning_rate": 2.317717874901858e-06,
      "loss": 0.4678,
      "step": 3084
    },
    {
      "epoch": 0.08074929764252185,
      "grad_norm": 29.33245849609375,
      "learning_rate": 2.3145773357759752e-06,
      "loss": 0.4307,
      "step": 3085
    },
    {
      "epoch": 0.08077547245537195,
      "grad_norm": 27.453763961791992,
      "learning_rate": 2.3114367966500915e-06,
      "loss": 0.4211,
      "step": 3086
    },
    {
      "epoch": 0.08080164726822203,
      "grad_norm": 22.540403366088867,
      "learning_rate": 2.3082962575242082e-06,
      "loss": 0.3387,
      "step": 3087
    },
    {
      "epoch": 0.08082782208107211,
      "grad_norm": 23.13062858581543,
      "learning_rate": 2.3051557183983254e-06,
      "loss": 0.2841,
      "step": 3088
    },
    {
      "epoch": 0.08085399689392221,
      "grad_norm": 13.816006660461426,
      "learning_rate": 2.302015179272442e-06,
      "loss": 0.3063,
      "step": 3089
    },
    {
      "epoch": 0.0808801717067723,
      "grad_norm": 21.129716873168945,
      "learning_rate": 2.2988746401465583e-06,
      "loss": 0.3716,
      "step": 3090
    },
    {
      "epoch": 0.08090634651962239,
      "grad_norm": 14.260115623474121,
      "learning_rate": 2.2957341010206755e-06,
      "loss": 0.2979,
      "step": 3091
    },
    {
      "epoch": 0.08093252133247247,
      "grad_norm": 18.41539192199707,
      "learning_rate": 2.292593561894792e-06,
      "loss": 0.3639,
      "step": 3092
    },
    {
      "epoch": 0.08095869614532256,
      "grad_norm": 20.56191635131836,
      "learning_rate": 2.2894530227689085e-06,
      "loss": 0.5125,
      "step": 3093
    },
    {
      "epoch": 0.08098487095817265,
      "grad_norm": 21.777475357055664,
      "learning_rate": 2.286312483643025e-06,
      "loss": 0.3796,
      "step": 3094
    },
    {
      "epoch": 0.08101104577102274,
      "grad_norm": 18.926671981811523,
      "learning_rate": 2.2831719445171423e-06,
      "loss": 0.3305,
      "step": 3095
    },
    {
      "epoch": 0.08103722058387283,
      "grad_norm": 18.563446044921875,
      "learning_rate": 2.280031405391259e-06,
      "loss": 0.4186,
      "step": 3096
    },
    {
      "epoch": 0.08106339539672291,
      "grad_norm": 12.066726684570312,
      "learning_rate": 2.2768908662653753e-06,
      "loss": 0.1718,
      "step": 3097
    },
    {
      "epoch": 0.081089570209573,
      "grad_norm": 28.478595733642578,
      "learning_rate": 2.2737503271394924e-06,
      "loss": 0.5,
      "step": 3098
    },
    {
      "epoch": 0.0811157450224231,
      "grad_norm": 29.250585556030273,
      "learning_rate": 2.270609788013609e-06,
      "loss": 0.4977,
      "step": 3099
    },
    {
      "epoch": 0.08114191983527318,
      "grad_norm": 30.22212028503418,
      "learning_rate": 2.267469248887726e-06,
      "loss": 0.5142,
      "step": 3100
    },
    {
      "epoch": 0.08116809464812326,
      "grad_norm": 21.915502548217773,
      "learning_rate": 2.2643287097618425e-06,
      "loss": 0.4817,
      "step": 3101
    },
    {
      "epoch": 0.08119426946097336,
      "grad_norm": 20.402109146118164,
      "learning_rate": 2.2611881706359592e-06,
      "loss": 0.3804,
      "step": 3102
    },
    {
      "epoch": 0.08122044427382344,
      "grad_norm": 16.05240821838379,
      "learning_rate": 2.258047631510076e-06,
      "loss": 0.2665,
      "step": 3103
    },
    {
      "epoch": 0.08124661908667354,
      "grad_norm": 20.650453567504883,
      "learning_rate": 2.2549070923841927e-06,
      "loss": 0.3322,
      "step": 3104
    },
    {
      "epoch": 0.08127279389952362,
      "grad_norm": 17.903366088867188,
      "learning_rate": 2.2517665532583094e-06,
      "loss": 0.2742,
      "step": 3105
    },
    {
      "epoch": 0.0812989687123737,
      "grad_norm": 11.144561767578125,
      "learning_rate": 2.248626014132426e-06,
      "loss": 0.1897,
      "step": 3106
    },
    {
      "epoch": 0.0813251435252238,
      "grad_norm": 14.89513111114502,
      "learning_rate": 2.2454854750065428e-06,
      "loss": 0.2197,
      "step": 3107
    },
    {
      "epoch": 0.08135131833807388,
      "grad_norm": 19.82956314086914,
      "learning_rate": 2.24234493588066e-06,
      "loss": 0.328,
      "step": 3108
    },
    {
      "epoch": 0.08137749315092398,
      "grad_norm": 33.781253814697266,
      "learning_rate": 2.239204396754776e-06,
      "loss": 0.5195,
      "step": 3109
    },
    {
      "epoch": 0.08140366796377406,
      "grad_norm": 19.139968872070312,
      "learning_rate": 2.236063857628893e-06,
      "loss": 0.2413,
      "step": 3110
    },
    {
      "epoch": 0.08142984277662414,
      "grad_norm": 28.96484375,
      "learning_rate": 2.2329233185030096e-06,
      "loss": 0.4808,
      "step": 3111
    },
    {
      "epoch": 0.08145601758947424,
      "grad_norm": 25.043445587158203,
      "learning_rate": 2.2297827793771267e-06,
      "loss": 0.4189,
      "step": 3112
    },
    {
      "epoch": 0.08148219240232432,
      "grad_norm": 14.628673553466797,
      "learning_rate": 2.226642240251243e-06,
      "loss": 0.2422,
      "step": 3113
    },
    {
      "epoch": 0.08150836721517442,
      "grad_norm": 20.654125213623047,
      "learning_rate": 2.2235017011253597e-06,
      "loss": 0.2839,
      "step": 3114
    },
    {
      "epoch": 0.0815345420280245,
      "grad_norm": 13.033486366271973,
      "learning_rate": 2.220361161999477e-06,
      "loss": 0.282,
      "step": 3115
    },
    {
      "epoch": 0.08156071684087458,
      "grad_norm": 16.01743507385254,
      "learning_rate": 2.2172206228735935e-06,
      "loss": 0.3419,
      "step": 3116
    },
    {
      "epoch": 0.08158689165372468,
      "grad_norm": 21.68035888671875,
      "learning_rate": 2.21408008374771e-06,
      "loss": 0.1864,
      "step": 3117
    },
    {
      "epoch": 0.08161306646657476,
      "grad_norm": 20.166555404663086,
      "learning_rate": 2.210939544621827e-06,
      "loss": 0.3153,
      "step": 3118
    },
    {
      "epoch": 0.08163924127942485,
      "grad_norm": 11.9136381149292,
      "learning_rate": 2.2077990054959437e-06,
      "loss": 0.2341,
      "step": 3119
    },
    {
      "epoch": 0.08166541609227494,
      "grad_norm": 18.551273345947266,
      "learning_rate": 2.2046584663700604e-06,
      "loss": 0.262,
      "step": 3120
    },
    {
      "epoch": 0.08169159090512502,
      "grad_norm": 20.939908981323242,
      "learning_rate": 2.201517927244177e-06,
      "loss": 0.2456,
      "step": 3121
    },
    {
      "epoch": 0.08171776571797512,
      "grad_norm": 13.42402458190918,
      "learning_rate": 2.1983773881182938e-06,
      "loss": 0.3568,
      "step": 3122
    },
    {
      "epoch": 0.0817439405308252,
      "grad_norm": 11.651957511901855,
      "learning_rate": 2.1952368489924105e-06,
      "loss": 0.172,
      "step": 3123
    },
    {
      "epoch": 0.08177011534367529,
      "grad_norm": 19.608251571655273,
      "learning_rate": 2.1920963098665268e-06,
      "loss": 0.4578,
      "step": 3124
    },
    {
      "epoch": 0.08179629015652538,
      "grad_norm": 17.94828987121582,
      "learning_rate": 2.188955770740644e-06,
      "loss": 0.2471,
      "step": 3125
    },
    {
      "epoch": 0.08182246496937547,
      "grad_norm": 27.1224365234375,
      "learning_rate": 2.1858152316147606e-06,
      "loss": 0.5121,
      "step": 3126
    },
    {
      "epoch": 0.08184863978222556,
      "grad_norm": 15.899423599243164,
      "learning_rate": 2.1826746924888773e-06,
      "loss": 0.196,
      "step": 3127
    },
    {
      "epoch": 0.08187481459507565,
      "grad_norm": 13.73009967803955,
      "learning_rate": 2.179534153362994e-06,
      "loss": 0.1432,
      "step": 3128
    },
    {
      "epoch": 0.08190098940792573,
      "grad_norm": 16.722885131835938,
      "learning_rate": 2.1763936142371107e-06,
      "loss": 0.2279,
      "step": 3129
    },
    {
      "epoch": 0.08192716422077582,
      "grad_norm": 23.678632736206055,
      "learning_rate": 2.1732530751112274e-06,
      "loss": 0.5479,
      "step": 3130
    },
    {
      "epoch": 0.08195333903362591,
      "grad_norm": 14.866035461425781,
      "learning_rate": 2.170112535985344e-06,
      "loss": 0.2581,
      "step": 3131
    },
    {
      "epoch": 0.081979513846476,
      "grad_norm": 29.25700569152832,
      "learning_rate": 2.166971996859461e-06,
      "loss": 0.3252,
      "step": 3132
    },
    {
      "epoch": 0.08200568865932609,
      "grad_norm": 19.09695816040039,
      "learning_rate": 2.1638314577335776e-06,
      "loss": 0.3136,
      "step": 3133
    },
    {
      "epoch": 0.08203186347217617,
      "grad_norm": 24.1387996673584,
      "learning_rate": 2.1606909186076943e-06,
      "loss": 0.4952,
      "step": 3134
    },
    {
      "epoch": 0.08205803828502627,
      "grad_norm": 22.48598289489746,
      "learning_rate": 2.1575503794818114e-06,
      "loss": 0.2777,
      "step": 3135
    },
    {
      "epoch": 0.08208421309787635,
      "grad_norm": 18.948457717895508,
      "learning_rate": 2.1544098403559277e-06,
      "loss": 0.2338,
      "step": 3136
    },
    {
      "epoch": 0.08211038791072643,
      "grad_norm": 21.7851619720459,
      "learning_rate": 2.1512693012300444e-06,
      "loss": 0.4294,
      "step": 3137
    },
    {
      "epoch": 0.08213656272357653,
      "grad_norm": 10.970200538635254,
      "learning_rate": 2.1481287621041615e-06,
      "loss": 0.1191,
      "step": 3138
    },
    {
      "epoch": 0.08216273753642661,
      "grad_norm": 25.217164993286133,
      "learning_rate": 2.1449882229782782e-06,
      "loss": 0.4208,
      "step": 3139
    },
    {
      "epoch": 0.08218891234927671,
      "grad_norm": 19.9113826751709,
      "learning_rate": 2.1418476838523945e-06,
      "loss": 0.3581,
      "step": 3140
    },
    {
      "epoch": 0.08221508716212679,
      "grad_norm": 25.21657943725586,
      "learning_rate": 2.138707144726511e-06,
      "loss": 0.5215,
      "step": 3141
    },
    {
      "epoch": 0.08224126197497687,
      "grad_norm": 33.15159606933594,
      "learning_rate": 2.1355666056006283e-06,
      "loss": 0.5337,
      "step": 3142
    },
    {
      "epoch": 0.08226743678782697,
      "grad_norm": 25.9572696685791,
      "learning_rate": 2.132426066474745e-06,
      "loss": 0.418,
      "step": 3143
    },
    {
      "epoch": 0.08229361160067705,
      "grad_norm": 26.375490188598633,
      "learning_rate": 2.1292855273488613e-06,
      "loss": 0.5081,
      "step": 3144
    },
    {
      "epoch": 0.08231978641352715,
      "grad_norm": 16.63170051574707,
      "learning_rate": 2.1261449882229785e-06,
      "loss": 0.2797,
      "step": 3145
    },
    {
      "epoch": 0.08234596122637723,
      "grad_norm": 21.016263961791992,
      "learning_rate": 2.123004449097095e-06,
      "loss": 0.25,
      "step": 3146
    },
    {
      "epoch": 0.08237213603922731,
      "grad_norm": 12.44058609008789,
      "learning_rate": 2.119863909971212e-06,
      "loss": 0.2442,
      "step": 3147
    },
    {
      "epoch": 0.08239831085207741,
      "grad_norm": 24.84274673461914,
      "learning_rate": 2.1167233708453286e-06,
      "loss": 0.2713,
      "step": 3148
    },
    {
      "epoch": 0.0824244856649275,
      "grad_norm": 23.790327072143555,
      "learning_rate": 2.1135828317194453e-06,
      "loss": 0.2473,
      "step": 3149
    },
    {
      "epoch": 0.08245066047777759,
      "grad_norm": 20.63752555847168,
      "learning_rate": 2.110442292593562e-06,
      "loss": 0.5323,
      "step": 3150
    },
    {
      "epoch": 0.08247683529062767,
      "grad_norm": 17.618789672851562,
      "learning_rate": 2.1073017534676787e-06,
      "loss": 0.1419,
      "step": 3151
    },
    {
      "epoch": 0.08250301010347776,
      "grad_norm": 11.662930488586426,
      "learning_rate": 2.1041612143417954e-06,
      "loss": 0.2325,
      "step": 3152
    },
    {
      "epoch": 0.08252918491632785,
      "grad_norm": 28.010156631469727,
      "learning_rate": 2.101020675215912e-06,
      "loss": 0.2177,
      "step": 3153
    },
    {
      "epoch": 0.08255535972917794,
      "grad_norm": 13.449740409851074,
      "learning_rate": 2.097880136090029e-06,
      "loss": 0.1708,
      "step": 3154
    },
    {
      "epoch": 0.08258153454202802,
      "grad_norm": 17.052610397338867,
      "learning_rate": 2.0947395969641455e-06,
      "loss": 0.3569,
      "step": 3155
    },
    {
      "epoch": 0.08260770935487811,
      "grad_norm": 26.742084503173828,
      "learning_rate": 2.0915990578382622e-06,
      "loss": 0.5761,
      "step": 3156
    },
    {
      "epoch": 0.0826338841677282,
      "grad_norm": 14.888005256652832,
      "learning_rate": 2.088458518712379e-06,
      "loss": 0.2249,
      "step": 3157
    },
    {
      "epoch": 0.0826600589805783,
      "grad_norm": 13.830554962158203,
      "learning_rate": 2.0853179795864956e-06,
      "loss": 0.3367,
      "step": 3158
    },
    {
      "epoch": 0.08268623379342838,
      "grad_norm": 41.05617141723633,
      "learning_rate": 2.0821774404606123e-06,
      "loss": 0.5102,
      "step": 3159
    },
    {
      "epoch": 0.08271240860627846,
      "grad_norm": 18.970306396484375,
      "learning_rate": 2.079036901334729e-06,
      "loss": 0.3374,
      "step": 3160
    },
    {
      "epoch": 0.08273858341912856,
      "grad_norm": 16.60548210144043,
      "learning_rate": 2.0758963622088458e-06,
      "loss": 0.2639,
      "step": 3161
    },
    {
      "epoch": 0.08276475823197864,
      "grad_norm": 18.577251434326172,
      "learning_rate": 2.072755823082963e-06,
      "loss": 0.422,
      "step": 3162
    },
    {
      "epoch": 0.08279093304482874,
      "grad_norm": 14.955747604370117,
      "learning_rate": 2.069615283957079e-06,
      "loss": 0.1648,
      "step": 3163
    },
    {
      "epoch": 0.08281710785767882,
      "grad_norm": 18.703502655029297,
      "learning_rate": 2.066474744831196e-06,
      "loss": 0.3614,
      "step": 3164
    },
    {
      "epoch": 0.0828432826705289,
      "grad_norm": 30.460899353027344,
      "learning_rate": 2.063334205705313e-06,
      "loss": 0.4213,
      "step": 3165
    },
    {
      "epoch": 0.082869457483379,
      "grad_norm": 16.484848022460938,
      "learning_rate": 2.0601936665794297e-06,
      "loss": 0.3696,
      "step": 3166
    },
    {
      "epoch": 0.08289563229622908,
      "grad_norm": 13.2991361618042,
      "learning_rate": 2.057053127453546e-06,
      "loss": 0.2054,
      "step": 3167
    },
    {
      "epoch": 0.08292180710907918,
      "grad_norm": 24.52594757080078,
      "learning_rate": 2.053912588327663e-06,
      "loss": 0.3825,
      "step": 3168
    },
    {
      "epoch": 0.08294798192192926,
      "grad_norm": 17.95149803161621,
      "learning_rate": 2.05077204920178e-06,
      "loss": 0.5388,
      "step": 3169
    },
    {
      "epoch": 0.08297415673477934,
      "grad_norm": 19.760446548461914,
      "learning_rate": 2.0476315100758965e-06,
      "loss": 0.2939,
      "step": 3170
    },
    {
      "epoch": 0.08300033154762944,
      "grad_norm": 14.206110000610352,
      "learning_rate": 2.0444909709500132e-06,
      "loss": 0.2297,
      "step": 3171
    },
    {
      "epoch": 0.08302650636047952,
      "grad_norm": 25.975187301635742,
      "learning_rate": 2.04135043182413e-06,
      "loss": 0.5191,
      "step": 3172
    },
    {
      "epoch": 0.0830526811733296,
      "grad_norm": 18.710834503173828,
      "learning_rate": 2.0382098926982467e-06,
      "loss": 0.4065,
      "step": 3173
    },
    {
      "epoch": 0.0830788559861797,
      "grad_norm": 22.22772216796875,
      "learning_rate": 2.0350693535723634e-06,
      "loss": 0.372,
      "step": 3174
    },
    {
      "epoch": 0.08310503079902978,
      "grad_norm": 20.713796615600586,
      "learning_rate": 2.03192881444648e-06,
      "loss": 0.2444,
      "step": 3175
    },
    {
      "epoch": 0.08313120561187988,
      "grad_norm": 12.72546100616455,
      "learning_rate": 2.0287882753205968e-06,
      "loss": 0.3573,
      "step": 3176
    },
    {
      "epoch": 0.08315738042472996,
      "grad_norm": 16.302709579467773,
      "learning_rate": 2.0256477361947135e-06,
      "loss": 0.43,
      "step": 3177
    },
    {
      "epoch": 0.08318355523758005,
      "grad_norm": 17.133180618286133,
      "learning_rate": 2.02250719706883e-06,
      "loss": 0.4593,
      "step": 3178
    },
    {
      "epoch": 0.08320973005043014,
      "grad_norm": 18.228246688842773,
      "learning_rate": 2.019366657942947e-06,
      "loss": 0.3465,
      "step": 3179
    },
    {
      "epoch": 0.08323590486328022,
      "grad_norm": 10.466468811035156,
      "learning_rate": 2.0162261188170636e-06,
      "loss": 0.1029,
      "step": 3180
    },
    {
      "epoch": 0.08326207967613032,
      "grad_norm": 19.97140884399414,
      "learning_rate": 2.0130855796911803e-06,
      "loss": 0.4928,
      "step": 3181
    },
    {
      "epoch": 0.0832882544889804,
      "grad_norm": 22.35940170288086,
      "learning_rate": 2.009945040565297e-06,
      "loss": 0.5029,
      "step": 3182
    },
    {
      "epoch": 0.08331442930183049,
      "grad_norm": 34.353546142578125,
      "learning_rate": 2.0068045014394137e-06,
      "loss": 0.6073,
      "step": 3183
    },
    {
      "epoch": 0.08334060411468058,
      "grad_norm": 14.310636520385742,
      "learning_rate": 2.0036639623135304e-06,
      "loss": 0.2512,
      "step": 3184
    },
    {
      "epoch": 0.08336677892753067,
      "grad_norm": 28.407609939575195,
      "learning_rate": 2.0005234231876475e-06,
      "loss": 0.3977,
      "step": 3185
    },
    {
      "epoch": 0.08339295374038076,
      "grad_norm": 21.434446334838867,
      "learning_rate": 1.997382884061764e-06,
      "loss": 0.3385,
      "step": 3186
    },
    {
      "epoch": 0.08341912855323085,
      "grad_norm": 14.1892671585083,
      "learning_rate": 1.9942423449358805e-06,
      "loss": 0.2045,
      "step": 3187
    },
    {
      "epoch": 0.08344530336608093,
      "grad_norm": 23.63383674621582,
      "learning_rate": 1.9911018058099972e-06,
      "loss": 0.4516,
      "step": 3188
    },
    {
      "epoch": 0.08347147817893102,
      "grad_norm": 25.201263427734375,
      "learning_rate": 1.9879612666841144e-06,
      "loss": 0.3735,
      "step": 3189
    },
    {
      "epoch": 0.08349765299178111,
      "grad_norm": 25.894819259643555,
      "learning_rate": 1.9848207275582307e-06,
      "loss": 0.341,
      "step": 3190
    },
    {
      "epoch": 0.0835238278046312,
      "grad_norm": 16.00188446044922,
      "learning_rate": 1.9816801884323474e-06,
      "loss": 0.325,
      "step": 3191
    },
    {
      "epoch": 0.08355000261748129,
      "grad_norm": 21.46156120300293,
      "learning_rate": 1.9785396493064645e-06,
      "loss": 0.3928,
      "step": 3192
    },
    {
      "epoch": 0.08357617743033137,
      "grad_norm": 33.10923767089844,
      "learning_rate": 1.975399110180581e-06,
      "loss": 0.4838,
      "step": 3193
    },
    {
      "epoch": 0.08360235224318147,
      "grad_norm": 17.440784454345703,
      "learning_rate": 1.9722585710546975e-06,
      "loss": 0.2354,
      "step": 3194
    },
    {
      "epoch": 0.08362852705603155,
      "grad_norm": 17.4401798248291,
      "learning_rate": 1.9691180319288146e-06,
      "loss": 0.1788,
      "step": 3195
    },
    {
      "epoch": 0.08365470186888163,
      "grad_norm": 15.594352722167969,
      "learning_rate": 1.9659774928029313e-06,
      "loss": 0.1844,
      "step": 3196
    },
    {
      "epoch": 0.08368087668173173,
      "grad_norm": 13.933838844299316,
      "learning_rate": 1.962836953677048e-06,
      "loss": 0.2486,
      "step": 3197
    },
    {
      "epoch": 0.08370705149458181,
      "grad_norm": 24.299358367919922,
      "learning_rate": 1.9596964145511647e-06,
      "loss": 0.3617,
      "step": 3198
    },
    {
      "epoch": 0.08373322630743191,
      "grad_norm": 21.52250862121582,
      "learning_rate": 1.9565558754252814e-06,
      "loss": 0.3304,
      "step": 3199
    },
    {
      "epoch": 0.08375940112028199,
      "grad_norm": 16.91095542907715,
      "learning_rate": 1.953415336299398e-06,
      "loss": 0.3249,
      "step": 3200
    },
    {
      "epoch": 0.08378557593313207,
      "grad_norm": 23.448162078857422,
      "learning_rate": 1.950274797173515e-06,
      "loss": 0.3942,
      "step": 3201
    },
    {
      "epoch": 0.08381175074598217,
      "grad_norm": 30.58951187133789,
      "learning_rate": 1.9471342580476316e-06,
      "loss": 0.5087,
      "step": 3202
    },
    {
      "epoch": 0.08383792555883225,
      "grad_norm": 25.236661911010742,
      "learning_rate": 1.9439937189217483e-06,
      "loss": 0.5247,
      "step": 3203
    },
    {
      "epoch": 0.08386410037168235,
      "grad_norm": 17.661518096923828,
      "learning_rate": 1.940853179795865e-06,
      "loss": 0.3135,
      "step": 3204
    },
    {
      "epoch": 0.08389027518453243,
      "grad_norm": 25.53377914428711,
      "learning_rate": 1.9377126406699817e-06,
      "loss": 0.3161,
      "step": 3205
    },
    {
      "epoch": 0.08391644999738251,
      "grad_norm": 24.670215606689453,
      "learning_rate": 1.9345721015440984e-06,
      "loss": 0.4894,
      "step": 3206
    },
    {
      "epoch": 0.08394262481023261,
      "grad_norm": 20.113847732543945,
      "learning_rate": 1.931431562418215e-06,
      "loss": 0.329,
      "step": 3207
    },
    {
      "epoch": 0.0839687996230827,
      "grad_norm": 16.60741424560547,
      "learning_rate": 1.928291023292332e-06,
      "loss": 0.4257,
      "step": 3208
    },
    {
      "epoch": 0.08399497443593279,
      "grad_norm": 37.82706832885742,
      "learning_rate": 1.925150484166449e-06,
      "loss": 0.5005,
      "step": 3209
    },
    {
      "epoch": 0.08402114924878287,
      "grad_norm": 18.600536346435547,
      "learning_rate": 1.922009945040565e-06,
      "loss": 0.325,
      "step": 3210
    },
    {
      "epoch": 0.08404732406163296,
      "grad_norm": 22.71414566040039,
      "learning_rate": 1.918869405914682e-06,
      "loss": 0.3089,
      "step": 3211
    },
    {
      "epoch": 0.08407349887448305,
      "grad_norm": 25.280214309692383,
      "learning_rate": 1.915728866788799e-06,
      "loss": 0.3952,
      "step": 3212
    },
    {
      "epoch": 0.08409967368733313,
      "grad_norm": 32.496681213378906,
      "learning_rate": 1.9125883276629153e-06,
      "loss": 0.4227,
      "step": 3213
    },
    {
      "epoch": 0.08412584850018322,
      "grad_norm": 31.03504180908203,
      "learning_rate": 1.909447788537032e-06,
      "loss": 0.4326,
      "step": 3214
    },
    {
      "epoch": 0.08415202331303331,
      "grad_norm": 13.087346076965332,
      "learning_rate": 1.906307249411149e-06,
      "loss": 0.1529,
      "step": 3215
    },
    {
      "epoch": 0.0841781981258834,
      "grad_norm": 24.734994888305664,
      "learning_rate": 1.9031667102852659e-06,
      "loss": 0.31,
      "step": 3216
    },
    {
      "epoch": 0.0842043729387335,
      "grad_norm": 13.795331001281738,
      "learning_rate": 1.9000261711593824e-06,
      "loss": 0.2917,
      "step": 3217
    },
    {
      "epoch": 0.08423054775158358,
      "grad_norm": 17.950422286987305,
      "learning_rate": 1.896885632033499e-06,
      "loss": 0.4145,
      "step": 3218
    },
    {
      "epoch": 0.08425672256443366,
      "grad_norm": 26.016021728515625,
      "learning_rate": 1.893745092907616e-06,
      "loss": 0.4175,
      "step": 3219
    },
    {
      "epoch": 0.08428289737728376,
      "grad_norm": 16.690683364868164,
      "learning_rate": 1.8906045537817327e-06,
      "loss": 0.2886,
      "step": 3220
    },
    {
      "epoch": 0.08430907219013384,
      "grad_norm": 19.41027069091797,
      "learning_rate": 1.8874640146558492e-06,
      "loss": 0.3737,
      "step": 3221
    },
    {
      "epoch": 0.08433524700298393,
      "grad_norm": 25.291404724121094,
      "learning_rate": 1.8843234755299659e-06,
      "loss": 0.451,
      "step": 3222
    },
    {
      "epoch": 0.08436142181583402,
      "grad_norm": 23.15741539001465,
      "learning_rate": 1.8811829364040828e-06,
      "loss": 0.2971,
      "step": 3223
    },
    {
      "epoch": 0.0843875966286841,
      "grad_norm": 18.35837173461914,
      "learning_rate": 1.8780423972781995e-06,
      "loss": 0.3113,
      "step": 3224
    },
    {
      "epoch": 0.0844137714415342,
      "grad_norm": 20.281536102294922,
      "learning_rate": 1.874901858152316e-06,
      "loss": 0.2634,
      "step": 3225
    },
    {
      "epoch": 0.08443994625438428,
      "grad_norm": 23.024272918701172,
      "learning_rate": 1.871761319026433e-06,
      "loss": 0.4385,
      "step": 3226
    },
    {
      "epoch": 0.08446612106723438,
      "grad_norm": 13.295486450195312,
      "learning_rate": 1.8686207799005496e-06,
      "loss": 0.3314,
      "step": 3227
    },
    {
      "epoch": 0.08449229588008446,
      "grad_norm": 29.047800064086914,
      "learning_rate": 1.8654802407746666e-06,
      "loss": 0.4775,
      "step": 3228
    },
    {
      "epoch": 0.08451847069293454,
      "grad_norm": 20.37081527709961,
      "learning_rate": 1.862339701648783e-06,
      "loss": 0.2751,
      "step": 3229
    },
    {
      "epoch": 0.08454464550578464,
      "grad_norm": 27.827529907226562,
      "learning_rate": 1.8591991625228998e-06,
      "loss": 0.4828,
      "step": 3230
    },
    {
      "epoch": 0.08457082031863472,
      "grad_norm": 21.899303436279297,
      "learning_rate": 1.8560586233970167e-06,
      "loss": 0.3651,
      "step": 3231
    },
    {
      "epoch": 0.0845969951314848,
      "grad_norm": 36.30252456665039,
      "learning_rate": 1.8529180842711334e-06,
      "loss": 0.6106,
      "step": 3232
    },
    {
      "epoch": 0.0846231699443349,
      "grad_norm": 21.28132438659668,
      "learning_rate": 1.8497775451452499e-06,
      "loss": 0.2011,
      "step": 3233
    },
    {
      "epoch": 0.08464934475718498,
      "grad_norm": 30.801088333129883,
      "learning_rate": 1.8466370060193668e-06,
      "loss": 0.4085,
      "step": 3234
    },
    {
      "epoch": 0.08467551957003508,
      "grad_norm": 22.754215240478516,
      "learning_rate": 1.8434964668934835e-06,
      "loss": 0.3394,
      "step": 3235
    },
    {
      "epoch": 0.08470169438288516,
      "grad_norm": 22.812192916870117,
      "learning_rate": 1.8403559277676004e-06,
      "loss": 0.5876,
      "step": 3236
    },
    {
      "epoch": 0.08472786919573524,
      "grad_norm": 12.502974510192871,
      "learning_rate": 1.8372153886417167e-06,
      "loss": 0.1917,
      "step": 3237
    },
    {
      "epoch": 0.08475404400858534,
      "grad_norm": 13.551673889160156,
      "learning_rate": 1.8340748495158336e-06,
      "loss": 0.2888,
      "step": 3238
    },
    {
      "epoch": 0.08478021882143542,
      "grad_norm": 24.56985092163086,
      "learning_rate": 1.8309343103899503e-06,
      "loss": 0.4265,
      "step": 3239
    },
    {
      "epoch": 0.08480639363428552,
      "grad_norm": 14.378690719604492,
      "learning_rate": 1.8277937712640668e-06,
      "loss": 0.2346,
      "step": 3240
    },
    {
      "epoch": 0.0848325684471356,
      "grad_norm": 22.661785125732422,
      "learning_rate": 1.8246532321381837e-06,
      "loss": 0.2886,
      "step": 3241
    },
    {
      "epoch": 0.08485874325998569,
      "grad_norm": 18.09503936767578,
      "learning_rate": 1.8215126930123004e-06,
      "loss": 0.251,
      "step": 3242
    },
    {
      "epoch": 0.08488491807283578,
      "grad_norm": 26.884418487548828,
      "learning_rate": 1.8183721538864174e-06,
      "loss": 0.4413,
      "step": 3243
    },
    {
      "epoch": 0.08491109288568587,
      "grad_norm": 21.50309181213379,
      "learning_rate": 1.8152316147605339e-06,
      "loss": 0.396,
      "step": 3244
    },
    {
      "epoch": 0.08493726769853596,
      "grad_norm": 26.039640426635742,
      "learning_rate": 1.8120910756346506e-06,
      "loss": 0.5179,
      "step": 3245
    },
    {
      "epoch": 0.08496344251138604,
      "grad_norm": 31.54339027404785,
      "learning_rate": 1.8089505365087675e-06,
      "loss": 0.4994,
      "step": 3246
    },
    {
      "epoch": 0.08498961732423613,
      "grad_norm": 24.375253677368164,
      "learning_rate": 1.8058099973828842e-06,
      "loss": 0.3386,
      "step": 3247
    },
    {
      "epoch": 0.08501579213708622,
      "grad_norm": 10.474702835083008,
      "learning_rate": 1.8026694582570007e-06,
      "loss": 0.2581,
      "step": 3248
    },
    {
      "epoch": 0.0850419669499363,
      "grad_norm": 25.245534896850586,
      "learning_rate": 1.7995289191311176e-06,
      "loss": 0.3575,
      "step": 3249
    },
    {
      "epoch": 0.08506814176278639,
      "grad_norm": 18.403602600097656,
      "learning_rate": 1.7963883800052343e-06,
      "loss": 0.2594,
      "step": 3250
    },
    {
      "epoch": 0.08509431657563649,
      "grad_norm": 26.232383728027344,
      "learning_rate": 1.7932478408793512e-06,
      "loss": 0.2904,
      "step": 3251
    },
    {
      "epoch": 0.08512049138848657,
      "grad_norm": 21.629661560058594,
      "learning_rate": 1.7901073017534675e-06,
      "loss": 0.3385,
      "step": 3252
    },
    {
      "epoch": 0.08514666620133667,
      "grad_norm": 17.29800033569336,
      "learning_rate": 1.7869667626275844e-06,
      "loss": 0.2498,
      "step": 3253
    },
    {
      "epoch": 0.08517284101418675,
      "grad_norm": 18.859756469726562,
      "learning_rate": 1.7838262235017011e-06,
      "loss": 0.306,
      "step": 3254
    },
    {
      "epoch": 0.08519901582703683,
      "grad_norm": 23.737213134765625,
      "learning_rate": 1.780685684375818e-06,
      "loss": 0.4476,
      "step": 3255
    },
    {
      "epoch": 0.08522519063988693,
      "grad_norm": 26.41429901123047,
      "learning_rate": 1.7775451452499345e-06,
      "loss": 0.35,
      "step": 3256
    },
    {
      "epoch": 0.08525136545273701,
      "grad_norm": 13.373008728027344,
      "learning_rate": 1.7744046061240512e-06,
      "loss": 0.2798,
      "step": 3257
    },
    {
      "epoch": 0.0852775402655871,
      "grad_norm": 18.094451904296875,
      "learning_rate": 1.7712640669981682e-06,
      "loss": 0.2592,
      "step": 3258
    },
    {
      "epoch": 0.08530371507843719,
      "grad_norm": 36.67206954956055,
      "learning_rate": 1.7681235278722849e-06,
      "loss": 0.337,
      "step": 3259
    },
    {
      "epoch": 0.08532988989128727,
      "grad_norm": 24.139934539794922,
      "learning_rate": 1.7649829887464014e-06,
      "loss": 0.3002,
      "step": 3260
    },
    {
      "epoch": 0.08535606470413737,
      "grad_norm": 17.869186401367188,
      "learning_rate": 1.7618424496205183e-06,
      "loss": 0.3143,
      "step": 3261
    },
    {
      "epoch": 0.08538223951698745,
      "grad_norm": 21.608564376831055,
      "learning_rate": 1.758701910494635e-06,
      "loss": 0.512,
      "step": 3262
    },
    {
      "epoch": 0.08540841432983755,
      "grad_norm": 15.67666244506836,
      "learning_rate": 1.755561371368752e-06,
      "loss": 0.374,
      "step": 3263
    },
    {
      "epoch": 0.08543458914268763,
      "grad_norm": 23.985450744628906,
      "learning_rate": 1.7524208322428684e-06,
      "loss": 0.4162,
      "step": 3264
    },
    {
      "epoch": 0.08546076395553771,
      "grad_norm": 15.539502143859863,
      "learning_rate": 1.749280293116985e-06,
      "loss": 0.3186,
      "step": 3265
    },
    {
      "epoch": 0.08548693876838781,
      "grad_norm": 26.106306076049805,
      "learning_rate": 1.746139753991102e-06,
      "loss": 0.4084,
      "step": 3266
    },
    {
      "epoch": 0.08551311358123789,
      "grad_norm": 19.763566970825195,
      "learning_rate": 1.7429992148652187e-06,
      "loss": 0.3358,
      "step": 3267
    },
    {
      "epoch": 0.08553928839408798,
      "grad_norm": 24.369691848754883,
      "learning_rate": 1.7398586757393352e-06,
      "loss": 0.3457,
      "step": 3268
    },
    {
      "epoch": 0.08556546320693807,
      "grad_norm": 25.844247817993164,
      "learning_rate": 1.736718136613452e-06,
      "loss": 0.4819,
      "step": 3269
    },
    {
      "epoch": 0.08559163801978815,
      "grad_norm": 13.575057983398438,
      "learning_rate": 1.7335775974875688e-06,
      "loss": 0.2352,
      "step": 3270
    },
    {
      "epoch": 0.08561781283263825,
      "grad_norm": 12.987674713134766,
      "learning_rate": 1.7304370583616853e-06,
      "loss": 0.1694,
      "step": 3271
    },
    {
      "epoch": 0.08564398764548833,
      "grad_norm": 17.367053985595703,
      "learning_rate": 1.727296519235802e-06,
      "loss": 0.3141,
      "step": 3272
    },
    {
      "epoch": 0.08567016245833842,
      "grad_norm": 23.393245697021484,
      "learning_rate": 1.724155980109919e-06,
      "loss": 0.5192,
      "step": 3273
    },
    {
      "epoch": 0.08569633727118851,
      "grad_norm": 25.34641456604004,
      "learning_rate": 1.7210154409840357e-06,
      "loss": 0.3476,
      "step": 3274
    },
    {
      "epoch": 0.0857225120840386,
      "grad_norm": 20.844167709350586,
      "learning_rate": 1.7178749018581522e-06,
      "loss": 0.5382,
      "step": 3275
    },
    {
      "epoch": 0.08574868689688869,
      "grad_norm": 23.187137603759766,
      "learning_rate": 1.714734362732269e-06,
      "loss": 0.3891,
      "step": 3276
    },
    {
      "epoch": 0.08577486170973878,
      "grad_norm": 16.752521514892578,
      "learning_rate": 1.7115938236063858e-06,
      "loss": 0.2816,
      "step": 3277
    },
    {
      "epoch": 0.08580103652258886,
      "grad_norm": 15.706387519836426,
      "learning_rate": 1.7084532844805027e-06,
      "loss": 0.2608,
      "step": 3278
    },
    {
      "epoch": 0.08582721133543895,
      "grad_norm": 27.156389236450195,
      "learning_rate": 1.7053127453546192e-06,
      "loss": 0.4175,
      "step": 3279
    },
    {
      "epoch": 0.08585338614828904,
      "grad_norm": 21.774555206298828,
      "learning_rate": 1.702172206228736e-06,
      "loss": 0.5959,
      "step": 3280
    },
    {
      "epoch": 0.08587956096113913,
      "grad_norm": 21.102598190307617,
      "learning_rate": 1.6990316671028528e-06,
      "loss": 0.4525,
      "step": 3281
    },
    {
      "epoch": 0.08590573577398922,
      "grad_norm": 19.509164810180664,
      "learning_rate": 1.6958911279769695e-06,
      "loss": 0.247,
      "step": 3282
    },
    {
      "epoch": 0.0859319105868393,
      "grad_norm": 13.63710880279541,
      "learning_rate": 1.692750588851086e-06,
      "loss": 0.3024,
      "step": 3283
    },
    {
      "epoch": 0.0859580853996894,
      "grad_norm": 10.670684814453125,
      "learning_rate": 1.6896100497252027e-06,
      "loss": 0.1672,
      "step": 3284
    },
    {
      "epoch": 0.08598426021253948,
      "grad_norm": 15.72222900390625,
      "learning_rate": 1.6864695105993197e-06,
      "loss": 0.1889,
      "step": 3285
    },
    {
      "epoch": 0.08601043502538956,
      "grad_norm": 41.201576232910156,
      "learning_rate": 1.6833289714734364e-06,
      "loss": 0.3351,
      "step": 3286
    },
    {
      "epoch": 0.08603660983823966,
      "grad_norm": 15.030851364135742,
      "learning_rate": 1.6801884323475529e-06,
      "loss": 0.1786,
      "step": 3287
    },
    {
      "epoch": 0.08606278465108974,
      "grad_norm": 22.997316360473633,
      "learning_rate": 1.6770478932216698e-06,
      "loss": 0.3367,
      "step": 3288
    },
    {
      "epoch": 0.08608895946393984,
      "grad_norm": 20.558549880981445,
      "learning_rate": 1.6739073540957865e-06,
      "loss": 0.2964,
      "step": 3289
    },
    {
      "epoch": 0.08611513427678992,
      "grad_norm": 15.602039337158203,
      "learning_rate": 1.6707668149699034e-06,
      "loss": 0.3029,
      "step": 3290
    },
    {
      "epoch": 0.08614130908964,
      "grad_norm": 19.39671516418457,
      "learning_rate": 1.6676262758440199e-06,
      "loss": 0.4317,
      "step": 3291
    },
    {
      "epoch": 0.0861674839024901,
      "grad_norm": 14.40040397644043,
      "learning_rate": 1.6644857367181366e-06,
      "loss": 0.3788,
      "step": 3292
    },
    {
      "epoch": 0.08619365871534018,
      "grad_norm": 15.315714836120605,
      "learning_rate": 1.6613451975922535e-06,
      "loss": 0.3228,
      "step": 3293
    },
    {
      "epoch": 0.08621983352819028,
      "grad_norm": 19.64505386352539,
      "learning_rate": 1.6582046584663702e-06,
      "loss": 0.2258,
      "step": 3294
    },
    {
      "epoch": 0.08624600834104036,
      "grad_norm": 10.947460174560547,
      "learning_rate": 1.6550641193404867e-06,
      "loss": 0.2605,
      "step": 3295
    },
    {
      "epoch": 0.08627218315389044,
      "grad_norm": 23.856220245361328,
      "learning_rate": 1.6519235802146036e-06,
      "loss": 0.3271,
      "step": 3296
    },
    {
      "epoch": 0.08629835796674054,
      "grad_norm": 21.770793914794922,
      "learning_rate": 1.6487830410887203e-06,
      "loss": 0.2772,
      "step": 3297
    },
    {
      "epoch": 0.08632453277959062,
      "grad_norm": 25.527286529541016,
      "learning_rate": 1.6456425019628368e-06,
      "loss": 0.5419,
      "step": 3298
    },
    {
      "epoch": 0.08635070759244072,
      "grad_norm": 8.416481971740723,
      "learning_rate": 1.6425019628369535e-06,
      "loss": 0.1415,
      "step": 3299
    },
    {
      "epoch": 0.0863768824052908,
      "grad_norm": 17.402711868286133,
      "learning_rate": 1.6393614237110705e-06,
      "loss": 0.3054,
      "step": 3300
    },
    {
      "epoch": 0.08640305721814089,
      "grad_norm": 26.720701217651367,
      "learning_rate": 1.6362208845851872e-06,
      "loss": 0.3238,
      "step": 3301
    },
    {
      "epoch": 0.08642923203099098,
      "grad_norm": 18.58193016052246,
      "learning_rate": 1.6330803454593037e-06,
      "loss": 0.3344,
      "step": 3302
    },
    {
      "epoch": 0.08645540684384107,
      "grad_norm": 17.146739959716797,
      "learning_rate": 1.6299398063334206e-06,
      "loss": 0.2785,
      "step": 3303
    },
    {
      "epoch": 0.08648158165669116,
      "grad_norm": 18.266803741455078,
      "learning_rate": 1.6267992672075373e-06,
      "loss": 0.2993,
      "step": 3304
    },
    {
      "epoch": 0.08650775646954124,
      "grad_norm": 23.001789093017578,
      "learning_rate": 1.6236587280816542e-06,
      "loss": 0.2416,
      "step": 3305
    },
    {
      "epoch": 0.08653393128239133,
      "grad_norm": 24.201915740966797,
      "learning_rate": 1.6205181889557707e-06,
      "loss": 0.4231,
      "step": 3306
    },
    {
      "epoch": 0.08656010609524142,
      "grad_norm": 21.134292602539062,
      "learning_rate": 1.6173776498298874e-06,
      "loss": 0.4294,
      "step": 3307
    },
    {
      "epoch": 0.0865862809080915,
      "grad_norm": 10.974215507507324,
      "learning_rate": 1.6142371107040043e-06,
      "loss": 0.1619,
      "step": 3308
    },
    {
      "epoch": 0.08661245572094159,
      "grad_norm": 29.319753646850586,
      "learning_rate": 1.611096571578121e-06,
      "loss": 0.4415,
      "step": 3309
    },
    {
      "epoch": 0.08663863053379169,
      "grad_norm": 11.882328033447266,
      "learning_rate": 1.6079560324522375e-06,
      "loss": 0.1676,
      "step": 3310
    },
    {
      "epoch": 0.08666480534664177,
      "grad_norm": 24.485010147094727,
      "learning_rate": 1.6048154933263544e-06,
      "loss": 0.2226,
      "step": 3311
    },
    {
      "epoch": 0.08669098015949186,
      "grad_norm": 13.449986457824707,
      "learning_rate": 1.6016749542004711e-06,
      "loss": 0.1855,
      "step": 3312
    },
    {
      "epoch": 0.08671715497234195,
      "grad_norm": 19.98228645324707,
      "learning_rate": 1.598534415074588e-06,
      "loss": 0.2077,
      "step": 3313
    },
    {
      "epoch": 0.08674332978519203,
      "grad_norm": 25.3158016204834,
      "learning_rate": 1.5953938759487043e-06,
      "loss": 0.3651,
      "step": 3314
    },
    {
      "epoch": 0.08676950459804213,
      "grad_norm": 23.98441505432129,
      "learning_rate": 1.5922533368228213e-06,
      "loss": 0.3796,
      "step": 3315
    },
    {
      "epoch": 0.08679567941089221,
      "grad_norm": 16.239498138427734,
      "learning_rate": 1.589112797696938e-06,
      "loss": 0.3368,
      "step": 3316
    },
    {
      "epoch": 0.0868218542237423,
      "grad_norm": 25.69401741027832,
      "learning_rate": 1.5859722585710549e-06,
      "loss": 0.4471,
      "step": 3317
    },
    {
      "epoch": 0.08684802903659239,
      "grad_norm": 18.528728485107422,
      "learning_rate": 1.5828317194451714e-06,
      "loss": 0.2626,
      "step": 3318
    },
    {
      "epoch": 0.08687420384944247,
      "grad_norm": 22.83640480041504,
      "learning_rate": 1.579691180319288e-06,
      "loss": 0.4549,
      "step": 3319
    },
    {
      "epoch": 0.08690037866229257,
      "grad_norm": 7.272606372833252,
      "learning_rate": 1.576550641193405e-06,
      "loss": 0.0518,
      "step": 3320
    },
    {
      "epoch": 0.08692655347514265,
      "grad_norm": 20.310298919677734,
      "learning_rate": 1.5734101020675217e-06,
      "loss": 0.4603,
      "step": 3321
    },
    {
      "epoch": 0.08695272828799275,
      "grad_norm": 18.58584976196289,
      "learning_rate": 1.5702695629416382e-06,
      "loss": 0.3031,
      "step": 3322
    },
    {
      "epoch": 0.08697890310084283,
      "grad_norm": 16.39595603942871,
      "learning_rate": 1.5671290238157551e-06,
      "loss": 0.317,
      "step": 3323
    },
    {
      "epoch": 0.08700507791369291,
      "grad_norm": 22.171180725097656,
      "learning_rate": 1.5639884846898718e-06,
      "loss": 0.4547,
      "step": 3324
    },
    {
      "epoch": 0.08703125272654301,
      "grad_norm": 18.639385223388672,
      "learning_rate": 1.5608479455639887e-06,
      "loss": 0.3666,
      "step": 3325
    },
    {
      "epoch": 0.08705742753939309,
      "grad_norm": 22.301076889038086,
      "learning_rate": 1.5577074064381052e-06,
      "loss": 0.3043,
      "step": 3326
    },
    {
      "epoch": 0.08708360235224318,
      "grad_norm": 18.435060501098633,
      "learning_rate": 1.554566867312222e-06,
      "loss": 0.2493,
      "step": 3327
    },
    {
      "epoch": 0.08710977716509327,
      "grad_norm": 16.45514488220215,
      "learning_rate": 1.5514263281863389e-06,
      "loss": 0.344,
      "step": 3328
    },
    {
      "epoch": 0.08713595197794335,
      "grad_norm": 20.561298370361328,
      "learning_rate": 1.5482857890604554e-06,
      "loss": 0.2701,
      "step": 3329
    },
    {
      "epoch": 0.08716212679079345,
      "grad_norm": 20.6446590423584,
      "learning_rate": 1.545145249934572e-06,
      "loss": 0.3753,
      "step": 3330
    },
    {
      "epoch": 0.08718830160364353,
      "grad_norm": 9.84206771850586,
      "learning_rate": 1.5420047108086888e-06,
      "loss": 0.1288,
      "step": 3331
    },
    {
      "epoch": 0.08721447641649362,
      "grad_norm": 13.731142044067383,
      "learning_rate": 1.5388641716828057e-06,
      "loss": 0.2261,
      "step": 3332
    },
    {
      "epoch": 0.08724065122934371,
      "grad_norm": 42.07485580444336,
      "learning_rate": 1.5357236325569222e-06,
      "loss": 0.5295,
      "step": 3333
    },
    {
      "epoch": 0.0872668260421938,
      "grad_norm": 20.806209564208984,
      "learning_rate": 1.5325830934310389e-06,
      "loss": 0.3616,
      "step": 3334
    },
    {
      "epoch": 0.08729300085504389,
      "grad_norm": 25.84351921081543,
      "learning_rate": 1.5294425543051558e-06,
      "loss": 0.4938,
      "step": 3335
    },
    {
      "epoch": 0.08731917566789398,
      "grad_norm": 16.005390167236328,
      "learning_rate": 1.5263020151792725e-06,
      "loss": 0.2225,
      "step": 3336
    },
    {
      "epoch": 0.08734535048074406,
      "grad_norm": 18.385108947753906,
      "learning_rate": 1.523161476053389e-06,
      "loss": 0.2383,
      "step": 3337
    },
    {
      "epoch": 0.08737152529359415,
      "grad_norm": 16.0833797454834,
      "learning_rate": 1.520020936927506e-06,
      "loss": 0.2439,
      "step": 3338
    },
    {
      "epoch": 0.08739770010644424,
      "grad_norm": 29.597576141357422,
      "learning_rate": 1.5168803978016226e-06,
      "loss": 0.5984,
      "step": 3339
    },
    {
      "epoch": 0.08742387491929433,
      "grad_norm": 19.69969367980957,
      "learning_rate": 1.5137398586757396e-06,
      "loss": 0.3869,
      "step": 3340
    },
    {
      "epoch": 0.08745004973214442,
      "grad_norm": 10.716504096984863,
      "learning_rate": 1.510599319549856e-06,
      "loss": 0.1969,
      "step": 3341
    },
    {
      "epoch": 0.0874762245449945,
      "grad_norm": 14.816800117492676,
      "learning_rate": 1.5074587804239728e-06,
      "loss": 0.2484,
      "step": 3342
    },
    {
      "epoch": 0.0875023993578446,
      "grad_norm": 22.683744430541992,
      "learning_rate": 1.5043182412980897e-06,
      "loss": 0.1886,
      "step": 3343
    },
    {
      "epoch": 0.08752857417069468,
      "grad_norm": 33.54377746582031,
      "learning_rate": 1.5011777021722064e-06,
      "loss": 0.4536,
      "step": 3344
    },
    {
      "epoch": 0.08755474898354476,
      "grad_norm": 17.418399810791016,
      "learning_rate": 1.498037163046323e-06,
      "loss": 0.2177,
      "step": 3345
    },
    {
      "epoch": 0.08758092379639486,
      "grad_norm": 27.738492965698242,
      "learning_rate": 1.4948966239204398e-06,
      "loss": 0.4443,
      "step": 3346
    },
    {
      "epoch": 0.08760709860924494,
      "grad_norm": 10.063409805297852,
      "learning_rate": 1.4917560847945565e-06,
      "loss": 0.2852,
      "step": 3347
    },
    {
      "epoch": 0.08763327342209504,
      "grad_norm": 8.40518856048584,
      "learning_rate": 1.4886155456686732e-06,
      "loss": 0.1108,
      "step": 3348
    },
    {
      "epoch": 0.08765944823494512,
      "grad_norm": 13.714468955993652,
      "learning_rate": 1.48547500654279e-06,
      "loss": 0.1783,
      "step": 3349
    },
    {
      "epoch": 0.0876856230477952,
      "grad_norm": 20.490097045898438,
      "learning_rate": 1.4823344674169066e-06,
      "loss": 0.2726,
      "step": 3350
    },
    {
      "epoch": 0.0877117978606453,
      "grad_norm": 25.19527816772461,
      "learning_rate": 1.4791939282910233e-06,
      "loss": 0.7522,
      "step": 3351
    },
    {
      "epoch": 0.08773797267349538,
      "grad_norm": 18.536230087280273,
      "learning_rate": 1.47605338916514e-06,
      "loss": 0.3414,
      "step": 3352
    },
    {
      "epoch": 0.08776414748634548,
      "grad_norm": 28.963525772094727,
      "learning_rate": 1.472912850039257e-06,
      "loss": 0.5837,
      "step": 3353
    },
    {
      "epoch": 0.08779032229919556,
      "grad_norm": 22.367473602294922,
      "learning_rate": 1.4697723109133734e-06,
      "loss": 0.2901,
      "step": 3354
    },
    {
      "epoch": 0.08781649711204564,
      "grad_norm": 21.22644805908203,
      "learning_rate": 1.4666317717874904e-06,
      "loss": 0.3898,
      "step": 3355
    },
    {
      "epoch": 0.08784267192489574,
      "grad_norm": 16.701839447021484,
      "learning_rate": 1.4634912326616069e-06,
      "loss": 0.3099,
      "step": 3356
    },
    {
      "epoch": 0.08786884673774582,
      "grad_norm": 24.41815948486328,
      "learning_rate": 1.4603506935357236e-06,
      "loss": 0.4433,
      "step": 3357
    },
    {
      "epoch": 0.08789502155059592,
      "grad_norm": 35.6885986328125,
      "learning_rate": 1.4572101544098405e-06,
      "loss": 0.4931,
      "step": 3358
    },
    {
      "epoch": 0.087921196363446,
      "grad_norm": 19.259490966796875,
      "learning_rate": 1.454069615283957e-06,
      "loss": 0.3332,
      "step": 3359
    },
    {
      "epoch": 0.08794737117629609,
      "grad_norm": 15.404192924499512,
      "learning_rate": 1.4509290761580739e-06,
      "loss": 0.1774,
      "step": 3360
    },
    {
      "epoch": 0.08797354598914618,
      "grad_norm": 15.178409576416016,
      "learning_rate": 1.4477885370321906e-06,
      "loss": 0.2581,
      "step": 3361
    },
    {
      "epoch": 0.08799972080199626,
      "grad_norm": 23.223743438720703,
      "learning_rate": 1.4446479979063073e-06,
      "loss": 0.4386,
      "step": 3362
    },
    {
      "epoch": 0.08802589561484635,
      "grad_norm": 25.007247924804688,
      "learning_rate": 1.441507458780424e-06,
      "loss": 0.297,
      "step": 3363
    },
    {
      "epoch": 0.08805207042769644,
      "grad_norm": 20.281330108642578,
      "learning_rate": 1.4383669196545407e-06,
      "loss": 0.301,
      "step": 3364
    },
    {
      "epoch": 0.08807824524054653,
      "grad_norm": 16.650888442993164,
      "learning_rate": 1.4352263805286574e-06,
      "loss": 0.3988,
      "step": 3365
    },
    {
      "epoch": 0.08810442005339662,
      "grad_norm": 27.98041343688965,
      "learning_rate": 1.4320858414027741e-06,
      "loss": 0.2249,
      "step": 3366
    },
    {
      "epoch": 0.0881305948662467,
      "grad_norm": 35.47264862060547,
      "learning_rate": 1.4289453022768908e-06,
      "loss": 0.6599,
      "step": 3367
    },
    {
      "epoch": 0.08815676967909679,
      "grad_norm": 18.075016021728516,
      "learning_rate": 1.4258047631510078e-06,
      "loss": 0.1918,
      "step": 3368
    },
    {
      "epoch": 0.08818294449194689,
      "grad_norm": 21.279878616333008,
      "learning_rate": 1.4226642240251242e-06,
      "loss": 0.3782,
      "step": 3369
    },
    {
      "epoch": 0.08820911930479697,
      "grad_norm": 35.66950988769531,
      "learning_rate": 1.4195236848992412e-06,
      "loss": 0.6338,
      "step": 3370
    },
    {
      "epoch": 0.08823529411764706,
      "grad_norm": 23.72509765625,
      "learning_rate": 1.4163831457733577e-06,
      "loss": 0.4378,
      "step": 3371
    },
    {
      "epoch": 0.08826146893049715,
      "grad_norm": 9.759449005126953,
      "learning_rate": 1.4132426066474746e-06,
      "loss": 0.1343,
      "step": 3372
    },
    {
      "epoch": 0.08828764374334723,
      "grad_norm": 13.440210342407227,
      "learning_rate": 1.4101020675215913e-06,
      "loss": 0.2047,
      "step": 3373
    },
    {
      "epoch": 0.08831381855619733,
      "grad_norm": 19.027002334594727,
      "learning_rate": 1.406961528395708e-06,
      "loss": 0.285,
      "step": 3374
    },
    {
      "epoch": 0.08833999336904741,
      "grad_norm": 18.614274978637695,
      "learning_rate": 1.4038209892698247e-06,
      "loss": 0.4319,
      "step": 3375
    },
    {
      "epoch": 0.0883661681818975,
      "grad_norm": 19.701623916625977,
      "learning_rate": 1.4006804501439414e-06,
      "loss": 0.3977,
      "step": 3376
    },
    {
      "epoch": 0.08839234299474759,
      "grad_norm": 12.903913497924805,
      "learning_rate": 1.3975399110180581e-06,
      "loss": 0.2237,
      "step": 3377
    },
    {
      "epoch": 0.08841851780759767,
      "grad_norm": 17.72046661376953,
      "learning_rate": 1.394399371892175e-06,
      "loss": 0.2369,
      "step": 3378
    },
    {
      "epoch": 0.08844469262044777,
      "grad_norm": 29.25196075439453,
      "learning_rate": 1.3912588327662915e-06,
      "loss": 0.3851,
      "step": 3379
    },
    {
      "epoch": 0.08847086743329785,
      "grad_norm": 13.282973289489746,
      "learning_rate": 1.3881182936404084e-06,
      "loss": 0.2313,
      "step": 3380
    },
    {
      "epoch": 0.08849704224614793,
      "grad_norm": 14.358569145202637,
      "learning_rate": 1.384977754514525e-06,
      "loss": 0.2698,
      "step": 3381
    },
    {
      "epoch": 0.08852321705899803,
      "grad_norm": 11.4870023727417,
      "learning_rate": 1.3818372153886419e-06,
      "loss": 0.2468,
      "step": 3382
    },
    {
      "epoch": 0.08854939187184811,
      "grad_norm": 29.350317001342773,
      "learning_rate": 1.3786966762627586e-06,
      "loss": 0.385,
      "step": 3383
    },
    {
      "epoch": 0.08857556668469821,
      "grad_norm": 34.00886154174805,
      "learning_rate": 1.3755561371368753e-06,
      "loss": 0.3024,
      "step": 3384
    },
    {
      "epoch": 0.08860174149754829,
      "grad_norm": 15.171599388122559,
      "learning_rate": 1.372415598010992e-06,
      "loss": 0.182,
      "step": 3385
    },
    {
      "epoch": 0.08862791631039837,
      "grad_norm": 19.551471710205078,
      "learning_rate": 1.3692750588851085e-06,
      "loss": 0.3111,
      "step": 3386
    },
    {
      "epoch": 0.08865409112324847,
      "grad_norm": 13.201918601989746,
      "learning_rate": 1.3661345197592254e-06,
      "loss": 0.2643,
      "step": 3387
    },
    {
      "epoch": 0.08868026593609855,
      "grad_norm": 24.99034309387207,
      "learning_rate": 1.362993980633342e-06,
      "loss": 0.3877,
      "step": 3388
    },
    {
      "epoch": 0.08870644074894865,
      "grad_norm": 22.500146865844727,
      "learning_rate": 1.3598534415074588e-06,
      "loss": 0.4175,
      "step": 3389
    },
    {
      "epoch": 0.08873261556179873,
      "grad_norm": 10.606009483337402,
      "learning_rate": 1.3567129023815755e-06,
      "loss": 0.2363,
      "step": 3390
    },
    {
      "epoch": 0.08875879037464882,
      "grad_norm": 27.882230758666992,
      "learning_rate": 1.3535723632556922e-06,
      "loss": 0.4531,
      "step": 3391
    },
    {
      "epoch": 0.08878496518749891,
      "grad_norm": 25.30727195739746,
      "learning_rate": 1.350431824129809e-06,
      "loss": 0.4205,
      "step": 3392
    },
    {
      "epoch": 0.088811140000349,
      "grad_norm": 21.007261276245117,
      "learning_rate": 1.3472912850039258e-06,
      "loss": 0.3966,
      "step": 3393
    },
    {
      "epoch": 0.08883731481319909,
      "grad_norm": 14.60384750366211,
      "learning_rate": 1.3441507458780423e-06,
      "loss": 0.2643,
      "step": 3394
    },
    {
      "epoch": 0.08886348962604917,
      "grad_norm": 7.968937873840332,
      "learning_rate": 1.3410102067521592e-06,
      "loss": 0.1302,
      "step": 3395
    },
    {
      "epoch": 0.08888966443889926,
      "grad_norm": 20.595699310302734,
      "learning_rate": 1.3378696676262757e-06,
      "loss": 0.2889,
      "step": 3396
    },
    {
      "epoch": 0.08891583925174935,
      "grad_norm": 22.171613693237305,
      "learning_rate": 1.3347291285003927e-06,
      "loss": 0.2926,
      "step": 3397
    },
    {
      "epoch": 0.08894201406459944,
      "grad_norm": 14.763070106506348,
      "learning_rate": 1.3315885893745094e-06,
      "loss": 0.2062,
      "step": 3398
    },
    {
      "epoch": 0.08896818887744952,
      "grad_norm": 24.8181209564209,
      "learning_rate": 1.328448050248626e-06,
      "loss": 0.4289,
      "step": 3399
    },
    {
      "epoch": 0.08899436369029962,
      "grad_norm": 25.03424644470215,
      "learning_rate": 1.3253075111227428e-06,
      "loss": 0.521,
      "step": 3400
    },
    {
      "epoch": 0.0890205385031497,
      "grad_norm": 13.498396873474121,
      "learning_rate": 1.3221669719968595e-06,
      "loss": 0.3055,
      "step": 3401
    },
    {
      "epoch": 0.0890467133159998,
      "grad_norm": 13.419083595275879,
      "learning_rate": 1.3190264328709762e-06,
      "loss": 0.1633,
      "step": 3402
    },
    {
      "epoch": 0.08907288812884988,
      "grad_norm": 13.59315299987793,
      "learning_rate": 1.3158858937450929e-06,
      "loss": 0.3492,
      "step": 3403
    },
    {
      "epoch": 0.08909906294169996,
      "grad_norm": 15.547048568725586,
      "learning_rate": 1.3127453546192096e-06,
      "loss": 0.204,
      "step": 3404
    },
    {
      "epoch": 0.08912523775455006,
      "grad_norm": 27.007925033569336,
      "learning_rate": 1.3096048154933265e-06,
      "loss": 0.4967,
      "step": 3405
    },
    {
      "epoch": 0.08915141256740014,
      "grad_norm": 17.608057022094727,
      "learning_rate": 1.306464276367443e-06,
      "loss": 0.2937,
      "step": 3406
    },
    {
      "epoch": 0.08917758738025024,
      "grad_norm": 18.588531494140625,
      "learning_rate": 1.30332373724156e-06,
      "loss": 0.437,
      "step": 3407
    },
    {
      "epoch": 0.08920376219310032,
      "grad_norm": 23.205793380737305,
      "learning_rate": 1.3001831981156766e-06,
      "loss": 0.328,
      "step": 3408
    },
    {
      "epoch": 0.0892299370059504,
      "grad_norm": 24.125486373901367,
      "learning_rate": 1.2970426589897933e-06,
      "loss": 0.358,
      "step": 3409
    },
    {
      "epoch": 0.0892561118188005,
      "grad_norm": 18.43383026123047,
      "learning_rate": 1.29390211986391e-06,
      "loss": 0.3502,
      "step": 3410
    },
    {
      "epoch": 0.08928228663165058,
      "grad_norm": 23.158832550048828,
      "learning_rate": 1.2907615807380268e-06,
      "loss": 0.4069,
      "step": 3411
    },
    {
      "epoch": 0.08930846144450068,
      "grad_norm": 23.651103973388672,
      "learning_rate": 1.2876210416121435e-06,
      "loss": 0.2665,
      "step": 3412
    },
    {
      "epoch": 0.08933463625735076,
      "grad_norm": 25.110557556152344,
      "learning_rate": 1.2844805024862602e-06,
      "loss": 0.6087,
      "step": 3413
    },
    {
      "epoch": 0.08936081107020084,
      "grad_norm": 21.760168075561523,
      "learning_rate": 1.2813399633603769e-06,
      "loss": 0.483,
      "step": 3414
    },
    {
      "epoch": 0.08938698588305094,
      "grad_norm": 17.817304611206055,
      "learning_rate": 1.2781994242344936e-06,
      "loss": 0.2761,
      "step": 3415
    },
    {
      "epoch": 0.08941316069590102,
      "grad_norm": 12.405414581298828,
      "learning_rate": 1.2750588851086103e-06,
      "loss": 0.2979,
      "step": 3416
    },
    {
      "epoch": 0.0894393355087511,
      "grad_norm": 16.334932327270508,
      "learning_rate": 1.271918345982727e-06,
      "loss": 0.2802,
      "step": 3417
    },
    {
      "epoch": 0.0894655103216012,
      "grad_norm": 23.102005004882812,
      "learning_rate": 1.2687778068568437e-06,
      "loss": 0.3315,
      "step": 3418
    },
    {
      "epoch": 0.08949168513445128,
      "grad_norm": 45.173423767089844,
      "learning_rate": 1.2656372677309604e-06,
      "loss": 0.6031,
      "step": 3419
    },
    {
      "epoch": 0.08951785994730138,
      "grad_norm": 22.8571834564209,
      "learning_rate": 1.2624967286050773e-06,
      "loss": 0.3751,
      "step": 3420
    },
    {
      "epoch": 0.08954403476015146,
      "grad_norm": 16.561609268188477,
      "learning_rate": 1.2593561894791938e-06,
      "loss": 0.2083,
      "step": 3421
    },
    {
      "epoch": 0.08957020957300155,
      "grad_norm": 16.514062881469727,
      "learning_rate": 1.2562156503533107e-06,
      "loss": 0.2604,
      "step": 3422
    },
    {
      "epoch": 0.08959638438585164,
      "grad_norm": 18.500059127807617,
      "learning_rate": 1.2530751112274274e-06,
      "loss": 0.2297,
      "step": 3423
    },
    {
      "epoch": 0.08962255919870173,
      "grad_norm": 23.328109741210938,
      "learning_rate": 1.2499345721015441e-06,
      "loss": 0.4525,
      "step": 3424
    },
    {
      "epoch": 0.08964873401155182,
      "grad_norm": 18.48256492614746,
      "learning_rate": 1.2467940329756609e-06,
      "loss": 0.5096,
      "step": 3425
    },
    {
      "epoch": 0.0896749088244019,
      "grad_norm": 13.455221176147461,
      "learning_rate": 1.2436534938497776e-06,
      "loss": 0.1762,
      "step": 3426
    },
    {
      "epoch": 0.08970108363725199,
      "grad_norm": 19.812496185302734,
      "learning_rate": 1.2405129547238943e-06,
      "loss": 0.2432,
      "step": 3427
    },
    {
      "epoch": 0.08972725845010208,
      "grad_norm": 16.432788848876953,
      "learning_rate": 1.237372415598011e-06,
      "loss": 0.316,
      "step": 3428
    },
    {
      "epoch": 0.08975343326295217,
      "grad_norm": 17.22280502319336,
      "learning_rate": 1.2342318764721277e-06,
      "loss": 0.3421,
      "step": 3429
    },
    {
      "epoch": 0.08977960807580226,
      "grad_norm": 14.684537887573242,
      "learning_rate": 1.2310913373462446e-06,
      "loss": 0.3045,
      "step": 3430
    },
    {
      "epoch": 0.08980578288865235,
      "grad_norm": 11.31345272064209,
      "learning_rate": 1.227950798220361e-06,
      "loss": 0.1412,
      "step": 3431
    },
    {
      "epoch": 0.08983195770150243,
      "grad_norm": 12.890829086303711,
      "learning_rate": 1.224810259094478e-06,
      "loss": 0.3814,
      "step": 3432
    },
    {
      "epoch": 0.08985813251435253,
      "grad_norm": 12.392135620117188,
      "learning_rate": 1.2216697199685947e-06,
      "loss": 0.1318,
      "step": 3433
    },
    {
      "epoch": 0.08988430732720261,
      "grad_norm": 19.839445114135742,
      "learning_rate": 1.2185291808427114e-06,
      "loss": 0.3572,
      "step": 3434
    },
    {
      "epoch": 0.0899104821400527,
      "grad_norm": 21.4797420501709,
      "learning_rate": 1.2153886417168281e-06,
      "loss": 0.352,
      "step": 3435
    },
    {
      "epoch": 0.08993665695290279,
      "grad_norm": 16.541383743286133,
      "learning_rate": 1.2122481025909448e-06,
      "loss": 0.3932,
      "step": 3436
    },
    {
      "epoch": 0.08996283176575287,
      "grad_norm": 20.916152954101562,
      "learning_rate": 1.2091075634650615e-06,
      "loss": 0.2923,
      "step": 3437
    },
    {
      "epoch": 0.08998900657860297,
      "grad_norm": 22.27550506591797,
      "learning_rate": 1.2059670243391782e-06,
      "loss": 0.3683,
      "step": 3438
    },
    {
      "epoch": 0.09001518139145305,
      "grad_norm": 20.216352462768555,
      "learning_rate": 1.202826485213295e-06,
      "loss": 0.3685,
      "step": 3439
    },
    {
      "epoch": 0.09004135620430313,
      "grad_norm": 30.209482192993164,
      "learning_rate": 1.1996859460874119e-06,
      "loss": 0.3312,
      "step": 3440
    },
    {
      "epoch": 0.09006753101715323,
      "grad_norm": 18.180784225463867,
      "learning_rate": 1.1965454069615284e-06,
      "loss": 0.2905,
      "step": 3441
    },
    {
      "epoch": 0.09009370583000331,
      "grad_norm": 16.118350982666016,
      "learning_rate": 1.1934048678356453e-06,
      "loss": 0.2843,
      "step": 3442
    },
    {
      "epoch": 0.09011988064285341,
      "grad_norm": 16.227094650268555,
      "learning_rate": 1.1902643287097618e-06,
      "loss": 0.2721,
      "step": 3443
    },
    {
      "epoch": 0.09014605545570349,
      "grad_norm": 23.889080047607422,
      "learning_rate": 1.1871237895838785e-06,
      "loss": 0.4155,
      "step": 3444
    },
    {
      "epoch": 0.09017223026855357,
      "grad_norm": 27.553937911987305,
      "learning_rate": 1.1839832504579954e-06,
      "loss": 0.4696,
      "step": 3445
    },
    {
      "epoch": 0.09019840508140367,
      "grad_norm": 21.70665740966797,
      "learning_rate": 1.180842711332112e-06,
      "loss": 0.4254,
      "step": 3446
    },
    {
      "epoch": 0.09022457989425375,
      "grad_norm": 14.093149185180664,
      "learning_rate": 1.1777021722062288e-06,
      "loss": 0.2241,
      "step": 3447
    },
    {
      "epoch": 0.09025075470710385,
      "grad_norm": 17.251779556274414,
      "learning_rate": 1.1745616330803455e-06,
      "loss": 0.397,
      "step": 3448
    },
    {
      "epoch": 0.09027692951995393,
      "grad_norm": 17.6119441986084,
      "learning_rate": 1.1714210939544622e-06,
      "loss": 0.3104,
      "step": 3449
    },
    {
      "epoch": 0.09030310433280402,
      "grad_norm": 14.416563034057617,
      "learning_rate": 1.168280554828579e-06,
      "loss": 0.2323,
      "step": 3450
    },
    {
      "epoch": 0.09032927914565411,
      "grad_norm": 9.886146545410156,
      "learning_rate": 1.1651400157026956e-06,
      "loss": 0.0997,
      "step": 3451
    },
    {
      "epoch": 0.0903554539585042,
      "grad_norm": 17.244375228881836,
      "learning_rate": 1.1619994765768123e-06,
      "loss": 0.3046,
      "step": 3452
    },
    {
      "epoch": 0.09038162877135429,
      "grad_norm": 17.323152542114258,
      "learning_rate": 1.158858937450929e-06,
      "loss": 0.2337,
      "step": 3453
    },
    {
      "epoch": 0.09040780358420437,
      "grad_norm": 16.459426879882812,
      "learning_rate": 1.1557183983250458e-06,
      "loss": 0.2363,
      "step": 3454
    },
    {
      "epoch": 0.09043397839705446,
      "grad_norm": 26.27742576599121,
      "learning_rate": 1.1525778591991627e-06,
      "loss": 0.4313,
      "step": 3455
    },
    {
      "epoch": 0.09046015320990455,
      "grad_norm": 8.083678245544434,
      "learning_rate": 1.1494373200732792e-06,
      "loss": 0.0834,
      "step": 3456
    },
    {
      "epoch": 0.09048632802275464,
      "grad_norm": 27.105621337890625,
      "learning_rate": 1.146296780947396e-06,
      "loss": 0.3074,
      "step": 3457
    },
    {
      "epoch": 0.09051250283560472,
      "grad_norm": 15.092912673950195,
      "learning_rate": 1.1431562418215126e-06,
      "loss": 0.2752,
      "step": 3458
    },
    {
      "epoch": 0.09053867764845482,
      "grad_norm": 17.797147750854492,
      "learning_rate": 1.1400157026956295e-06,
      "loss": 0.2262,
      "step": 3459
    },
    {
      "epoch": 0.0905648524613049,
      "grad_norm": 16.354795455932617,
      "learning_rate": 1.1368751635697462e-06,
      "loss": 0.3572,
      "step": 3460
    },
    {
      "epoch": 0.090591027274155,
      "grad_norm": 18.97016143798828,
      "learning_rate": 1.133734624443863e-06,
      "loss": 0.3748,
      "step": 3461
    },
    {
      "epoch": 0.09061720208700508,
      "grad_norm": 33.616390228271484,
      "learning_rate": 1.1305940853179796e-06,
      "loss": 0.3804,
      "step": 3462
    },
    {
      "epoch": 0.09064337689985516,
      "grad_norm": 13.007120132446289,
      "learning_rate": 1.1274535461920963e-06,
      "loss": 0.1529,
      "step": 3463
    },
    {
      "epoch": 0.09066955171270526,
      "grad_norm": 15.75558090209961,
      "learning_rate": 1.124313007066213e-06,
      "loss": 0.185,
      "step": 3464
    },
    {
      "epoch": 0.09069572652555534,
      "grad_norm": 22.00530242919922,
      "learning_rate": 1.12117246794033e-06,
      "loss": 0.298,
      "step": 3465
    },
    {
      "epoch": 0.09072190133840544,
      "grad_norm": 13.260967254638672,
      "learning_rate": 1.1180319288144464e-06,
      "loss": 0.1995,
      "step": 3466
    },
    {
      "epoch": 0.09074807615125552,
      "grad_norm": 30.139074325561523,
      "learning_rate": 1.1148913896885634e-06,
      "loss": 0.2446,
      "step": 3467
    },
    {
      "epoch": 0.0907742509641056,
      "grad_norm": 18.164649963378906,
      "learning_rate": 1.1117508505626799e-06,
      "loss": 0.2159,
      "step": 3468
    },
    {
      "epoch": 0.0908004257769557,
      "grad_norm": 27.214235305786133,
      "learning_rate": 1.1086103114367968e-06,
      "loss": 0.3902,
      "step": 3469
    },
    {
      "epoch": 0.09082660058980578,
      "grad_norm": 29.993921279907227,
      "learning_rate": 1.1054697723109135e-06,
      "loss": 0.3828,
      "step": 3470
    },
    {
      "epoch": 0.09085277540265588,
      "grad_norm": 17.250812530517578,
      "learning_rate": 1.1023292331850302e-06,
      "loss": 0.3167,
      "step": 3471
    },
    {
      "epoch": 0.09087895021550596,
      "grad_norm": 14.261625289916992,
      "learning_rate": 1.0991886940591469e-06,
      "loss": 0.2996,
      "step": 3472
    },
    {
      "epoch": 0.09090512502835604,
      "grad_norm": 23.128456115722656,
      "learning_rate": 1.0960481549332634e-06,
      "loss": 0.4556,
      "step": 3473
    },
    {
      "epoch": 0.09093129984120614,
      "grad_norm": 22.326719284057617,
      "learning_rate": 1.0929076158073803e-06,
      "loss": 0.6129,
      "step": 3474
    },
    {
      "epoch": 0.09095747465405622,
      "grad_norm": 14.64742374420166,
      "learning_rate": 1.089767076681497e-06,
      "loss": 0.2076,
      "step": 3475
    },
    {
      "epoch": 0.0909836494669063,
      "grad_norm": 23.116668701171875,
      "learning_rate": 1.0866265375556137e-06,
      "loss": 0.4503,
      "step": 3476
    },
    {
      "epoch": 0.0910098242797564,
      "grad_norm": 18.56131362915039,
      "learning_rate": 1.0834859984297304e-06,
      "loss": 0.3322,
      "step": 3477
    },
    {
      "epoch": 0.09103599909260648,
      "grad_norm": 18.045188903808594,
      "learning_rate": 1.0803454593038471e-06,
      "loss": 0.199,
      "step": 3478
    },
    {
      "epoch": 0.09106217390545658,
      "grad_norm": 23.379304885864258,
      "learning_rate": 1.0772049201779638e-06,
      "loss": 0.316,
      "step": 3479
    },
    {
      "epoch": 0.09108834871830666,
      "grad_norm": 28.67458724975586,
      "learning_rate": 1.0740643810520808e-06,
      "loss": 0.3058,
      "step": 3480
    },
    {
      "epoch": 0.09111452353115675,
      "grad_norm": 17.847618103027344,
      "learning_rate": 1.0709238419261972e-06,
      "loss": 0.3283,
      "step": 3481
    },
    {
      "epoch": 0.09114069834400684,
      "grad_norm": 14.363595008850098,
      "learning_rate": 1.0677833028003142e-06,
      "loss": 0.1531,
      "step": 3482
    },
    {
      "epoch": 0.09116687315685693,
      "grad_norm": 13.340188980102539,
      "learning_rate": 1.0646427636744307e-06,
      "loss": 0.1991,
      "step": 3483
    },
    {
      "epoch": 0.09119304796970702,
      "grad_norm": 28.38844108581543,
      "learning_rate": 1.0615022245485476e-06,
      "loss": 0.299,
      "step": 3484
    },
    {
      "epoch": 0.0912192227825571,
      "grad_norm": 17.420835494995117,
      "learning_rate": 1.0583616854226643e-06,
      "loss": 0.2698,
      "step": 3485
    },
    {
      "epoch": 0.09124539759540719,
      "grad_norm": 20.833925247192383,
      "learning_rate": 1.055221146296781e-06,
      "loss": 0.2945,
      "step": 3486
    },
    {
      "epoch": 0.09127157240825728,
      "grad_norm": 22.698633193969727,
      "learning_rate": 1.0520806071708977e-06,
      "loss": 0.3744,
      "step": 3487
    },
    {
      "epoch": 0.09129774722110737,
      "grad_norm": 21.06345558166504,
      "learning_rate": 1.0489400680450144e-06,
      "loss": 0.3318,
      "step": 3488
    },
    {
      "epoch": 0.09132392203395746,
      "grad_norm": 32.630455017089844,
      "learning_rate": 1.0457995289191311e-06,
      "loss": 0.4868,
      "step": 3489
    },
    {
      "epoch": 0.09135009684680755,
      "grad_norm": 14.167927742004395,
      "learning_rate": 1.0426589897932478e-06,
      "loss": 0.2882,
      "step": 3490
    },
    {
      "epoch": 0.09137627165965763,
      "grad_norm": 17.792953491210938,
      "learning_rate": 1.0395184506673645e-06,
      "loss": 0.2288,
      "step": 3491
    },
    {
      "epoch": 0.09140244647250773,
      "grad_norm": 23.103164672851562,
      "learning_rate": 1.0363779115414814e-06,
      "loss": 0.2793,
      "step": 3492
    },
    {
      "epoch": 0.09142862128535781,
      "grad_norm": 19.183435440063477,
      "learning_rate": 1.033237372415598e-06,
      "loss": 0.2676,
      "step": 3493
    },
    {
      "epoch": 0.09145479609820789,
      "grad_norm": 19.59161376953125,
      "learning_rate": 1.0300968332897149e-06,
      "loss": 0.317,
      "step": 3494
    },
    {
      "epoch": 0.09148097091105799,
      "grad_norm": 13.939697265625,
      "learning_rate": 1.0269562941638316e-06,
      "loss": 0.3317,
      "step": 3495
    },
    {
      "epoch": 0.09150714572390807,
      "grad_norm": 13.928842544555664,
      "learning_rate": 1.0238157550379483e-06,
      "loss": 0.2019,
      "step": 3496
    },
    {
      "epoch": 0.09153332053675817,
      "grad_norm": 12.27055549621582,
      "learning_rate": 1.020675215912065e-06,
      "loss": 0.1766,
      "step": 3497
    },
    {
      "epoch": 0.09155949534960825,
      "grad_norm": 28.26992416381836,
      "learning_rate": 1.0175346767861817e-06,
      "loss": 0.245,
      "step": 3498
    },
    {
      "epoch": 0.09158567016245833,
      "grad_norm": 14.933760643005371,
      "learning_rate": 1.0143941376602984e-06,
      "loss": 0.2928,
      "step": 3499
    },
    {
      "epoch": 0.09161184497530843,
      "grad_norm": 21.5974178314209,
      "learning_rate": 1.011253598534415e-06,
      "loss": 0.666,
      "step": 3500
    },
    {
      "epoch": 0.09163801978815851,
      "grad_norm": 13.731534957885742,
      "learning_rate": 1.0081130594085318e-06,
      "loss": 0.242,
      "step": 3501
    },
    {
      "epoch": 0.09166419460100861,
      "grad_norm": 25.028562545776367,
      "learning_rate": 1.0049725202826485e-06,
      "loss": 0.437,
      "step": 3502
    },
    {
      "epoch": 0.09169036941385869,
      "grad_norm": 16.908658981323242,
      "learning_rate": 1.0018319811567652e-06,
      "loss": 0.3893,
      "step": 3503
    },
    {
      "epoch": 0.09171654422670877,
      "grad_norm": 18.624786376953125,
      "learning_rate": 9.98691442030882e-07,
      "loss": 0.4136,
      "step": 3504
    },
    {
      "epoch": 0.09174271903955887,
      "grad_norm": 23.021087646484375,
      "learning_rate": 9.955509029049986e-07,
      "loss": 0.4208,
      "step": 3505
    },
    {
      "epoch": 0.09176889385240895,
      "grad_norm": 22.47648811340332,
      "learning_rate": 9.924103637791153e-07,
      "loss": 0.4188,
      "step": 3506
    },
    {
      "epoch": 0.09179506866525905,
      "grad_norm": 29.46966552734375,
      "learning_rate": 9.892698246532322e-07,
      "loss": 0.5614,
      "step": 3507
    },
    {
      "epoch": 0.09182124347810913,
      "grad_norm": 32.5091552734375,
      "learning_rate": 9.861292855273487e-07,
      "loss": 0.4089,
      "step": 3508
    },
    {
      "epoch": 0.09184741829095922,
      "grad_norm": 16.15316390991211,
      "learning_rate": 9.829887464014657e-07,
      "loss": 0.2735,
      "step": 3509
    },
    {
      "epoch": 0.09187359310380931,
      "grad_norm": 20.34417724609375,
      "learning_rate": 9.798482072755824e-07,
      "loss": 0.3127,
      "step": 3510
    },
    {
      "epoch": 0.0918997679166594,
      "grad_norm": 15.592655181884766,
      "learning_rate": 9.76707668149699e-07,
      "loss": 0.2353,
      "step": 3511
    },
    {
      "epoch": 0.09192594272950948,
      "grad_norm": 26.209880828857422,
      "learning_rate": 9.735671290238158e-07,
      "loss": 0.3276,
      "step": 3512
    },
    {
      "epoch": 0.09195211754235957,
      "grad_norm": 19.3769474029541,
      "learning_rate": 9.704265898979325e-07,
      "loss": 0.282,
      "step": 3513
    },
    {
      "epoch": 0.09197829235520966,
      "grad_norm": 32.417274475097656,
      "learning_rate": 9.672860507720492e-07,
      "loss": 0.625,
      "step": 3514
    },
    {
      "epoch": 0.09200446716805975,
      "grad_norm": 41.67056655883789,
      "learning_rate": 9.64145511646166e-07,
      "loss": 0.4237,
      "step": 3515
    },
    {
      "epoch": 0.09203064198090984,
      "grad_norm": 23.287790298461914,
      "learning_rate": 9.610049725202826e-07,
      "loss": 0.4236,
      "step": 3516
    },
    {
      "epoch": 0.09205681679375992,
      "grad_norm": 10.668556213378906,
      "learning_rate": 9.578644333943995e-07,
      "loss": 0.1712,
      "step": 3517
    },
    {
      "epoch": 0.09208299160661002,
      "grad_norm": 8.330228805541992,
      "learning_rate": 9.54723894268516e-07,
      "loss": 0.1032,
      "step": 3518
    },
    {
      "epoch": 0.0921091664194601,
      "grad_norm": 16.161453247070312,
      "learning_rate": 9.515833551426329e-07,
      "loss": 0.2423,
      "step": 3519
    },
    {
      "epoch": 0.0921353412323102,
      "grad_norm": 21.077341079711914,
      "learning_rate": 9.484428160167495e-07,
      "loss": 0.435,
      "step": 3520
    },
    {
      "epoch": 0.09216151604516028,
      "grad_norm": 15.158782005310059,
      "learning_rate": 9.453022768908663e-07,
      "loss": 0.2092,
      "step": 3521
    },
    {
      "epoch": 0.09218769085801036,
      "grad_norm": 18.246803283691406,
      "learning_rate": 9.421617377649829e-07,
      "loss": 0.32,
      "step": 3522
    },
    {
      "epoch": 0.09221386567086046,
      "grad_norm": 22.86768341064453,
      "learning_rate": 9.390211986390998e-07,
      "loss": 0.4281,
      "step": 3523
    },
    {
      "epoch": 0.09224004048371054,
      "grad_norm": 15.009023666381836,
      "learning_rate": 9.358806595132165e-07,
      "loss": 0.2725,
      "step": 3524
    },
    {
      "epoch": 0.09226621529656064,
      "grad_norm": 14.262762069702148,
      "learning_rate": 9.327401203873333e-07,
      "loss": 0.228,
      "step": 3525
    },
    {
      "epoch": 0.09229239010941072,
      "grad_norm": 21.29462242126465,
      "learning_rate": 9.295995812614499e-07,
      "loss": 0.2609,
      "step": 3526
    },
    {
      "epoch": 0.0923185649222608,
      "grad_norm": 13.176029205322266,
      "learning_rate": 9.264590421355667e-07,
      "loss": 0.2256,
      "step": 3527
    },
    {
      "epoch": 0.0923447397351109,
      "grad_norm": 14.317953109741211,
      "learning_rate": 9.233185030096834e-07,
      "loss": 0.2549,
      "step": 3528
    },
    {
      "epoch": 0.09237091454796098,
      "grad_norm": 27.56165885925293,
      "learning_rate": 9.201779638838002e-07,
      "loss": 0.5487,
      "step": 3529
    },
    {
      "epoch": 0.09239708936081106,
      "grad_norm": 22.221149444580078,
      "learning_rate": 9.170374247579168e-07,
      "loss": 0.356,
      "step": 3530
    },
    {
      "epoch": 0.09242326417366116,
      "grad_norm": 12.715895652770996,
      "learning_rate": 9.138968856320334e-07,
      "loss": 0.2869,
      "step": 3531
    },
    {
      "epoch": 0.09244943898651124,
      "grad_norm": 30.41288948059082,
      "learning_rate": 9.107563465061502e-07,
      "loss": 0.355,
      "step": 3532
    },
    {
      "epoch": 0.09247561379936134,
      "grad_norm": 41.089786529541016,
      "learning_rate": 9.076158073802669e-07,
      "loss": 0.5542,
      "step": 3533
    },
    {
      "epoch": 0.09250178861221142,
      "grad_norm": 27.4390811920166,
      "learning_rate": 9.044752682543837e-07,
      "loss": 0.2982,
      "step": 3534
    },
    {
      "epoch": 0.0925279634250615,
      "grad_norm": 26.919878005981445,
      "learning_rate": 9.013347291285003e-07,
      "loss": 0.3077,
      "step": 3535
    },
    {
      "epoch": 0.0925541382379116,
      "grad_norm": 21.914508819580078,
      "learning_rate": 8.981941900026172e-07,
      "loss": 0.3759,
      "step": 3536
    },
    {
      "epoch": 0.09258031305076168,
      "grad_norm": 19.475942611694336,
      "learning_rate": 8.950536508767338e-07,
      "loss": 0.4132,
      "step": 3537
    },
    {
      "epoch": 0.09260648786361178,
      "grad_norm": 22.090103149414062,
      "learning_rate": 8.919131117508506e-07,
      "loss": 0.2142,
      "step": 3538
    },
    {
      "epoch": 0.09263266267646186,
      "grad_norm": 35.017486572265625,
      "learning_rate": 8.887725726249673e-07,
      "loss": 0.5109,
      "step": 3539
    },
    {
      "epoch": 0.09265883748931195,
      "grad_norm": 14.935866355895996,
      "learning_rate": 8.856320334990841e-07,
      "loss": 0.2873,
      "step": 3540
    },
    {
      "epoch": 0.09268501230216204,
      "grad_norm": 25.61105728149414,
      "learning_rate": 8.824914943732007e-07,
      "loss": 0.4729,
      "step": 3541
    },
    {
      "epoch": 0.09271118711501213,
      "grad_norm": 20.20452308654785,
      "learning_rate": 8.793509552473175e-07,
      "loss": 0.4115,
      "step": 3542
    },
    {
      "epoch": 0.09273736192786222,
      "grad_norm": 16.88941192626953,
      "learning_rate": 8.762104161214342e-07,
      "loss": 0.2722,
      "step": 3543
    },
    {
      "epoch": 0.0927635367407123,
      "grad_norm": 12.448838233947754,
      "learning_rate": 8.73069876995551e-07,
      "loss": 0.1019,
      "step": 3544
    },
    {
      "epoch": 0.09278971155356239,
      "grad_norm": 19.072101593017578,
      "learning_rate": 8.699293378696676e-07,
      "loss": 0.2612,
      "step": 3545
    },
    {
      "epoch": 0.09281588636641248,
      "grad_norm": 30.127357482910156,
      "learning_rate": 8.667887987437844e-07,
      "loss": 0.2832,
      "step": 3546
    },
    {
      "epoch": 0.09284206117926257,
      "grad_norm": 16.929861068725586,
      "learning_rate": 8.63648259617901e-07,
      "loss": 0.1005,
      "step": 3547
    },
    {
      "epoch": 0.09286823599211266,
      "grad_norm": 29.047285079956055,
      "learning_rate": 8.605077204920178e-07,
      "loss": 0.2502,
      "step": 3548
    },
    {
      "epoch": 0.09289441080496275,
      "grad_norm": 15.238314628601074,
      "learning_rate": 8.573671813661345e-07,
      "loss": 0.2375,
      "step": 3549
    },
    {
      "epoch": 0.09292058561781283,
      "grad_norm": 22.80430030822754,
      "learning_rate": 8.542266422402514e-07,
      "loss": 0.3258,
      "step": 3550
    },
    {
      "epoch": 0.09294676043066293,
      "grad_norm": 23.41741943359375,
      "learning_rate": 8.51086103114368e-07,
      "loss": 0.5899,
      "step": 3551
    },
    {
      "epoch": 0.09297293524351301,
      "grad_norm": 19.982196807861328,
      "learning_rate": 8.479455639884848e-07,
      "loss": 0.281,
      "step": 3552
    },
    {
      "epoch": 0.09299911005636309,
      "grad_norm": 28.114299774169922,
      "learning_rate": 8.448050248626014e-07,
      "loss": 0.3362,
      "step": 3553
    },
    {
      "epoch": 0.09302528486921319,
      "grad_norm": 12.449545860290527,
      "learning_rate": 8.416644857367182e-07,
      "loss": 0.1831,
      "step": 3554
    },
    {
      "epoch": 0.09305145968206327,
      "grad_norm": 28.764591217041016,
      "learning_rate": 8.385239466108349e-07,
      "loss": 0.7238,
      "step": 3555
    },
    {
      "epoch": 0.09307763449491337,
      "grad_norm": 23.715059280395508,
      "learning_rate": 8.353834074849517e-07,
      "loss": 0.4612,
      "step": 3556
    },
    {
      "epoch": 0.09310380930776345,
      "grad_norm": 19.139047622680664,
      "learning_rate": 8.322428683590683e-07,
      "loss": 0.3232,
      "step": 3557
    },
    {
      "epoch": 0.09312998412061353,
      "grad_norm": 21.5065975189209,
      "learning_rate": 8.291023292331851e-07,
      "loss": 0.3627,
      "step": 3558
    },
    {
      "epoch": 0.09315615893346363,
      "grad_norm": 30.126068115234375,
      "learning_rate": 8.259617901073018e-07,
      "loss": 0.4212,
      "step": 3559
    },
    {
      "epoch": 0.09318233374631371,
      "grad_norm": 27.78098487854004,
      "learning_rate": 8.228212509814184e-07,
      "loss": 0.4077,
      "step": 3560
    },
    {
      "epoch": 0.09320850855916381,
      "grad_norm": 25.498382568359375,
      "learning_rate": 8.196807118555352e-07,
      "loss": 0.3316,
      "step": 3561
    },
    {
      "epoch": 0.09323468337201389,
      "grad_norm": 20.858966827392578,
      "learning_rate": 8.165401727296518e-07,
      "loss": 0.3876,
      "step": 3562
    },
    {
      "epoch": 0.09326085818486397,
      "grad_norm": 27.159523010253906,
      "learning_rate": 8.133996336037686e-07,
      "loss": 0.4725,
      "step": 3563
    },
    {
      "epoch": 0.09328703299771407,
      "grad_norm": 22.432424545288086,
      "learning_rate": 8.102590944778853e-07,
      "loss": 0.3844,
      "step": 3564
    },
    {
      "epoch": 0.09331320781056415,
      "grad_norm": 19.84834098815918,
      "learning_rate": 8.071185553520022e-07,
      "loss": 0.3848,
      "step": 3565
    },
    {
      "epoch": 0.09333938262341425,
      "grad_norm": 23.30337905883789,
      "learning_rate": 8.039780162261188e-07,
      "loss": 0.302,
      "step": 3566
    },
    {
      "epoch": 0.09336555743626433,
      "grad_norm": 14.80574893951416,
      "learning_rate": 8.008374771002356e-07,
      "loss": 0.1978,
      "step": 3567
    },
    {
      "epoch": 0.09339173224911441,
      "grad_norm": 46.871498107910156,
      "learning_rate": 7.976969379743522e-07,
      "loss": 0.4831,
      "step": 3568
    },
    {
      "epoch": 0.09341790706196451,
      "grad_norm": 18.044021606445312,
      "learning_rate": 7.94556398848469e-07,
      "loss": 0.4097,
      "step": 3569
    },
    {
      "epoch": 0.0934440818748146,
      "grad_norm": 16.52120590209961,
      "learning_rate": 7.914158597225857e-07,
      "loss": 0.2707,
      "step": 3570
    },
    {
      "epoch": 0.09347025668766468,
      "grad_norm": 26.97692108154297,
      "learning_rate": 7.882753205967025e-07,
      "loss": 0.3219,
      "step": 3571
    },
    {
      "epoch": 0.09349643150051477,
      "grad_norm": 11.78614616394043,
      "learning_rate": 7.851347814708191e-07,
      "loss": 0.1505,
      "step": 3572
    },
    {
      "epoch": 0.09352260631336486,
      "grad_norm": 25.73763656616211,
      "learning_rate": 7.819942423449359e-07,
      "loss": 0.411,
      "step": 3573
    },
    {
      "epoch": 0.09354878112621495,
      "grad_norm": 23.28497886657715,
      "learning_rate": 7.788537032190526e-07,
      "loss": 0.3995,
      "step": 3574
    },
    {
      "epoch": 0.09357495593906504,
      "grad_norm": 21.797409057617188,
      "learning_rate": 7.757131640931694e-07,
      "loss": 0.2914,
      "step": 3575
    },
    {
      "epoch": 0.09360113075191512,
      "grad_norm": 24.958213806152344,
      "learning_rate": 7.72572624967286e-07,
      "loss": 0.2647,
      "step": 3576
    },
    {
      "epoch": 0.09362730556476521,
      "grad_norm": 22.4291934967041,
      "learning_rate": 7.694320858414028e-07,
      "loss": 0.4087,
      "step": 3577
    },
    {
      "epoch": 0.0936534803776153,
      "grad_norm": 21.875722885131836,
      "learning_rate": 7.662915467155194e-07,
      "loss": 0.3846,
      "step": 3578
    },
    {
      "epoch": 0.0936796551904654,
      "grad_norm": 23.309555053710938,
      "learning_rate": 7.631510075896363e-07,
      "loss": 0.6433,
      "step": 3579
    },
    {
      "epoch": 0.09370583000331548,
      "grad_norm": 14.255001068115234,
      "learning_rate": 7.60010468463753e-07,
      "loss": 0.275,
      "step": 3580
    },
    {
      "epoch": 0.09373200481616556,
      "grad_norm": 18.26328468322754,
      "learning_rate": 7.568699293378698e-07,
      "loss": 0.297,
      "step": 3581
    },
    {
      "epoch": 0.09375817962901566,
      "grad_norm": 17.119491577148438,
      "learning_rate": 7.537293902119864e-07,
      "loss": 0.3136,
      "step": 3582
    },
    {
      "epoch": 0.09378435444186574,
      "grad_norm": 23.71054458618164,
      "learning_rate": 7.505888510861032e-07,
      "loss": 0.3924,
      "step": 3583
    },
    {
      "epoch": 0.09381052925471584,
      "grad_norm": 17.364971160888672,
      "learning_rate": 7.474483119602199e-07,
      "loss": 0.4638,
      "step": 3584
    },
    {
      "epoch": 0.09383670406756592,
      "grad_norm": 21.61435317993164,
      "learning_rate": 7.443077728343366e-07,
      "loss": 0.2992,
      "step": 3585
    },
    {
      "epoch": 0.093862878880416,
      "grad_norm": 32.45465850830078,
      "learning_rate": 7.411672337084533e-07,
      "loss": 0.4921,
      "step": 3586
    },
    {
      "epoch": 0.0938890536932661,
      "grad_norm": 17.36029052734375,
      "learning_rate": 7.3802669458257e-07,
      "loss": 0.4229,
      "step": 3587
    },
    {
      "epoch": 0.09391522850611618,
      "grad_norm": 11.512957572937012,
      "learning_rate": 7.348861554566867e-07,
      "loss": 0.1862,
      "step": 3588
    },
    {
      "epoch": 0.09394140331896626,
      "grad_norm": 23.565532684326172,
      "learning_rate": 7.317456163308034e-07,
      "loss": 0.3902,
      "step": 3589
    },
    {
      "epoch": 0.09396757813181636,
      "grad_norm": 16.124013900756836,
      "learning_rate": 7.286050772049202e-07,
      "loss": 0.2194,
      "step": 3590
    },
    {
      "epoch": 0.09399375294466644,
      "grad_norm": 19.57329750061035,
      "learning_rate": 7.254645380790369e-07,
      "loss": 0.3056,
      "step": 3591
    },
    {
      "epoch": 0.09401992775751654,
      "grad_norm": 34.18194580078125,
      "learning_rate": 7.223239989531537e-07,
      "loss": 0.6096,
      "step": 3592
    },
    {
      "epoch": 0.09404610257036662,
      "grad_norm": 27.36029815673828,
      "learning_rate": 7.191834598272704e-07,
      "loss": 0.4243,
      "step": 3593
    },
    {
      "epoch": 0.0940722773832167,
      "grad_norm": 15.900996208190918,
      "learning_rate": 7.160429207013871e-07,
      "loss": 0.2223,
      "step": 3594
    },
    {
      "epoch": 0.0940984521960668,
      "grad_norm": 16.551176071166992,
      "learning_rate": 7.129023815755039e-07,
      "loss": 0.3087,
      "step": 3595
    },
    {
      "epoch": 0.09412462700891688,
      "grad_norm": 12.840459823608398,
      "learning_rate": 7.097618424496206e-07,
      "loss": 0.2651,
      "step": 3596
    },
    {
      "epoch": 0.09415080182176698,
      "grad_norm": 18.62626838684082,
      "learning_rate": 7.066213033237373e-07,
      "loss": 0.3764,
      "step": 3597
    },
    {
      "epoch": 0.09417697663461706,
      "grad_norm": 24.31450653076172,
      "learning_rate": 7.03480764197854e-07,
      "loss": 0.3474,
      "step": 3598
    },
    {
      "epoch": 0.09420315144746715,
      "grad_norm": 19.118276596069336,
      "learning_rate": 7.003402250719707e-07,
      "loss": 0.2098,
      "step": 3599
    },
    {
      "epoch": 0.09422932626031724,
      "grad_norm": 24.258888244628906,
      "learning_rate": 6.971996859460875e-07,
      "loss": 0.4009,
      "step": 3600
    },
    {
      "epoch": 0.09425550107316732,
      "grad_norm": 30.42157554626465,
      "learning_rate": 6.940591468202042e-07,
      "loss": 0.3292,
      "step": 3601
    },
    {
      "epoch": 0.09428167588601742,
      "grad_norm": 18.440826416015625,
      "learning_rate": 6.909186076943209e-07,
      "loss": 0.1935,
      "step": 3602
    },
    {
      "epoch": 0.0943078506988675,
      "grad_norm": 22.163372039794922,
      "learning_rate": 6.877780685684376e-07,
      "loss": 0.305,
      "step": 3603
    },
    {
      "epoch": 0.09433402551171759,
      "grad_norm": 28.223737716674805,
      "learning_rate": 6.846375294425542e-07,
      "loss": 0.3433,
      "step": 3604
    },
    {
      "epoch": 0.09436020032456768,
      "grad_norm": 24.005950927734375,
      "learning_rate": 6.81496990316671e-07,
      "loss": 0.264,
      "step": 3605
    },
    {
      "epoch": 0.09438637513741777,
      "grad_norm": 23.98711395263672,
      "learning_rate": 6.783564511907878e-07,
      "loss": 0.4237,
      "step": 3606
    },
    {
      "epoch": 0.09441254995026785,
      "grad_norm": 28.69396209716797,
      "learning_rate": 6.752159120649045e-07,
      "loss": 0.4951,
      "step": 3607
    },
    {
      "epoch": 0.09443872476311795,
      "grad_norm": 21.826509475708008,
      "learning_rate": 6.720753729390212e-07,
      "loss": 0.3567,
      "step": 3608
    },
    {
      "epoch": 0.09446489957596803,
      "grad_norm": 20.49100685119629,
      "learning_rate": 6.689348338131379e-07,
      "loss": 0.4306,
      "step": 3609
    },
    {
      "epoch": 0.09449107438881812,
      "grad_norm": 15.272256851196289,
      "learning_rate": 6.657942946872547e-07,
      "loss": 0.2401,
      "step": 3610
    },
    {
      "epoch": 0.09451724920166821,
      "grad_norm": 19.867910385131836,
      "learning_rate": 6.626537555613714e-07,
      "loss": 0.3656,
      "step": 3611
    },
    {
      "epoch": 0.09454342401451829,
      "grad_norm": 24.733116149902344,
      "learning_rate": 6.595132164354881e-07,
      "loss": 0.4068,
      "step": 3612
    },
    {
      "epoch": 0.09456959882736839,
      "grad_norm": 17.993131637573242,
      "learning_rate": 6.563726773096048e-07,
      "loss": 0.2624,
      "step": 3613
    },
    {
      "epoch": 0.09459577364021847,
      "grad_norm": 19.25414276123047,
      "learning_rate": 6.532321381837215e-07,
      "loss": 0.2685,
      "step": 3614
    },
    {
      "epoch": 0.09462194845306857,
      "grad_norm": 22.781177520751953,
      "learning_rate": 6.500915990578383e-07,
      "loss": 0.2684,
      "step": 3615
    },
    {
      "epoch": 0.09464812326591865,
      "grad_norm": 13.090982437133789,
      "learning_rate": 6.46951059931955e-07,
      "loss": 0.2078,
      "step": 3616
    },
    {
      "epoch": 0.09467429807876873,
      "grad_norm": 12.958003044128418,
      "learning_rate": 6.438105208060717e-07,
      "loss": 0.2196,
      "step": 3617
    },
    {
      "epoch": 0.09470047289161883,
      "grad_norm": 21.457151412963867,
      "learning_rate": 6.406699816801884e-07,
      "loss": 0.3621,
      "step": 3618
    },
    {
      "epoch": 0.09472664770446891,
      "grad_norm": 16.73446273803711,
      "learning_rate": 6.375294425543051e-07,
      "loss": 0.2074,
      "step": 3619
    },
    {
      "epoch": 0.09475282251731901,
      "grad_norm": 24.629844665527344,
      "learning_rate": 6.343889034284218e-07,
      "loss": 0.3507,
      "step": 3620
    },
    {
      "epoch": 0.09477899733016909,
      "grad_norm": 14.14023494720459,
      "learning_rate": 6.312483643025387e-07,
      "loss": 0.2838,
      "step": 3621
    },
    {
      "epoch": 0.09480517214301917,
      "grad_norm": 12.244000434875488,
      "learning_rate": 6.281078251766554e-07,
      "loss": 0.1684,
      "step": 3622
    },
    {
      "epoch": 0.09483134695586927,
      "grad_norm": 25.263578414916992,
      "learning_rate": 6.249672860507721e-07,
      "loss": 0.3642,
      "step": 3623
    },
    {
      "epoch": 0.09485752176871935,
      "grad_norm": 15.25902271270752,
      "learning_rate": 6.218267469248888e-07,
      "loss": 0.2838,
      "step": 3624
    },
    {
      "epoch": 0.09488369658156943,
      "grad_norm": 15.91098403930664,
      "learning_rate": 6.186862077990055e-07,
      "loss": 0.3308,
      "step": 3625
    },
    {
      "epoch": 0.09490987139441953,
      "grad_norm": 17.33119010925293,
      "learning_rate": 6.155456686731223e-07,
      "loss": 0.2808,
      "step": 3626
    },
    {
      "epoch": 0.09493604620726961,
      "grad_norm": 19.96273422241211,
      "learning_rate": 6.12405129547239e-07,
      "loss": 0.3934,
      "step": 3627
    },
    {
      "epoch": 0.09496222102011971,
      "grad_norm": 16.617938995361328,
      "learning_rate": 6.092645904213557e-07,
      "loss": 0.245,
      "step": 3628
    },
    {
      "epoch": 0.0949883958329698,
      "grad_norm": 23.866731643676758,
      "learning_rate": 6.061240512954724e-07,
      "loss": 0.447,
      "step": 3629
    },
    {
      "epoch": 0.09501457064581988,
      "grad_norm": 21.28534698486328,
      "learning_rate": 6.029835121695891e-07,
      "loss": 0.3539,
      "step": 3630
    },
    {
      "epoch": 0.09504074545866997,
      "grad_norm": 20.14683723449707,
      "learning_rate": 5.998429730437059e-07,
      "loss": 0.3003,
      "step": 3631
    },
    {
      "epoch": 0.09506692027152006,
      "grad_norm": 16.487117767333984,
      "learning_rate": 5.967024339178226e-07,
      "loss": 0.2741,
      "step": 3632
    },
    {
      "epoch": 0.09509309508437015,
      "grad_norm": 20.6639404296875,
      "learning_rate": 5.935618947919392e-07,
      "loss": 0.5626,
      "step": 3633
    },
    {
      "epoch": 0.09511926989722023,
      "grad_norm": 17.1444091796875,
      "learning_rate": 5.90421355666056e-07,
      "loss": 0.2553,
      "step": 3634
    },
    {
      "epoch": 0.09514544471007032,
      "grad_norm": 18.561288833618164,
      "learning_rate": 5.872808165401728e-07,
      "loss": 0.4569,
      "step": 3635
    },
    {
      "epoch": 0.09517161952292041,
      "grad_norm": 26.989717483520508,
      "learning_rate": 5.841402774142895e-07,
      "loss": 0.5241,
      "step": 3636
    },
    {
      "epoch": 0.0951977943357705,
      "grad_norm": 27.14459228515625,
      "learning_rate": 5.809997382884062e-07,
      "loss": 0.4116,
      "step": 3637
    },
    {
      "epoch": 0.0952239691486206,
      "grad_norm": 22.42402458190918,
      "learning_rate": 5.778591991625229e-07,
      "loss": 0.4433,
      "step": 3638
    },
    {
      "epoch": 0.09525014396147068,
      "grad_norm": 16.671871185302734,
      "learning_rate": 5.747186600366396e-07,
      "loss": 0.3567,
      "step": 3639
    },
    {
      "epoch": 0.09527631877432076,
      "grad_norm": 19.758907318115234,
      "learning_rate": 5.715781209107563e-07,
      "loss": 0.3159,
      "step": 3640
    },
    {
      "epoch": 0.09530249358717086,
      "grad_norm": 18.004806518554688,
      "learning_rate": 5.684375817848731e-07,
      "loss": 0.2327,
      "step": 3641
    },
    {
      "epoch": 0.09532866840002094,
      "grad_norm": 18.900836944580078,
      "learning_rate": 5.652970426589898e-07,
      "loss": 0.2498,
      "step": 3642
    },
    {
      "epoch": 0.09535484321287102,
      "grad_norm": 25.364500045776367,
      "learning_rate": 5.621565035331065e-07,
      "loss": 0.3271,
      "step": 3643
    },
    {
      "epoch": 0.09538101802572112,
      "grad_norm": 14.358928680419922,
      "learning_rate": 5.590159644072232e-07,
      "loss": 0.1872,
      "step": 3644
    },
    {
      "epoch": 0.0954071928385712,
      "grad_norm": 29.370624542236328,
      "learning_rate": 5.558754252813399e-07,
      "loss": 0.4343,
      "step": 3645
    },
    {
      "epoch": 0.0954333676514213,
      "grad_norm": 11.892261505126953,
      "learning_rate": 5.527348861554567e-07,
      "loss": 0.2076,
      "step": 3646
    },
    {
      "epoch": 0.09545954246427138,
      "grad_norm": 18.07562828063965,
      "learning_rate": 5.495943470295734e-07,
      "loss": 0.1837,
      "step": 3647
    },
    {
      "epoch": 0.09548571727712146,
      "grad_norm": 21.577341079711914,
      "learning_rate": 5.464538079036902e-07,
      "loss": 0.3053,
      "step": 3648
    },
    {
      "epoch": 0.09551189208997156,
      "grad_norm": 19.341135025024414,
      "learning_rate": 5.433132687778069e-07,
      "loss": 0.3375,
      "step": 3649
    },
    {
      "epoch": 0.09553806690282164,
      "grad_norm": 12.44001579284668,
      "learning_rate": 5.401727296519236e-07,
      "loss": 0.2524,
      "step": 3650
    },
    {
      "epoch": 0.09556424171567174,
      "grad_norm": 19.6269588470459,
      "learning_rate": 5.370321905260404e-07,
      "loss": 0.3435,
      "step": 3651
    },
    {
      "epoch": 0.09559041652852182,
      "grad_norm": 20.460477828979492,
      "learning_rate": 5.338916514001571e-07,
      "loss": 0.3105,
      "step": 3652
    },
    {
      "epoch": 0.0956165913413719,
      "grad_norm": 22.008419036865234,
      "learning_rate": 5.307511122742738e-07,
      "loss": 0.3457,
      "step": 3653
    },
    {
      "epoch": 0.095642766154222,
      "grad_norm": 12.65740966796875,
      "learning_rate": 5.276105731483905e-07,
      "loss": 0.1938,
      "step": 3654
    },
    {
      "epoch": 0.09566894096707208,
      "grad_norm": 12.133246421813965,
      "learning_rate": 5.244700340225072e-07,
      "loss": 0.1823,
      "step": 3655
    },
    {
      "epoch": 0.09569511577992218,
      "grad_norm": 27.02764892578125,
      "learning_rate": 5.213294948966239e-07,
      "loss": 0.4471,
      "step": 3656
    },
    {
      "epoch": 0.09572129059277226,
      "grad_norm": 19.028968811035156,
      "learning_rate": 5.181889557707407e-07,
      "loss": 0.3323,
      "step": 3657
    },
    {
      "epoch": 0.09574746540562235,
      "grad_norm": 26.891477584838867,
      "learning_rate": 5.150484166448574e-07,
      "loss": 0.3875,
      "step": 3658
    },
    {
      "epoch": 0.09577364021847244,
      "grad_norm": 22.503620147705078,
      "learning_rate": 5.119078775189741e-07,
      "loss": 0.3085,
      "step": 3659
    },
    {
      "epoch": 0.09579981503132252,
      "grad_norm": 26.873491287231445,
      "learning_rate": 5.087673383930908e-07,
      "loss": 0.4278,
      "step": 3660
    },
    {
      "epoch": 0.09582598984417262,
      "grad_norm": 21.07273292541504,
      "learning_rate": 5.056267992672075e-07,
      "loss": 0.2624,
      "step": 3661
    },
    {
      "epoch": 0.0958521646570227,
      "grad_norm": 7.1788411140441895,
      "learning_rate": 5.024862601413243e-07,
      "loss": 0.1,
      "step": 3662
    },
    {
      "epoch": 0.09587833946987279,
      "grad_norm": 22.03641700744629,
      "learning_rate": 4.99345721015441e-07,
      "loss": 0.4063,
      "step": 3663
    },
    {
      "epoch": 0.09590451428272288,
      "grad_norm": 18.78973960876465,
      "learning_rate": 4.962051818895577e-07,
      "loss": 0.4626,
      "step": 3664
    },
    {
      "epoch": 0.09593068909557297,
      "grad_norm": 11.398391723632812,
      "learning_rate": 4.930646427636744e-07,
      "loss": 0.2159,
      "step": 3665
    },
    {
      "epoch": 0.09595686390842305,
      "grad_norm": 27.48072624206543,
      "learning_rate": 4.899241036377912e-07,
      "loss": 0.4699,
      "step": 3666
    },
    {
      "epoch": 0.09598303872127315,
      "grad_norm": 16.78904151916504,
      "learning_rate": 4.867835645119079e-07,
      "loss": 0.2625,
      "step": 3667
    },
    {
      "epoch": 0.09600921353412323,
      "grad_norm": 17.150203704833984,
      "learning_rate": 4.836430253860246e-07,
      "loss": 0.2242,
      "step": 3668
    },
    {
      "epoch": 0.09603538834697332,
      "grad_norm": 14.373023986816406,
      "learning_rate": 4.805024862601413e-07,
      "loss": 0.1975,
      "step": 3669
    },
    {
      "epoch": 0.09606156315982341,
      "grad_norm": 27.16855239868164,
      "learning_rate": 4.77361947134258e-07,
      "loss": 0.3409,
      "step": 3670
    },
    {
      "epoch": 0.09608773797267349,
      "grad_norm": 22.37746238708496,
      "learning_rate": 4.7422140800837477e-07,
      "loss": 0.3959,
      "step": 3671
    },
    {
      "epoch": 0.09611391278552359,
      "grad_norm": 26.69382667541504,
      "learning_rate": 4.7108086888249147e-07,
      "loss": 0.3499,
      "step": 3672
    },
    {
      "epoch": 0.09614008759837367,
      "grad_norm": 28.742504119873047,
      "learning_rate": 4.6794032975660823e-07,
      "loss": 0.3428,
      "step": 3673
    },
    {
      "epoch": 0.09616626241122377,
      "grad_norm": 27.33993148803711,
      "learning_rate": 4.6479979063072494e-07,
      "loss": 0.4291,
      "step": 3674
    },
    {
      "epoch": 0.09619243722407385,
      "grad_norm": 14.351786613464355,
      "learning_rate": 4.616592515048417e-07,
      "loss": 0.2734,
      "step": 3675
    },
    {
      "epoch": 0.09621861203692393,
      "grad_norm": 24.88285255432129,
      "learning_rate": 4.585187123789584e-07,
      "loss": 0.3189,
      "step": 3676
    },
    {
      "epoch": 0.09624478684977403,
      "grad_norm": 19.81352996826172,
      "learning_rate": 4.553781732530751e-07,
      "loss": 0.3042,
      "step": 3677
    },
    {
      "epoch": 0.09627096166262411,
      "grad_norm": 28.406740188598633,
      "learning_rate": 4.5223763412719187e-07,
      "loss": 0.3175,
      "step": 3678
    },
    {
      "epoch": 0.09629713647547421,
      "grad_norm": 22.306665420532227,
      "learning_rate": 4.490970950013086e-07,
      "loss": 0.3497,
      "step": 3679
    },
    {
      "epoch": 0.09632331128832429,
      "grad_norm": 16.566938400268555,
      "learning_rate": 4.459565558754253e-07,
      "loss": 0.3097,
      "step": 3680
    },
    {
      "epoch": 0.09634948610117437,
      "grad_norm": 17.758241653442383,
      "learning_rate": 4.4281601674954204e-07,
      "loss": 0.3253,
      "step": 3681
    },
    {
      "epoch": 0.09637566091402447,
      "grad_norm": 19.165546417236328,
      "learning_rate": 4.3967547762365875e-07,
      "loss": 0.487,
      "step": 3682
    },
    {
      "epoch": 0.09640183572687455,
      "grad_norm": 25.37455940246582,
      "learning_rate": 4.365349384977755e-07,
      "loss": 0.4183,
      "step": 3683
    },
    {
      "epoch": 0.09642801053972463,
      "grad_norm": 19.045625686645508,
      "learning_rate": 4.333943993718922e-07,
      "loss": 0.4073,
      "step": 3684
    },
    {
      "epoch": 0.09645418535257473,
      "grad_norm": 14.622788429260254,
      "learning_rate": 4.302538602460089e-07,
      "loss": 0.2565,
      "step": 3685
    },
    {
      "epoch": 0.09648036016542481,
      "grad_norm": 15.489789962768555,
      "learning_rate": 4.271133211201257e-07,
      "loss": 0.3211,
      "step": 3686
    },
    {
      "epoch": 0.09650653497827491,
      "grad_norm": 16.94680404663086,
      "learning_rate": 4.239727819942424e-07,
      "loss": 0.3197,
      "step": 3687
    },
    {
      "epoch": 0.096532709791125,
      "grad_norm": 30.309782028198242,
      "learning_rate": 4.208322428683591e-07,
      "loss": 0.4042,
      "step": 3688
    },
    {
      "epoch": 0.09655888460397508,
      "grad_norm": 28.04390525817871,
      "learning_rate": 4.1769170374247585e-07,
      "loss": 0.4207,
      "step": 3689
    },
    {
      "epoch": 0.09658505941682517,
      "grad_norm": 30.487789154052734,
      "learning_rate": 4.1455116461659256e-07,
      "loss": 0.3669,
      "step": 3690
    },
    {
      "epoch": 0.09661123422967526,
      "grad_norm": 10.190549850463867,
      "learning_rate": 4.114106254907092e-07,
      "loss": 0.1717,
      "step": 3691
    },
    {
      "epoch": 0.09663740904252535,
      "grad_norm": 31.991037368774414,
      "learning_rate": 4.082700863648259e-07,
      "loss": 0.5085,
      "step": 3692
    },
    {
      "epoch": 0.09666358385537543,
      "grad_norm": 18.971343994140625,
      "learning_rate": 4.051295472389427e-07,
      "loss": 0.3713,
      "step": 3693
    },
    {
      "epoch": 0.09668975866822552,
      "grad_norm": 30.502532958984375,
      "learning_rate": 4.019890081130594e-07,
      "loss": 0.2979,
      "step": 3694
    },
    {
      "epoch": 0.09671593348107561,
      "grad_norm": 31.28369903564453,
      "learning_rate": 3.988484689871761e-07,
      "loss": 0.3395,
      "step": 3695
    },
    {
      "epoch": 0.0967421082939257,
      "grad_norm": 30.9097843170166,
      "learning_rate": 3.9570792986129285e-07,
      "loss": 0.4141,
      "step": 3696
    },
    {
      "epoch": 0.0967682831067758,
      "grad_norm": 13.1896390914917,
      "learning_rate": 3.9256739073540955e-07,
      "loss": 0.1871,
      "step": 3697
    },
    {
      "epoch": 0.09679445791962588,
      "grad_norm": 14.286089897155762,
      "learning_rate": 3.894268516095263e-07,
      "loss": 0.3237,
      "step": 3698
    },
    {
      "epoch": 0.09682063273247596,
      "grad_norm": 30.730918884277344,
      "learning_rate": 3.86286312483643e-07,
      "loss": 0.403,
      "step": 3699
    },
    {
      "epoch": 0.09684680754532606,
      "grad_norm": 14.539217948913574,
      "learning_rate": 3.831457733577597e-07,
      "loss": 0.3535,
      "step": 3700
    },
    {
      "epoch": 0.09687298235817614,
      "grad_norm": 19.790264129638672,
      "learning_rate": 3.800052342318765e-07,
      "loss": 0.4106,
      "step": 3701
    },
    {
      "epoch": 0.09689915717102622,
      "grad_norm": 21.1683292388916,
      "learning_rate": 3.768646951059932e-07,
      "loss": 0.2671,
      "step": 3702
    },
    {
      "epoch": 0.09692533198387632,
      "grad_norm": 17.609619140625,
      "learning_rate": 3.7372415598010995e-07,
      "loss": 0.3099,
      "step": 3703
    },
    {
      "epoch": 0.0969515067967264,
      "grad_norm": 27.368711471557617,
      "learning_rate": 3.7058361685422665e-07,
      "loss": 0.5402,
      "step": 3704
    },
    {
      "epoch": 0.0969776816095765,
      "grad_norm": 20.12662696838379,
      "learning_rate": 3.6744307772834336e-07,
      "loss": 0.2436,
      "step": 3705
    },
    {
      "epoch": 0.09700385642242658,
      "grad_norm": 19.806577682495117,
      "learning_rate": 3.643025386024601e-07,
      "loss": 0.2707,
      "step": 3706
    },
    {
      "epoch": 0.09703003123527666,
      "grad_norm": 23.465999603271484,
      "learning_rate": 3.611619994765768e-07,
      "loss": 0.5215,
      "step": 3707
    },
    {
      "epoch": 0.09705620604812676,
      "grad_norm": 15.103630065917969,
      "learning_rate": 3.5802146035069353e-07,
      "loss": 0.2054,
      "step": 3708
    },
    {
      "epoch": 0.09708238086097684,
      "grad_norm": 25.01662254333496,
      "learning_rate": 3.548809212248103e-07,
      "loss": 0.5159,
      "step": 3709
    },
    {
      "epoch": 0.09710855567382694,
      "grad_norm": 30.916379928588867,
      "learning_rate": 3.51740382098927e-07,
      "loss": 0.5348,
      "step": 3710
    },
    {
      "epoch": 0.09713473048667702,
      "grad_norm": 15.059144973754883,
      "learning_rate": 3.4859984297304376e-07,
      "loss": 0.2935,
      "step": 3711
    },
    {
      "epoch": 0.0971609052995271,
      "grad_norm": 12.571011543273926,
      "learning_rate": 3.4545930384716046e-07,
      "loss": 0.2237,
      "step": 3712
    },
    {
      "epoch": 0.0971870801123772,
      "grad_norm": 23.528217315673828,
      "learning_rate": 3.423187647212771e-07,
      "loss": 0.304,
      "step": 3713
    },
    {
      "epoch": 0.09721325492522728,
      "grad_norm": 26.85362434387207,
      "learning_rate": 3.391782255953939e-07,
      "loss": 0.3546,
      "step": 3714
    },
    {
      "epoch": 0.09723942973807738,
      "grad_norm": 21.463510513305664,
      "learning_rate": 3.360376864695106e-07,
      "loss": 0.2663,
      "step": 3715
    },
    {
      "epoch": 0.09726560455092746,
      "grad_norm": 16.59992027282715,
      "learning_rate": 3.3289714734362734e-07,
      "loss": 0.2922,
      "step": 3716
    },
    {
      "epoch": 0.09729177936377754,
      "grad_norm": 31.939638137817383,
      "learning_rate": 3.2975660821774405e-07,
      "loss": 0.4248,
      "step": 3717
    },
    {
      "epoch": 0.09731795417662764,
      "grad_norm": 26.21902847290039,
      "learning_rate": 3.2661606909186075e-07,
      "loss": 0.3976,
      "step": 3718
    },
    {
      "epoch": 0.09734412898947772,
      "grad_norm": 16.3249454498291,
      "learning_rate": 3.234755299659775e-07,
      "loss": 0.1714,
      "step": 3719
    },
    {
      "epoch": 0.0973703038023278,
      "grad_norm": 16.415897369384766,
      "learning_rate": 3.203349908400942e-07,
      "loss": 0.2348,
      "step": 3720
    },
    {
      "epoch": 0.0973964786151779,
      "grad_norm": 18.0613956451416,
      "learning_rate": 3.171944517142109e-07,
      "loss": 0.2565,
      "step": 3721
    },
    {
      "epoch": 0.09742265342802799,
      "grad_norm": 21.051301956176758,
      "learning_rate": 3.140539125883277e-07,
      "loss": 0.3403,
      "step": 3722
    },
    {
      "epoch": 0.09744882824087808,
      "grad_norm": 35.02796936035156,
      "learning_rate": 3.109133734624444e-07,
      "loss": 0.3285,
      "step": 3723
    },
    {
      "epoch": 0.09747500305372817,
      "grad_norm": 17.308940887451172,
      "learning_rate": 3.0777283433656115e-07,
      "loss": 0.4135,
      "step": 3724
    },
    {
      "epoch": 0.09750117786657825,
      "grad_norm": 15.916816711425781,
      "learning_rate": 3.0463229521067786e-07,
      "loss": 0.2377,
      "step": 3725
    },
    {
      "epoch": 0.09752735267942834,
      "grad_norm": 23.205541610717773,
      "learning_rate": 3.0149175608479456e-07,
      "loss": 0.4085,
      "step": 3726
    },
    {
      "epoch": 0.09755352749227843,
      "grad_norm": 25.928207397460938,
      "learning_rate": 2.983512169589113e-07,
      "loss": 0.4619,
      "step": 3727
    },
    {
      "epoch": 0.09757970230512852,
      "grad_norm": 20.918785095214844,
      "learning_rate": 2.95210677833028e-07,
      "loss": 0.4796,
      "step": 3728
    },
    {
      "epoch": 0.0976058771179786,
      "grad_norm": 27.95805549621582,
      "learning_rate": 2.9207013870714473e-07,
      "loss": 0.3763,
      "step": 3729
    },
    {
      "epoch": 0.09763205193082869,
      "grad_norm": 17.191064834594727,
      "learning_rate": 2.8892959958126144e-07,
      "loss": 0.3381,
      "step": 3730
    },
    {
      "epoch": 0.09765822674367879,
      "grad_norm": 23.597196578979492,
      "learning_rate": 2.8578906045537815e-07,
      "loss": 0.3521,
      "step": 3731
    },
    {
      "epoch": 0.09768440155652887,
      "grad_norm": 21.995336532592773,
      "learning_rate": 2.826485213294949e-07,
      "loss": 0.4098,
      "step": 3732
    },
    {
      "epoch": 0.09771057636937897,
      "grad_norm": 14.576983451843262,
      "learning_rate": 2.795079822036116e-07,
      "loss": 0.2287,
      "step": 3733
    },
    {
      "epoch": 0.09773675118222905,
      "grad_norm": 23.12648582458496,
      "learning_rate": 2.7636744307772837e-07,
      "loss": 0.2872,
      "step": 3734
    },
    {
      "epoch": 0.09776292599507913,
      "grad_norm": 22.093896865844727,
      "learning_rate": 2.732269039518451e-07,
      "loss": 0.349,
      "step": 3735
    },
    {
      "epoch": 0.09778910080792923,
      "grad_norm": 14.338345527648926,
      "learning_rate": 2.700863648259618e-07,
      "loss": 0.2907,
      "step": 3736
    },
    {
      "epoch": 0.09781527562077931,
      "grad_norm": 19.61227035522461,
      "learning_rate": 2.6694582570007854e-07,
      "loss": 0.3056,
      "step": 3737
    },
    {
      "epoch": 0.09784145043362939,
      "grad_norm": 31.334821701049805,
      "learning_rate": 2.6380528657419525e-07,
      "loss": 0.3829,
      "step": 3738
    },
    {
      "epoch": 0.09786762524647949,
      "grad_norm": 13.199037551879883,
      "learning_rate": 2.6066474744831195e-07,
      "loss": 0.238,
      "step": 3739
    },
    {
      "epoch": 0.09789380005932957,
      "grad_norm": 13.10763168334961,
      "learning_rate": 2.575242083224287e-07,
      "loss": 0.2805,
      "step": 3740
    },
    {
      "epoch": 0.09791997487217967,
      "grad_norm": 19.208444595336914,
      "learning_rate": 2.543836691965454e-07,
      "loss": 0.3319,
      "step": 3741
    },
    {
      "epoch": 0.09794614968502975,
      "grad_norm": 14.381275177001953,
      "learning_rate": 2.512431300706621e-07,
      "loss": 0.397,
      "step": 3742
    },
    {
      "epoch": 0.09797232449787983,
      "grad_norm": 18.22126579284668,
      "learning_rate": 2.4810259094477883e-07,
      "loss": 0.2314,
      "step": 3743
    },
    {
      "epoch": 0.09799849931072993,
      "grad_norm": 39.76565933227539,
      "learning_rate": 2.449620518188956e-07,
      "loss": 0.2813,
      "step": 3744
    },
    {
      "epoch": 0.09802467412358001,
      "grad_norm": 17.607271194458008,
      "learning_rate": 2.418215126930123e-07,
      "loss": 0.3972,
      "step": 3745
    },
    {
      "epoch": 0.09805084893643011,
      "grad_norm": 18.727672576904297,
      "learning_rate": 2.38680973567129e-07,
      "loss": 0.2276,
      "step": 3746
    },
    {
      "epoch": 0.09807702374928019,
      "grad_norm": 22.42340850830078,
      "learning_rate": 2.3554043444124574e-07,
      "loss": 0.4409,
      "step": 3747
    },
    {
      "epoch": 0.09810319856213028,
      "grad_norm": 27.032255172729492,
      "learning_rate": 2.3239989531536247e-07,
      "loss": 0.6032,
      "step": 3748
    },
    {
      "epoch": 0.09812937337498037,
      "grad_norm": 18.56482696533203,
      "learning_rate": 2.292593561894792e-07,
      "loss": 0.394,
      "step": 3749
    },
    {
      "epoch": 0.09815554818783045,
      "grad_norm": 21.011005401611328,
      "learning_rate": 2.2611881706359593e-07,
      "loss": 0.4452,
      "step": 3750
    },
    {
      "epoch": 0.09818172300068055,
      "grad_norm": 13.125988960266113,
      "learning_rate": 2.2297827793771264e-07,
      "loss": 0.287,
      "step": 3751
    },
    {
      "epoch": 0.09820789781353063,
      "grad_norm": 17.26704216003418,
      "learning_rate": 2.1983773881182937e-07,
      "loss": 0.4229,
      "step": 3752
    },
    {
      "epoch": 0.09823407262638072,
      "grad_norm": 19.308361053466797,
      "learning_rate": 2.166971996859461e-07,
      "loss": 0.1343,
      "step": 3753
    },
    {
      "epoch": 0.09826024743923081,
      "grad_norm": 21.114538192749023,
      "learning_rate": 2.1355666056006284e-07,
      "loss": 0.3167,
      "step": 3754
    },
    {
      "epoch": 0.0982864222520809,
      "grad_norm": 13.85857105255127,
      "learning_rate": 2.1041612143417954e-07,
      "loss": 0.4027,
      "step": 3755
    },
    {
      "epoch": 0.09831259706493098,
      "grad_norm": 22.355737686157227,
      "learning_rate": 2.0727558230829628e-07,
      "loss": 0.3064,
      "step": 3756
    },
    {
      "epoch": 0.09833877187778108,
      "grad_norm": 32.039859771728516,
      "learning_rate": 2.0413504318241296e-07,
      "loss": 0.4198,
      "step": 3757
    },
    {
      "epoch": 0.09836494669063116,
      "grad_norm": 20.771881103515625,
      "learning_rate": 2.009945040565297e-07,
      "loss": 0.4635,
      "step": 3758
    },
    {
      "epoch": 0.09839112150348125,
      "grad_norm": 12.47254467010498,
      "learning_rate": 1.9785396493064642e-07,
      "loss": 0.2199,
      "step": 3759
    },
    {
      "epoch": 0.09841729631633134,
      "grad_norm": 24.547836303710938,
      "learning_rate": 1.9471342580476316e-07,
      "loss": 0.409,
      "step": 3760
    },
    {
      "epoch": 0.09844347112918142,
      "grad_norm": 18.2034969329834,
      "learning_rate": 1.9157288667887986e-07,
      "loss": 0.3071,
      "step": 3761
    },
    {
      "epoch": 0.09846964594203152,
      "grad_norm": 22.851640701293945,
      "learning_rate": 1.884323475529966e-07,
      "loss": 0.2186,
      "step": 3762
    },
    {
      "epoch": 0.0984958207548816,
      "grad_norm": 22.5870361328125,
      "learning_rate": 1.8529180842711333e-07,
      "loss": 0.4424,
      "step": 3763
    },
    {
      "epoch": 0.0985219955677317,
      "grad_norm": 11.740909576416016,
      "learning_rate": 1.8215126930123006e-07,
      "loss": 0.2927,
      "step": 3764
    },
    {
      "epoch": 0.09854817038058178,
      "grad_norm": 13.101034164428711,
      "learning_rate": 1.7901073017534677e-07,
      "loss": 0.2142,
      "step": 3765
    },
    {
      "epoch": 0.09857434519343186,
      "grad_norm": 20.788280487060547,
      "learning_rate": 1.758701910494635e-07,
      "loss": 0.3257,
      "step": 3766
    },
    {
      "epoch": 0.09860052000628196,
      "grad_norm": 12.902499198913574,
      "learning_rate": 1.7272965192358023e-07,
      "loss": 0.1635,
      "step": 3767
    },
    {
      "epoch": 0.09862669481913204,
      "grad_norm": 24.85701560974121,
      "learning_rate": 1.6958911279769694e-07,
      "loss": 0.2431,
      "step": 3768
    },
    {
      "epoch": 0.09865286963198214,
      "grad_norm": 9.956838607788086,
      "learning_rate": 1.6644857367181367e-07,
      "loss": 0.2631,
      "step": 3769
    },
    {
      "epoch": 0.09867904444483222,
      "grad_norm": 20.43240737915039,
      "learning_rate": 1.6330803454593038e-07,
      "loss": 0.337,
      "step": 3770
    },
    {
      "epoch": 0.0987052192576823,
      "grad_norm": 20.711387634277344,
      "learning_rate": 1.601674954200471e-07,
      "loss": 0.5223,
      "step": 3771
    },
    {
      "epoch": 0.0987313940705324,
      "grad_norm": 26.31517219543457,
      "learning_rate": 1.5702695629416384e-07,
      "loss": 0.3895,
      "step": 3772
    },
    {
      "epoch": 0.09875756888338248,
      "grad_norm": 19.874465942382812,
      "learning_rate": 1.5388641716828057e-07,
      "loss": 0.3723,
      "step": 3773
    },
    {
      "epoch": 0.09878374369623258,
      "grad_norm": 18.229761123657227,
      "learning_rate": 1.5074587804239728e-07,
      "loss": 0.2528,
      "step": 3774
    },
    {
      "epoch": 0.09880991850908266,
      "grad_norm": 18.231510162353516,
      "learning_rate": 1.47605338916514e-07,
      "loss": 0.2979,
      "step": 3775
    },
    {
      "epoch": 0.09883609332193274,
      "grad_norm": 13.325865745544434,
      "learning_rate": 1.4446479979063072e-07,
      "loss": 0.2544,
      "step": 3776
    },
    {
      "epoch": 0.09886226813478284,
      "grad_norm": 16.368202209472656,
      "learning_rate": 1.4132426066474745e-07,
      "loss": 0.3882,
      "step": 3777
    },
    {
      "epoch": 0.09888844294763292,
      "grad_norm": 25.938459396362305,
      "learning_rate": 1.3818372153886419e-07,
      "loss": 0.3732,
      "step": 3778
    },
    {
      "epoch": 0.098914617760483,
      "grad_norm": 25.731861114501953,
      "learning_rate": 1.350431824129809e-07,
      "loss": 0.4738,
      "step": 3779
    },
    {
      "epoch": 0.0989407925733331,
      "grad_norm": 15.971735000610352,
      "learning_rate": 1.3190264328709762e-07,
      "loss": 0.2757,
      "step": 3780
    },
    {
      "epoch": 0.09896696738618319,
      "grad_norm": 44.89279556274414,
      "learning_rate": 1.2876210416121436e-07,
      "loss": 0.7273,
      "step": 3781
    },
    {
      "epoch": 0.09899314219903328,
      "grad_norm": 15.162711143493652,
      "learning_rate": 1.2562156503533106e-07,
      "loss": 0.3259,
      "step": 3782
    },
    {
      "epoch": 0.09901931701188336,
      "grad_norm": 31.1270809173584,
      "learning_rate": 1.224810259094478e-07,
      "loss": 0.6238,
      "step": 3783
    },
    {
      "epoch": 0.09904549182473345,
      "grad_norm": 12.24692440032959,
      "learning_rate": 1.193404867835645e-07,
      "loss": 0.1919,
      "step": 3784
    },
    {
      "epoch": 0.09907166663758354,
      "grad_norm": 13.557698249816895,
      "learning_rate": 1.1619994765768123e-07,
      "loss": 0.2536,
      "step": 3785
    },
    {
      "epoch": 0.09909784145043363,
      "grad_norm": 19.630537033081055,
      "learning_rate": 1.1305940853179797e-07,
      "loss": 0.374,
      "step": 3786
    },
    {
      "epoch": 0.09912401626328372,
      "grad_norm": 20.149948120117188,
      "learning_rate": 1.0991886940591469e-07,
      "loss": 0.2279,
      "step": 3787
    },
    {
      "epoch": 0.0991501910761338,
      "grad_norm": 22.178916931152344,
      "learning_rate": 1.0677833028003142e-07,
      "loss": 0.2744,
      "step": 3788
    },
    {
      "epoch": 0.09917636588898389,
      "grad_norm": 36.37748718261719,
      "learning_rate": 1.0363779115414814e-07,
      "loss": 0.3507,
      "step": 3789
    },
    {
      "epoch": 0.09920254070183399,
      "grad_norm": 27.55782699584961,
      "learning_rate": 1.0049725202826485e-07,
      "loss": 0.5129,
      "step": 3790
    },
    {
      "epoch": 0.09922871551468407,
      "grad_norm": 32.67668533325195,
      "learning_rate": 9.735671290238158e-08,
      "loss": 0.5536,
      "step": 3791
    },
    {
      "epoch": 0.09925489032753416,
      "grad_norm": 10.8208646774292,
      "learning_rate": 9.42161737764983e-08,
      "loss": 0.1428,
      "step": 3792
    },
    {
      "epoch": 0.09928106514038425,
      "grad_norm": 13.647860527038574,
      "learning_rate": 9.107563465061503e-08,
      "loss": 0.1764,
      "step": 3793
    },
    {
      "epoch": 0.09930723995323433,
      "grad_norm": 14.197077751159668,
      "learning_rate": 8.793509552473175e-08,
      "loss": 0.1815,
      "step": 3794
    },
    {
      "epoch": 0.09933341476608443,
      "grad_norm": 26.134899139404297,
      "learning_rate": 8.479455639884847e-08,
      "loss": 0.4521,
      "step": 3795
    },
    {
      "epoch": 0.09935958957893451,
      "grad_norm": 46.68376159667969,
      "learning_rate": 8.165401727296519e-08,
      "loss": 0.5736,
      "step": 3796
    },
    {
      "epoch": 0.09938576439178459,
      "grad_norm": 41.35911560058594,
      "learning_rate": 7.851347814708192e-08,
      "loss": 0.3607,
      "step": 3797
    },
    {
      "epoch": 0.09941193920463469,
      "grad_norm": 25.65618133544922,
      "learning_rate": 7.537293902119864e-08,
      "loss": 0.4451,
      "step": 3798
    },
    {
      "epoch": 0.09943811401748477,
      "grad_norm": 21.356494903564453,
      "learning_rate": 7.223239989531536e-08,
      "loss": 0.3339,
      "step": 3799
    },
    {
      "epoch": 0.09946428883033487,
      "grad_norm": 19.21754264831543,
      "learning_rate": 6.909186076943209e-08,
      "loss": 0.3908,
      "step": 3800
    },
    {
      "epoch": 0.09949046364318495,
      "grad_norm": 18.84501838684082,
      "learning_rate": 6.595132164354881e-08,
      "loss": 0.4265,
      "step": 3801
    },
    {
      "epoch": 0.09951663845603503,
      "grad_norm": 17.11476707458496,
      "learning_rate": 6.281078251766553e-08,
      "loss": 0.3384,
      "step": 3802
    },
    {
      "epoch": 0.09954281326888513,
      "grad_norm": 32.820621490478516,
      "learning_rate": 5.967024339178225e-08,
      "loss": 0.3105,
      "step": 3803
    },
    {
      "epoch": 0.09956898808173521,
      "grad_norm": 11.966936111450195,
      "learning_rate": 5.6529704265898984e-08,
      "loss": 0.1665,
      "step": 3804
    },
    {
      "epoch": 0.09959516289458531,
      "grad_norm": 20.911569595336914,
      "learning_rate": 5.338916514001571e-08,
      "loss": 0.3694,
      "step": 3805
    },
    {
      "epoch": 0.09962133770743539,
      "grad_norm": 15.919448852539062,
      "learning_rate": 5.024862601413242e-08,
      "loss": 0.2785,
      "step": 3806
    },
    {
      "epoch": 0.09964751252028548,
      "grad_norm": 13.004676818847656,
      "learning_rate": 4.710808688824915e-08,
      "loss": 0.2058,
      "step": 3807
    },
    {
      "epoch": 0.09967368733313557,
      "grad_norm": 22.05438995361328,
      "learning_rate": 4.3967547762365875e-08,
      "loss": 0.3417,
      "step": 3808
    },
    {
      "epoch": 0.09969986214598565,
      "grad_norm": 14.413435935974121,
      "learning_rate": 4.0827008636482594e-08,
      "loss": 0.2447,
      "step": 3809
    },
    {
      "epoch": 0.09972603695883575,
      "grad_norm": 23.337919235229492,
      "learning_rate": 3.768646951059932e-08,
      "loss": 0.3389,
      "step": 3810
    },
    {
      "epoch": 0.09975221177168583,
      "grad_norm": 12.339102745056152,
      "learning_rate": 3.4545930384716046e-08,
      "loss": 0.2584,
      "step": 3811
    },
    {
      "epoch": 0.09977838658453592,
      "grad_norm": 23.668968200683594,
      "learning_rate": 3.1405391258832766e-08,
      "loss": 0.2317,
      "step": 3812
    },
    {
      "epoch": 0.09980456139738601,
      "grad_norm": 23.93088150024414,
      "learning_rate": 2.8264852132949492e-08,
      "loss": 0.4695,
      "step": 3813
    },
    {
      "epoch": 0.0998307362102361,
      "grad_norm": 29.844846725463867,
      "learning_rate": 2.512431300706621e-08,
      "loss": 0.4778,
      "step": 3814
    },
    {
      "epoch": 0.09985691102308618,
      "grad_norm": 38.51411437988281,
      "learning_rate": 2.1983773881182937e-08,
      "loss": 0.5628,
      "step": 3815
    },
    {
      "epoch": 0.09988308583593627,
      "grad_norm": 9.810235977172852,
      "learning_rate": 1.884323475529966e-08,
      "loss": 0.197,
      "step": 3816
    },
    {
      "epoch": 0.09990926064878636,
      "grad_norm": 18.079242706298828,
      "learning_rate": 1.5702695629416383e-08,
      "loss": 0.4042,
      "step": 3817
    },
    {
      "epoch": 0.09993543546163645,
      "grad_norm": 31.431198120117188,
      "learning_rate": 1.2562156503533106e-08,
      "loss": 0.5596,
      "step": 3818
    },
    {
      "epoch": 0.09996161027448654,
      "grad_norm": 17.298336029052734,
      "learning_rate": 9.42161737764983e-09,
      "loss": 0.2501,
      "step": 3819
    },
    {
      "epoch": 0.09998778508733662,
      "grad_norm": 16.53080177307129,
      "learning_rate": 6.281078251766553e-09,
      "loss": 0.2812,
      "step": 3820
    },
    {
      "epoch": 0.10001395990018672,
      "grad_norm": 16.179168701171875,
      "learning_rate": 3.1405391258832764e-09,
      "loss": 0.2927,
      "step": 3821
    }
  ],
  "logging_steps": 1,
  "max_steps": 3821,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5743817847710208.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
